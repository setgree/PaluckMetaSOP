---
title: "04: Performing meta-analysis the Paluck Lab way"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{04: Performing meta-analysis the Paluck Lab way}
  %\VignetteEncoding{UTF-8}
  %\VignetteEngine{knitr::rmarkdown}
editor_options: 
  chunk_output_type: console
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```

```{r setup}
library(PaluckMetaSOP)
```

This vignette walks through the Paluck Lab's approach to meta-analysis itself, i.e. the statistical procedure by which the results of many studies are pooled. 

If you've gotten this far, you should already have a dataset with vectors of SMDs, varainces, and standard errors, which we explain further in the previous vignettes.

For this vignette, we'll demonstrate our functions using a built-in dataset from the `PaluckMetaSOP` package called `sv_data`. This is the dataset for "[Preventing Sexual Violence —A Behavioral Problem Without a Behaviorally-Informed Solution](https://osf.io/preprints/psyarxiv/xgbzj)". See the package manual for more details on all the variables. (We'll also use `contact_data` from "[The Contact Hypothesis Re-evaluated](https://www.cambridge.org/core/journals/behavioural-public-policy/article/contact-hypothesis-reevaluated/142C913E7FA9E121277B29E994124EC5)" for a few examples.)

# The easy case: combining all estimates into a single pooled effect size

The main function we'll use in this vignette is called `map_robust()`. 

It's a wrapped around a pre-exixsting function from the `metafor` package called `robust`, which is itself a wrapped around the function `rma`. See the `man` pages for those functions for full details.

`map_robust` takes a dataset and returns a tibble with an N of the number of data points; a Delta signifying the pooled effect size; an se signifying the standard error; and a p value.

```{r single_pooled_eff_size}
sv_data |> map_robust()
```

This is equivalent to running the following:
```{r robust_rma}
library(metafor)
robust(x = metafor::rma(yi = sv_data$d, vi = sv_data$var_d), 
       cluster = sv_data$unique_study_id)
```

Our function is different in a few respects. First, it's designed to take in a dataset via a piped operation (`|>`), which was actually the main reason we wrote it in the first place (we were in the habit of using a lot of pipes). Second, so long as your dataset contains variables called `d`, `var_d`, and `unique_study_id`, it's a lot easier to type. Third, it returns a truncated output that corresponds to the information we were actually recording in our results sections.

#### a note about clustering

Our meta-analyses cluster at the level of study. You could also cluster at the level of a paper, or of a team of authors, or at no level at all. Also, if you run our function on a dataset where each unique_study_id corresponds to only one row in the dataset, it's equivalent to not clustering. We'll demonstrate that with `PaluckMetaSOP::contact_data`:

```{r contact_data_demonstration}
library(dplyr)
contact_data |> select(-unique_study_id) |> # remove unique study id to reconstruct it
  group_by(name_short) |> mutate(unique_study_id = cur_group_id()) |>
  map_robust()
#' note: this number is a little different than the one we report in TCHR
#' we used Stata at the time to calculate the pooled average effect size
#' (and probably a few other small differences)

# now robust analysis
robust(x = metafor::rma(yi = contact_data$d, vi = contact_data$var_d), cluster = contact_data$unique_study_id)

# same as if we drop the clustering information
rma(yi = d, vi = var_d, data = contact_data)
```

You'll also notice that `map_robust` returns the total number of observations, where `metafor::robust(rma)` returns both the number of obsearvations and the number of clusters. We also wrote small wrapper around `map_robust` called `count_and_rovbust` that reports the number of clusters, which, in our papers, is equivalent to the number of studies:

```{r count_and_robust}

sv_data |> count_and_robust()
# vs
sv_data |> map_robust()
```

If you want to cluster at a different level, or call your cluster variable something different and modify the function accordingly, go ahead! Every meta-analysis is different, and we encourage you to to modify these functions to serve your own needs. (We typically have a `functions` folder in our replication archives where we keep all our custom functions, which is a nice way to keep your function files organized separately from your scripts.)

### subgroup analyses

The second main way we use `map_robust` and `count_and_robust` is via subgroup analyses. For instance, in "Preventing Sexual Violence," we wanted to compare the results on ideas-based outcomes to our major categories of behavioral outcomes (perpetration, victimization, bystander behaviors, and involvement behaviors.) To do this,we use the `map` function from the `purrr` library to split the dataset and apply a function (Hadley Wickham calls this  "[The Split-Apply-Combine Strategy for Data Analysis](https://vita.had.co.nz/papers/plyr.pdf)").

From hereon out we'll use `count_and_robust` rather than `map_robust` because we typically want the N of studies rather than of observations.

```{r split_by_behavioral_ideas_outcomes}
library(purrr)
sv_data |> split(~behavior_type) |> map(count_and_robust) |> bind_rows() 
```

A note about this: if your "group" (the thing you are running `purrr::split()` on) has only one data point, the resulting numbers won't be a pooled average effect, but instead will just be the ∆ and se and p-value of that individual estimate. If this comes up, you should explain that to your readers in a note on the table. 

### Making publication-ready tables out of subgroup analyses
If we apply a few more functions, we can actually get pretty close to a publication-ready table. 

`dplyr::bind_cols()` combines multiple data frames (in this case, a list of lists) into a single data frame.

```{r split_plus_bind_cols}

sv_data |> split(~behavior_type) |> map(count_and_robust) |> bind_rows(.id = "behavior_type")
```

Which you can turn into a variety of formats, e.g LaTex or markdown, via `knitr::kable()`:

```{r split_plus_kable}
library(knitr)
sv_data |> split(~behavior_type) |> map(count_and_robust) |> bind_rows() |> kable('markdown')
```

Here is a small function that add stars to text corresponding to p values:
```{r significance_stars}
get_significance_stars <- function(pval) {
  sapply(pval, function(x) {
    if (is.na(x)) {
      ""
    } else if (x < 0.001) {
      "***"
    } else if (x < 0.01) {
      "**"
    } else if (x < 0.05) {
      "*"
    } else {
      ""
    }
  })
}
```

Here we apply this function directly to the R code, and modify the columns to combine the ∆ and se into one column with the format `∆ (se)`: 

```{r closer_to_pub_ready_table}
sv_data |> split(~behavior_type) |> map(count_and_robust) |> bind_rows(.id = "behavior_type") |> 
  mutate(delta_se = sprintf("%.3f%s (%.3f)", 
                            Delta, 
                            get_significance_stars(pval), se)) |> 
  select(behavior_type, N_unique, delta_se)
```

Finally, we can use the "[great table"](https://gt.rstudio.com/)" library (`gt`) library to format everything nicely and add explanatory notes:

```{r pub_ready_table_with_gt}
library(gt) 
sv_data |> split(~behavior_type) |> map(count_and_robust) |> 
  bind_rows(.id = "behavior_type") |> 
  mutate(delta_se = sprintf("%.3f%s (%.3f)", 
                            Delta, 
                            get_significance_stars(pval), se)) |>
  select(behavior_type, N_unique, delta_se) |> 
gt() |>
  tab_header(
    title = "∆ by category of dependent variable") |>
  cols_label(
    behavior_type = "Behavior type",
    N_unique = "N (Studies)",
    delta_se = "Glass's ∆ (se)"
  ) |>
  tab_source_note(
    source_note = "* < 0.05, ** < 0.01, *** < 0.001.")
 
```

`gt` has a function called `as_latex()` that will convert this all into latex code directly. 

(By the way, the above table-making code is a little more sophisticated than anything we did in our  published meta-analyses to date, but the ideas were germinating during "Preventing Sexual Violence.")
