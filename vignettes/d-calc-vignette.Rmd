---
title: "d_calc: converting studies into d or ∆ estimates"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{d-calc-vignette}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

# Converting a study into a point estimate and its variance

## 1 Introduction

This vignette walks through the process of converting a study's results into an estimate of standardized mean difference (SMD), and its associated variance, using the functions `d_calc` and `var_d_calc`. We demonstrate with examples from our prior meta-analyses — mostly [The Contact Hypothesis Re-evaluated](https://doi.org/10.1017/bpp.2018.25), with a few examples drawn from [Preventing Sexual Violence —A Behavioral Problem Without a Behaviorally-Informed Solution](https://osf.io/preprints/psyarxiv/xgbzj).

The contact hypothesis meta-analytic dataset is included, with small modifications, in this package as `PaluckMetaSOP::contact_data`.

### 1.1 Standardized Mean Difference and Variance

SMD is the difference in average outcomes between the treatment and control groups divided by the standard deviation (SD) of the outcome.

$$SMD = \frac{M_1-M_2}{SD}$$

We calculate SMD via `d_calc`.

Once you have a study's SMD, you calculate its variance using `var_d_calc`. This tells you how precise your estimate is. The standard error of your SMD is the square root of its variance.

Meta-analysis takes a vector of SMD and a vector of standard errors as inputs and returns a pooled average effect and standard error. This procedure gives more weight to more precise estimates.

If you standardize your SMD with the estimated SD of the outcome for the entire population, your estimator is called [Cohen's d](https://resources.nu.edu/statsresources/cohensd). If you use an estimated SD from just the control group, you get [Glass's ∆](https://www.statisticshowto.com/glasss-delta/) (Delta). The Paluck lab generally prefers Glass's ∆ because we're interested in the efficacy of interventions relative to the population's baseline state.

When the SD of the control group isn't available but the SD of the entire population is, we use the SD of the entire population. When neither is available, or can't be figured out from a paper's results, we have a problem.

### 1.2 Estimating SMD can be hard because of how studies report their results.

Some studies report $M_1$, $M_2$, and $SD$, which makes calculating an SMD easy. Others report statistical summaries, which then need to be converted into SMDs using a variety of formulas. `d_calc` implements a selection of these formulas drawn from chapters 12 and 13 of [Cooper, Hedges, and Valentine (2009)](https://www.scholars.northwestern.edu/en/publications/the-handbook-of-research-synthesis-and-meta-analysis-2nd-edition).

In one case — the difference in proportions estimator — we depart from those equations, which we'll explain when we get to it.

Converting statistical results into SMDs is an topic of debate among researchers. In some cases, our conversion formulas are going to imperfectly capture the true SMD because 1) we don't have access to the raw data and 2) the reported results don't accuaretely capture the underlying distribution of outcomes — for instance, if results are only reported via multivariate regression, where some of the variance is accounted for via control variables.

The meta-analyst faces a trade-off here. On the one hand, you can discard all such studies, or exclude them from your main analyses, and limit your analysis to studies that make your job easy. On the other, you can use imperfect and potentially biased estimation procedures that nevertheless capture something *directionally* important about an intervention's effects. In general, we make all available efforts to figure out a "good enough" estimate of ∆, but if we really can't figure something out, we'll omit it or just include the study as part of a vote counting procedure.

See [this page on SMDs](https://handbook-5-1.cochrane.org/chapter_9/9_2_3_2_the_standardized_mean_difference.htm) from the Cochrane collaboration and the `escalc` documentation from Wolfgang Viechtbauer's `metafor` package for further reading.

## 2 the functions

### 2.1 `d_calc`

`d_calc` is a function that takes (up to) five parameters and returns an SMD. The first two are essential, and the next three are necessary depending on the value of the first parameter.

Those parameters are:

-   `stat_type` , where you put the kind of statistical results you're converting into SMD. its possible values are:

    -   `d` or `s_m_d`, for when a paper provides its own estimate of d/∆/SMD.

    -   `d_i_m` for difference in means.

    -   `d_i_d` for difference in differences.

    -   `reg_coef`, `regression`, or `beta`, for when you have the regression coefficient associated with treatment.

    -   `t_test` or `T-test` for when you have a t-test.

    -   `f_test` or `F-test` or `F` for when you have an F-test.

    -   `odds_ratio` for odds ratio.

    -   `log_odds_ratio` for log odds ratio.

    -   `d_i_p` for a difference in proportions.

    -   `unspecified_null` or `unspecified null` for when you can't figure out a precise estimate, but know that the overall effect was null.

-   `stat`, the value of the statistic that you're reporting or converting into SMD.

-   `sample_sd`, the standard deviation that you're using to standardize your estimate.

    -   This is generally a necessary input, but not when the stat type already tells you something about the variance of the outcome (`d`, `SMD`, `F test`, `t test`, `odds ratio`, or `log odds ratio`).

-   `n_t` and `n_c`, the sample sizes for treatment and control.

    -   These are necessary only for converting `F test` and `t test` results into SMDs.

Run `PaluckMetaSOP::d_calc` without parentheses to see all the conversion formulas.

### 2.1 `var_d_calc`

`var_d_calc` takes three inputs:

-   `d`, which is typically generated by `d_calc.`

-   `n_t` and `n_c`, the sample sizes for treatment and control.

The function first turns this into an (uncorrected) estimate of variance via

${variance_{uncorrected}} = (\frac{n_t + n_c}{n_t * n_c}) + (\frac{d^2}{2} * (n_t + n_c))$

And then applies a correction for small study variance called hedge's *g:*

$g = 1 - (3/((4 * (n_t + n_c - 2)) - 1))$

And the final variance estimate is

$variance = g^2 * variance_{uncorrected}$

## 3. Calculating SMD and variance using `d_calc` and `var_d_calc`

Some papers present results that make calculating an SMD easy.

**difference in means**: here is table X from Ditullio (1982), a study of workplace integration programs aimed at reducing prejudice towards people with developmental disabilities:

The SMD here is X1 minus X2 divided by SD. In code:

`ditullio_effects <- d_calc(stat_type = "d_i_m", stat = X1 - X2, sample_sd = SD)`

Its variance is

`ditullio_variance <- var_d_calc(d = ditullio_effects, n_t = whatever, n_c = whatever)`.

And its standard error is

`ditullio_se <- sqrt(ditullio_variance)`

**difference in differences:** Some studies present the mean outcomes for each group at both baseline and posttest. Here is table X from Dessel (2010):

In these cases, we subtract the baseline values from the posttest values for both treatment and control, and divide the resulting difference by the SD of the control group at baseline. This procedure creates more precise estimates of the mean difference between groups by controlling for underlying differences betweent the treatment and control groups.

In code:

`dessel_effects <- d_calc(stat_type = "d_i_d", stat = (YT_1-YT_0) - (Y0_1 - Y0_2), sample_sd = SD)`

(The equations for variance and standard error don't vary between types of statistical results, so we'll omit them from this point on.)

**regression coefficient/beta:**

In a bivariate (CHECK THIS) regression of the relationship between treatment status and outcome, the $\beta$ coefficient is is equivalent to the difference in means between the groups. Calculating the SD from a regression table can be tricky, but some studies report both $\beta$ and the standard deviation for the outcome. Here are tables X and Y from Marmaros (2006):

Converting these to SMD via code, we get:

`marmaros_effects <- d_calc(stat_type = "beta", stat = whatever, sample_sd = whatever)`

### Conversion formulas

Sometimes, papers report summary statistics that require more complex conversions to SMD estimates.

#### a novel estimator for difference in proportions

We try to avoid converting odds ratios or log odds ratios to SMDs. As Gomilla (2021) writes,

### What if you have nothing besides "efffects were not significant" or "there was no observed effect?"

For [The Contact Hypothesis Re-evaluated](https://doi.org/10.1017/bpp.2018.25), we had one study (whatever it was called YEAR) that didn't report its results with enough precision for us to say anything besides "there was a null effect." This seems to be a convention in some fields — if the results weren't statistically significant, report them with no statistical information. At the time, we dealt with this by including the study in what's called a vote counting procedure that tallies up statistically significant positive, statistically significant negative, and null results. Over the course of subsequent papers, however, we decided to start setting all of these cases to d = 0.01 so we can include them in our meta-analysis. You could set the null to 0.0, or 0.001, or anything that substantively communicates "there was no effect" and get basically the same result.

As code:

`paper_results <- d_calc(stat_type = "unspecified_null")`

And that's all the information you need to produce a result of 0.01.

### Taking a guess based on what's available

Let's work through some hard cases.

**Backing an estimate of SD from a regression table**

**Figuring out *your* outcome of interest when it's different than what the paper reports**

## how we use `d_calc` and `var_d_calc` in practice

When we wrote [The Contact Hypothesis Re-evaluated](https://doi.org/10.1017/bpp.2018.25), I (Seth) recorded every use of `d_calc` and `var_d_calc` for every study in a script, and then copied pasted the resulting values into our final dataset. That script looked like this:

```{r d_and_var_d_script, eval=F}


```

For our next two meta-analyses — Prejudice Reduction: Progress and Challenges and [Preventing Sexual Violence —A Behavioral Problem Without a Behaviorally-Informed Solution](https://osf.io/preprints/psyarxiv/xgbzj) — we incorporated `d_calc` and `var_d_calc` into our analysis scripts, This is a simplified version of our script:

```{r sv_d_var_d_se_d_creation_script, eval=F}

library(dplyr)

dat <- dat |>
    mutate(d = mapply(
          FUN = d_calc,
          stat_type = eff_type,
          stat =  u_s_d,
          sample_sd = ctrl_sd,
          n_t = n_t_post,
          n_c = n_c_post),
        var_d = mapply(
          FUN = var_d_calc,
          d = d,
          n_t = n_t_post,
          n_c = n_c_post)) |> 
    mutate(se_d = sqrt(var_d))

```

And this produces three new variables: `d`, `var_d`, and `se_d`. (The actual process was a bit more complicated owing to peculiarities in how we coded Ns for cluster-assigned vs individually assigned studies and a few other data-specific quirks, but this is the general idea.)

Here's the same script applied to this package's dataset:

```{r contact_d_se_var, eval=F}
dat <- PaluckMetaSOP::contact_data

library(dplyr)
dat_with_new_ds <-  dat |> 
  select(-c(d, se_d, var_d)) |>  # delete vars and then reproduce them
  mutate(d = mapply(
          FUN = d_calc,
          stat_type = statistic,
          stat =  unstand,
          sample_sd = sd_c,
          n_t = n_t,
          n_c = n_c),
        var_d = mapply(
          FUN = var_d_calc,
          d = d,
          n_t = n_t,
          n_c = n_c)) |> 
    mutate(se_d = sqrt(var_d))
# fix all the NAs! lot of dataset editing left to do

```

This approach will ultimately save the meta-analyst a fair bit of time and make more of their analysis computationally reproducible.

