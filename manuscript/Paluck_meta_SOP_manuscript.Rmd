---
title: "Towards meta-scientific meta-analyses: standard operating procedures for systematic reviews in the Paluck Lab"
shorttitle: "Paluck_meta_SOP"

author: 
  - name: "Seth Green"
    affiliation: "1"
    corresponding: yes
    address: "Kahneman-Treisman Center, Princeton University"
    email: "sag2212@columbia.edu"
    role:
      - "Conceptualization"
      - "Writing - Original Draft Preparation"
      - "Writing - Review & Editing"
  - name: "Elizabeth Levy Paluck"
    affiliation: "1"
    role:
      - "Writing - Review & Editing"
      - "Supervision"
  - name: "Roni Porat"
    affiliation: "2"
    role:
      - "Writing - Review & Editing"
      - "Supervision"
  
affiliation:
  - id: "1"
    institution: "Princeton University"
  - id: "2"
    institution: "Hebrew University, Jerusalem"

authornote: |
  This work was supported by [whoever supports it]

abstract: |
  This paper describes the motivations and procedures for meta-analyses by Betsy Levy Paluck and co-authors, with reference to and examples drawn from three such papers (Paluck, Green, and Green 2019, Paluck et al. 2020, and Porat et al. 2024). We first describe our conceptual aims when writing meta-analytic papers, noting where we think quantitative reviews add value and how we think through the trade-off between comprehensive and selective inclusion criteria. Second, we describe the typical 'flow' of the papers as a whole. Third, we describe our meta-analytic procedures, detailing our default analytic choices and introducing an R package that implements those choices. We conclude with some open questions for meta-analysts.
  
keywords: "meta-analysis, standard-operating-procedures, meta-science"
wordcount: "7431"

bibliography: "meta-sop-references.bib"

floatsintext: no
linenumbers: yes
draft: no
mask: no

figurelist: no
tablelist: no
footnotelist: no

classoption: "man"
output: papaja::apa6_pdf
---

## Introduction: An unexpectged gap between meta-analysis and meta-science

What purposes do meta-analyses serve above and beyond those accomplished by systematic reviews? Judging by recent papers in social psychology [@pettigrew2006; @bem2015; @cuddy2018; @bezrukova2016], the answer is to bolster and validate existing findings through aggregation. By pooling findings from many studies, researchers furnish meta-analytic estimates that are typically heartening to a literature's boosters because they are statistically precise and drawn from diverse settings, which gives readers the impression of durable, widely applicable findings. Such papers often have a valedictory quality to them: they provide leading practitioners a chance to survey the body of work they've helped inspire, highlight the strength of the evidence for their subfield's central tenets, and suggest future research directions that assume those tenets are no longer in need of testing.

Meanwhile, a parallel movement in psychology has begun to implement the recommendations of the open science [@nosek2015] and meta-science [@schooler2014] movements into study designs [@ferguson2023], journal policies [@hardwicke2023transparency], and literature reviews [@hardwicke2018]. Broadly speaking, meta-scientific papers aim to assess whether a literature's findings are *credible* rather than to confirm them. While meta-analyses seek to combine results, taken more or less at face value, meta-science papers, by contrast, ask whether the designs, analyses and implementations within a literature license its authors' inferences.

Some meta-scientific reviews quantitatively assess collective outcomes, for instance, what percentage of a subset of papers are computationally reproducible [@obels2020] or consistent with open data standards [@hardwicke2021]. In one sense, these estimates are 'meta-analytic,' but they aren't meta-analyses in the conventional sense of pooling existing findings into weighted average effects [@glass1977; @kleinstauber1996; @cooper2019]. In tone, meta-science papers are often much more critical than meta-analyses are, and they are often written by outsiders or newcomers to an academic field who appear more motivated to interrogate than to validate. Thus, their similar names notwithstanding, meta-analytic and meta-scientific papers in social psychology typically have radically different perspectives, assumptions, and goals.

At the Paluck lab, we seek to synthesize these two approaches by writing meta-analyses that are fundamentally meta-scientific. Whil writing three such papers [@paluck2009; @paluck2019; @paluck2021; @porat2024], we've developed distinct beliefs about where and when a meta-analysis is useful, how to structure our papers, and which specifications are sensible defaults.  This paper articulates those beliefs, and aims to illuminate how meta-analysis might become a standard component of the meta-scientific toolkit.

### Bridging the gap between meta-analytic theory and Standard Operating Procedures 

A healthy research methods literature has interrogated meta-analystic methods, noting common pitfalls (e.g. @greco2013), statistical errors [@kadlec2023], and practical limitations, e.g. a widespread lack of reproducibility [@lakens2017]. This paper sits in that tradition, but is a bit of an offshoot in that we are not attempting to correct statistical or practical mistakes in meta-analyses, but rather articulate what it is that we are trying to do when we write them. In this regard, this paper is closely in dialogue with two recent contributions that attempt to put meta-analysis on firmer theoretical footing.

@slough2023 "highlight the dangers of conflating conceptual differences across studies with statistical sources of variation" (p. 29). They argue that for studies to "identify the same empirical target" (p. 1), they must have fundamentally *harmonized* contrasts and measurement strategies, i.e. the "substantive comparison across studies is the same" and "the outcome of interest is the same and it is measured in the same way" (p. 2). Absent these conditions, meta-analytic results might not be "meaningful and interpretable" (p. 2). Further,  target equivalence cannot be achieved "solely with statistical techniques" (p. 23); it follows instead from the use of proper "design or inclusion criteria" (p. 2).

We agree that an internally and externally valid causal meta-analytic estimate requires harmonization of contrasts and outcomes, as well as a high degree of internal validity within constituent studies. However, we also think that meta-analytic estimates which do *not* meet these conditions \textemdash those that combine results from many different designs, or whose diverse outcomes bear an unknown relationship to the true outcome of interest \textemdash can still be useful and informative if interpreted and framed correctly. We think that when a literature without clear harmony of the independent and dependent variables is a case where the underlying model might be "wrong" but still useful.

The other paper in this vein we look to is @simonsohn2022, who argue that while the typical meta-analysis in social psychology aims "to be comprehensive, results-focused and transcriptive," this is "misguided, leading to uninterpretable results that misrepresent research literatures." First, they argue that a desire for comprehensiveness typically leads the meta- analyst to average "studies of the highest quality" with "studies that lack internal validity or external validity, which are obtained using incorrect statistical techniques, or studies where results seem to arise from methodological artefacts." This throw-it-all-in-the-blender approach produces results that are "virtually guaranteed to lack a meaningful interpretation," despite their putative statistical precision. Second, the authors encourage meta-analysts to focus on studies' designs, rather than just their results, to provide "readers with information that they can use to evaluate" the quality of a research literature for themselves. Rather than describing every study, meta-analysts "can succinctly describe and summarize common design failures...and provide detailed descriptions only of the studies they deem most important or compelling." This advice ties closely to their last dictum to meta-analysts: to evaluate rather than to transcribe, i.e., detail the evidence from the very best studies, identify common methodological shortcomings of the literature as a whole, and, finally, "discuss what open questions remain and what kinds of evidence would help answer those questions."

Our meta-analyses share these goals (and @paluck2021 is cited by that paper for being "selective, design-focused, and evaluative"). However, as mentioned above, we part ways from this paper in that we believe comprehensive meta-analytic estimates still have a place in the meta-scientist's toolkit, so long as those estimates are ascribed descriptive rather than causal meaning and are used to contextualize more selective analyses.

This paper also builds on a tradition of Standard Operating Procedures (SOPs) in the behavioral and medical sciences [@rapport2013; @wallace2015; @lakens2023]. In particular, we look to @lin2016, which details how the Green lab at Columbia sets "default practices to guide decisions when issues arise that were not anticipated" in pre-analysis plans. Like that paper, this article is not intended as a comprehensive guide to the task in question \textemdash see @cumpston2019 and @cooper2019 for textbook-length treatments of meta-analysis, and @frank2024, chapter 16, and @lakens2022, chapter 11 for modern treatments of the subject aimed at social scientists. Rather, we articulate our lab's approach to this class of paper. We hope this will prove interesting and informative, but not definitive: we expect readers to adapt and expand our guidelines to meet their own needs. In general, we hope to see more labs publish papers about how they produce knowledge.

### The remainder of this paper

This paper is a hybrid of a conceptual analysis of meta-analyses in general and a report on how we in particular write them. The first section articulates our general approach to meta-analysis. The second describes our typical approach to writing meta-analytic papers. The third details our meta-analytic defaults and introduces an R packaege, `PaluckMetaSOP`, that implements them and helps us write papers. Our conclusion recaps and notes some open questions for our lab.

First, we describe our conceptual approach to meta-analysis. We believe that meta-analyses are useful for evaluating the strengths and weaknesses of large, heterogeneous literatures, especially those that bring multiple theoretical perspectives to bear. We further believe that condensing each study down to a point estimate (or cluster of estimates) and associated variance(s) can furnish useful meta-scientific quantities. We also detail our thinking about the trade-off between comprehensive, potentially biased meta-analytic estimates and focused but also narrower quantitative reviews that set strict inclusion criteria around study design, interventions, and/or dependent variables. Contrary to critics who argue that core assumptions about homogeneity across experimental contrasts and outcomes are necessary for the standard meta-analytic model to hold [@slough2023], we think that comprehensive, heterogeneous meta-analyses can still be useful when these conditions aren't met; in such cases, meta-analytic estimates should be treated as *descriptive* rather than *causal* inferences. Moreover, we see value in contrasting all-inclusive estimates, which are fundamentally descriptive, to more rigorous, focused estimates, which, because they reflect strict selection criteria related to study design and outcome measurement, are more plausibly causal. The within-paper contrast in magnitude and precision between these two approaches is often startling and illuminating. In general, we favor comprehensive meta-analyses if resources permit, but more focused reviews can also be illuminating.

Second, we describe our default paper structures and analytic choices when writing meta-analyses. In general, our papers explain the main ideas and trends in a literature, our search processes and meta-analytic methods, meta-analytic findings,a nd a review of open questions. Our quantitative analyses are structured to start big and go small. We first present meta-analytic estimates and tests for publication bias for the entire literature and then hone in on key quantities of interest from smaller subsets of the data. Our concluding investigations tend to probe differences in effect size by some marker of study quality, e.g. the presence of a pre-analysis plan or sample size.

Third, we detail our meta-analytic defaults, e.g. a preference for Glass's $\Delta$ over Cohen's _d_, random effects over fixed effects estimates, and clustering our standard errors at the level of a study (as opposed to a paper or a team of authors). Here, we also detail a novel estimator for converting difference in proportion to an effect size that has some desirable properties relative to the conventional conversion from an odds ratio [@gomila2021]. Here we introduce an R package, `PaluckMetaSOP`, that contains functions to help us quickly and efficiently perform meta-analyses.

Fourth and finally, we conclude by noting some open questions for meta-scientific meta-analyses. While we tend to begin meta-analytic projects with a sense of what's true and what's interesting, in practice, the most incisive questions often emerge after reading and coding studies, i.e. after data collection has begun. In other words, meta-analyses are both *hypothesis_driven* (re: deductive) and *data-driven* (re: inductive); However, typical checks on researcher degrees of freedom assume a hypothesis-driven framework. This makes it tricky to "tie one's hands to the mast" [@elster1977], e.g., by writing a detailed pre-analysis plan and pre-specifying which outcomes to focus on. Last, we discuss the difficulty of integrating indirect cues of study quality, e.g. obviously mistaken statistical results or special attention devoted to reproducibility, into our analyses in a principled way.

We now detail our conceptual aims when we write meta-analyses.

## The purposes and contributions of meta-analysis

The main goals of our meta-analysis are the goals of any systematic review: to evaluate a body of knowledge for its strengths, weaknesses, central findings, open questions and empirical limitations. We write broadly for any reader who is interested in the underlying question, but generally aim for a non-specialist audience. In our experience, a literature's most noteworthy contributions often come from researchers outside the core field (e.g. @mousa2020, @boisjoly2006, or @scacco2018 for the contact hypothesis, @munger2017 for anti-prejudice work, or @haushofer2019 for violence against women); to such researchers, we aim to provide signposts, in the form of unearthed, widespread empirical gaps and theoretical limitations, about where future empirical work will be most valuable. We also believe that grant-making institutuions and policymakers will generally benefit from systematic, critical reviews that do not take subject matter expertise for granted.

In general, large, heterogeneous literatures, especially those with multiple theoretical perspectives, are good candidates for meta-analysis. In such cases, a meta-analysis might provide a useful contribution simply by dividing a literature into its constituent parts. For example, @paluck2021 categorizes 418 experiments published between 2007 and 2019 and organizes them into thirteen theoretical approaches and twelve categories of prejudice targeted. Simply identifying this heterogeneity of approaches and outcomes, along with describing a few constituent studies from the major theoretical strands in detail, helps the reader evaluate a literature's major findings for herself. In this particular case, a reader might reasonably wonder: does an intervention that aims to change political attitudes towards the rights of trans persons [@broockman2016] shed light on what might reduce tensions between castes in India [@lowe2021]? Is an extended contact intervention [@shamoa2023] similar enough to a diversity training [@chang2019] that you can pool the effects together into something that coherently reflects the effects of a class of intervention? These questions come down to beliefs, and, as @slough2023 argue, cannot be resolved with statistics. A comprehensive and well-organized review is an indispensable tool for clarifying which questions to ask in the first place.

We also encourage authors not to be discouraged from writing meta-analyses of subjects that have been previously reviewed and analyzed, even many times. First, in our experience, simply reproducing the work of previous meta-analyses verbatim might generate surprises. For example, Pettigrew & Troop's landmark meta-analysis [-@pettigrew2006] identified 515 studies and 36 "true experiments" (p. 759) on intergroup contact; however, when we took a look at these papers for ourselves, we found that many "were mislabeled as randomly assigned, did not feature 'actual face-to-face interaction' or did not have a non-contact control group;" between these issues and papers that lacked long-term outcome measurement, we ended up with nine experiments that met our inclusion criteria. Second, we share Munger's [-@munger2023]  concern for "temporal validity," the idea that because the conditions under which findings hold might change dramatically going forward, "no research design, no empirical knowledge, is perfectible" (p. 1). For instance, the implicit bias [@gawronski2019] and symbolic racism [@mcconahay1976; @sears2003] research agendas emerged because researchers observed a dramatic drop in how readily white Americans would express overtly racist ideas in the 1970s and 80s, which called for a paradigm shift in measurement. Meta-analyses are essential for assessing which classes of interventions still 'work' in light of such paradigm shifts. Further, as the credibility revolution spreads across the social sciences [@angrist2010; @samii2016; @vazire2018], we can generally expect recent research to meet comparatively higher standards of rigor and transparency, and therefore have a disproportionate impact on our understanding of the world. Fourth, even in fields with many systematic reviews, there's still often an open lane for a paper that's laser-focused on the findings of the best, most policy-relevant research [@paluck2019] or that's both comprehensive and quantitative [@porat2024; @paluck2021].

### Why meta-analysis and not just a systematic review?

The aims we articulated above could all be met by a non-quantitative systematic review. However, We think that meta-analysis can help illuminate several meta-scientific inferences.

The first is publication bias. Classical approaches to assessing publication bias in meta-analysis are "based on the fact that precision in estimating the underlying treatment effect will increase as the sample size of component studies increases" [@egger1997]. The funnel plot detects publication bias under the assumption that smaller, more imprecisely estimated studies that produce null or backlash results are more likely to be shelved than large, precisely estimated studies that produce the same. (The underlying theory of researcher behavior seems to be a kind of widespread acceptance of the sunk cost fallacy \textemdash that once you've invested the kind of time and energy that a large, rigorous study requires, you'll push through to publication even if you'd put the results back in the 'file drawer' [@rosenthal1979] if you'd put less work in.) Alternatively, one can simply plot the relationship between effect size and standard error, as we did in @paluck2019, where the strong positive relationship between the two suggested that "a very large study would be expected to produce a minuscule *increase* in prejudice." These tests require estimates of effect size and variance for each study. Further, estimates of publication bias can be interesting *between* metas, for instance, the evidence of publication bias that we found in our 2019 and 2021 papers but did not find in @porat2024, which drew more from public health perspectives and departments than did the previous two papers. This kind of evidence can shed light on whether (and how) the credibility revolution is taking hold across disciplines \textemdash a meta-meta-scientific inference.

Second, quantitative estimates allow for head-to-head tests of efficacy across different classes of interventions. For example, @paluck2021 estimated that the "20 experiments testing antibias, multicultural, and moral education programs" have an average effect size of *d* = 0.30, compared to *d* = 0.43 for 12 entertainment interventions. This does not necessarily mean that entertainment interventions reduce prejudice more effectively than antibias trainiings do \textemdash there are many confounding differences between these two categories of study \textemdash but the differences do tell the reader something about how effectively people in different disciplines are manipulating the key quantity of interest by their own lights.

Third, relationships between effect and study quality, measured in a variety of ways, are often illuminating. We provide some examples in section three of this paper. 

Fourth, quantitative estimates allow for assessing differences in effect size between literatures. In general, we don't ascribe any particular meaning to small differences in estimated effect size \textemdash the difference between d = 0.3 vs d = 0.28 might mean something in some disciplines, but given the prevalence of measurement error in the fields we've analyzed [@schmidt1996; @cook2002], we would call this difference substantively insignificant. However, *large* differences between literatures raise some interesting questions. The three meta-analyses discussed in this paper all found overall effect sizes between 0.28 and 0.357: a relatively small range, whereas @green2024, which sets relatively stringent inclusion criteria for both independent and dependent variables, finds an average effect size of 0.131 of interventions intended to reduce consumption of meat and animal products. Again, this is not a well-identified effect; these literatures diverge on many dimensions. But the difference in effect sizes between a narrow, selective meta-analysis and its bigger siblings suggests that there are systematic differences between the literatures, and the researchers, that are worth exploring.

### The trade-off between comprehensiveness and narrow, focused reviews

#### Addressing contemporary criticisms by relaxing the causality assumption
@slough2023 and @simonsohn2022 have raised trenchant critiques about whether the typical meta-analytic estimate is meaningful. Both papers argue that meta-analyses should be more selective in what papers they include \textemdash @slough2023 to assure harmonization of the independent and dependent variables, and @simonsohn2022 to prevent mixing "studies of the highest quality" with "studies that lack internal validity."

Of our three meta-analyses, @paluck2019 most closely hews to this model of inquiry. That paper looked solely at randomized controlled trials where intergroup contact was the treatment variable and where there was at least a day of delay between the commencement of treatment and outcome measurement. (Admittedly we could have tightened our study even further by looking for harmonized dependent variables, but given the diversity of outcomes we found, that would have effectively precluded meta-analysis.) Our subsequent two meta-analyses [@paluck2021; @porat2024], are much more clearly at risk of the kinds of incoherence that recent critics identify. Both studies include a mixture of experiments, quasi-experiments, and observational designs (so long as there was a control group with pre- and post-treatment outcomes); combine papers that are drawing from vastly different theoretical traditions, and mix divergent dependent variables.

Although we haven't seen the issue portrayed in these terms, each of these is a bias-variance trade-off. Conventional meta-analyses consistently resolve this trade-off in favor of reduced variance with only minimal attention to concordant bias. When this challenge *is* discussed, it's typically framed in terms of a desirable trade between internal and external validity: what's lost in terms of unbiased causal inference is appropriately compensated for by gains in knowledge about the diversity of settings in which a treatment works. (See @mathur2022 for discussion.)

In general, we do not find this argument persuasive.As @thye2000 puts it, "if there are doubts or questions about whether a relationship is real or spurious, then whether or not the finding applies to other settings is irrelevant" (p. 1303). There is a reasonable counterargument that if randomized controlled trials and quasi-experimental/observational designs tend to yield roughly equivalent effect sizes, this would ameliorate concerns about potential confounders or reverse causality in the observational literature. (Indeed, @pettigrew2006 argue this, but as mentioned above, when we re-examined the assembled RCTs, we found them much too scant to say anything definitive.)

Either way, this does not address potential bias introduced by non-harmonized treatments and outcomes. The threat there is not from confounders, but the much more conceptual problem that it simply doesn't make sense to pool what's being pooled. Consider the sexual violence literature, which includes, for instance, a building-based intervention comprising "higher levels of faculty/security presence in safe/unsafe 'hot spots' mapped by students" [@taylor2013], and a module of peer-based consent training on a college campus [@crane2017]. When you average these two things, what does the resulting quantity signify? Ascribing a causal meaning to that average \textemdash saying that both interventions are tests of the same core ideas, just in different settings \textemdash requires some strong additional assumptions. Likewise with dependent variables; in that paper, we coded the Sexual Experiences Survey [@koss1982, @koss1985] for behavioral outcomes and the Illinois Rape Myth Acceptance Scale [@thelan2022] for ideas-based outcomes when they were available, but the nearest substitutes for each, in our assessment, when they weren't. What additional assumptions do we need to argue that these multitudinous outcomes are coherently integrable?

We take a different tack: to remove the assumption that a pooled meta-analytic effect should be considered causal. Instead, when we combine studies from heterogeneous designs, treatments, and outcomes, the pooled estimate is a *descriptive* statistic representing the average size of changes measured by a group of researchers, according to them and with outcomes they chose. For @paluck2021, thus number represented the average effects found by prejudice researchers between 2007 and 2019; for @porat2024, the average effects found by sexual violence researchers between 1986 and 2018.

We were not as clear about the distinction between descriptive and causal analyses as we should have been in our three previous meta-analyses. Our thinking on this matter has evolved over time and we aim to be clearer in future papers.

#### The case for inclusive and comprehensive meta-analyses
We think there is value in inclusive searches and analyses, even if the resulting pooled estimate is not causal. First, the pooled estimate provides a benchmark for all subsequent analyses, including those with sufficient internal validity and harmonization of treatments and outcomes to be plausibly causal. For example, the pooled effect observed in @porat2024 is $\Delta$ = 0.28, but just 0.086 for perpetration outcomes with randomized designs; of the 28 studies meeting these stricter inclusion criteria, 16 self-reported null results. We think this gap between the superficially encouraging results of the literature as a whole and the bleaker results of the best studies looking at the most crucial outcome in the primary prevention literature is a call to action to researchers. Whatever solace they take in the putative effects of the entire literature should be weighed against the scant evidence of change on perpetration outcomes. The contrast helps build the intuition that the literature is in need of a change of direction.

Second, coding more studies leads to more representative estimates of publication bias.

Third, how well different outcomes are harmonized or not is, in some cases, testable. In @porat2024, the central methodological divide was between studies that measured ideas-based outcomes and those that measured behaviors (a small subset measured both). We found an overall random effects estimates of $\Delta$ = 0.071 for behavioral outcomes and $\Delta$ = 0.366 for ideas-based results. This divergence, we think, is face value evidence that these outcomes represent different theoretical quantities and therefore cannot be combined into a causal analysis. Researchers can help readers come to their own conclusion by meta-analyzing data subsetted by dependent variable and present the results side by side. 

Fourth, coding a wide variety of interventions allows for subsequent analyses to focus on the effects of particular interventions, or classes of intervention, and comparing them. Again, the overall meta-analytic estimate provides a backdrop to these more focused analyses.

Fifth, starting from a precise estimate, gleaned from hundreds of studies, and then moving to a much higher quality but sparser dataset of the very best studies, is an effective narrative device. This device can also work between papers. Consider the contrast provided by @pettigrew2006 and @paluck2019. The former paper observes that across hundreds of studies comprising hundreds of thousands of people, contact typically reduces prejudice. The latter paper, however, notes that as of 2018, there had been zero evaluations of interracial contact in adults that had both random assignment and a delay of at least a single day between the beginning of treatment and outcome measurement. In other words, some essential basic research was missing. The apparent solidity of the first claim makes the second all the more startling.

In sum, we see value to identifying, coding, and heterogeneous estimates from study designs, interventions, and outcomes. However, a focused meta-analysis with strict inclusion criteria can also have appreciable impact on the field. So when is one strategy or the other appropriate?

We favor comprehensive meta-analyses if resources allow. On the one hand, these projects are much larger undertakings: @porat2024 took six years, five authors, and a team of RAs, whereas @paluck2019 was basically written in a summer by three people (though identifying relevant studies took years). On the other hand, comprehensive meta-analyses help authors develop, and then impart to readers, a truly thorough understanding of the literature. For "The Leading Approach to Reducing Sexual Violence" in @porat2024, there was simply no substitute for reading hundreds of studies. (We'd also add that quantitative coding studies is a kind of enforcement mechanism for reading a paper closely.) Further, a more focused analysis of the very best studies can be encompassed within a comprehensive meta-analysis: the dataset assembled in @paluck2021 could (hypothetically) be filtered to reproduce the dataset of @paluck2019 precisely, but the revers, of course, is not true.

However, a more focused meta-analysis can still be a very useful contribution, especially as a rejoinder to prior credulous reviews. We recommend that such papers choose inclusion criteria that select for validity on three fronts: design, contrast, and measurement. For example, @green2024 analyze interventions intended to reduce consumption of meat and animal products (MAP) and look solely at randomized controlled trials that 1) measure MAP directly 2) at least a single day after treatment was administered with 3) at least 25 subjects in treatment and control (or 10 clusters for cluster-randomized trials). This has the pragmatic benefit of winnowing a literature with literally thousands of studies down to a few dozen, which makes for a much more tractable analysis and paper-writing process.  

We now turn to the pragmatic section of our paper, where we detail the typical structure of our meta-analytic papers, our analytic defaults, and an R package that implements those defaults. 

## Default structure and analytic choices for Paluck lab meta-analytic papers

We do not expect anything in this section to be groundbreaking or novel. However, we think there is pedagogical value in explicitly sharing a lab's approach to a class of paper. We hope to see more papers in this veins as a way of identifying and disseminating best practices. We also note that each paper was tailored to its venue and there is some variation between papers about where each of the following sections occurs. 

### How our meta-analyses typically unfold

our meta-analyses' introductions describe the context and stakes of the paper. @paluck2019 and @porat2024 cover the major theoretical developments in their respective literatures, whereas @paluck2021 describes the need for an update on the major developments since a previous review [@paluck2009]. @porat2024 focuses particularly on 'zeitgeist' studies and ideas, which we think works nicely as a framing device for subsequent quantitative analyses, especially if some of those zeitgeist notions are revealed to have a surprising lack of evidence for efficacy.

We then typically provide details about our database of studies and how we assembled it. These sections are pretty stanndard for meta-analyses, e.g. including our PRISMA diagrams [@moher2009], search terms, where and when our studies took place, and criteria for study collection.

Next, we detail our meta-analytic methods. These sections are not intended as exhaustive treatments of meta-analysis, but rather to detail a few important choices, e.g. how we selected dependent variables [@paluck2019], our contrasts between overall results and those from "studies with reasonably good statistical precision" [@paluck2021, p. 14.7], and how readers can access our original datasets and reproduce our analyses (all three papers). 

Next, we come to the main course: our meta-analyses. These sections are tailored to their literatures, and so follow distinct paths. However, we can describe a few general guiding principles.

First, we typically start big and then go small, meaning we start by meta-analyzing everything and then zoom in on different subsets of the literature. As we argued above, we think that starting big is an effective framing device for our more focused analyses. We typically follow this with various tests for publication bias, e.g. a plot of effect sizes by standard errors, a statistical summary of the relationship between the two, and tests for systematic differences in effect sizes found in published or unpublished studies.

Our more focused analyses typically apply the "split-apply-combine" framework for data analysis [@wickham2011]. For instance, in a literature with randomized controlled trials, quasi-experiments, and observational designs, as well as a mix of attitudinal and behavioral outcomes, we would present a 3 by 2 table where each subset of study design + outcome strategy gets its own meta-analytic estimate [table 2 in @porat2024]. Alternatively, in a literature with many similar but distinct quantities of interest, e.g. racial/ethnic prejudice vs. prejudice against LGBTQ+ people vs. prejudice against people with physical and mental disabilities, we would present the meta-analytic effects within each subset of data. These sub-analyses aim to identify two meta-scientific desiderata. The first is places where basic research has not yet been done, e.g. the finding of zero evaluations of interracial contact in adults with both random assignment and a measurement delay in @paluck2019. The second is the  data subsets that have sufficient contrast measurement harmonization to have a plausibly causal interpretation. For instance, @porat2024 assessed the effects of interventions with a bystander component [@banyard2004; @banyard2007] separately, and found null results on perpetration ($\Delta$ = 0.019) and victimization ($\Delta$ =  -0.009) outcomes. This suggests "that nearly 1 in 3 studies in our database is pursuing a theory of behavior change that is not grounded in reality" (p. XXX). 

Finally, we aim to include tests of the relationship between study quality and effect size. @paluck2019, for instance, found that studies with pre-analysis plans had an average effect size of 0.016, vs. 0.451 for everything else. This is not a well-identified estimate of the 'effect' of pre-analysis plans \textemdash pre-analysis plans were not randomly assigned, and many other things varied between these studies \textemdash but it is at least a 'hoop test' [@collier2011] for the hypothesis that p-hacking is a problem in the contact hypothesis literature. Likewise, @paluck2021 found a highly significant relationship between meta-analytic effect size and sample sizes, with the smallest quintile of studies producing an average effect of *d* = 0.61 vs *d* = 0.19 for the largest quintile. This is, again, not a well-identified test of the effect of increasing sample size, but it does suggest that the literature's "results are not robust to the most basic assessments of study quality" (p. 149). 

In a related vein, @porat2024 tested whether changes in ideas predicted changes in behaviors in studies that measured both, and found a disappointing lack of correlation between the two. This suggests that these classes of outcomes are not fundamentally assessing the same theoretical quantity, and that the paradigm on which this literature is based \textemdash myths about sexual violence cause sexual violence, so changing the myths should reduce the violence \textemdash does not seem to hold.

This typically concludes our meta-analytic sections, at which point we move to a discussion of open questions and next steps. Here we write specifically for researchers. In @porat2024, we highlight positive developments in measurement strategies as well as some viable theories of behavioral change that have yet to be tested. In @paluck2019, we encourage researchers to test which of the moderating conditions proposed in @allport1954 are most essential for anti-prejudicial effects, and call on researchers to fill in important gaps in basic research gaps. In @paluck2021, we discuss some concerning trends in measurement and design, and then celebrate seven 'landmark' studies. These papers "are exceptionally well-designed and executed" and "provide a glimpse of what a meta-analysis would reveal if we could weight studies by quality as well as quantity" (p. 14.19). The intention here is to provide guidance to researchers who are motivated to produce research of the highest quality. 

Further, because our meta-analyses tend to produce more equivocal evidence of the efficacy of research programs than that furnished by other meta-analyses, our papers provide an intellectual backdrop to researchers who find inconclusive or null results that might run counter to a field's main findings. For example, @scacco2018 and @mousa2020 ran landmark contact hypothesis papers that found much less salubrious effects than the literature's boosters might have expected; we hope that the analyses in @paluck2019 helped make these results seem more legible, and less like statistical anomalies, to readers.

### default meta-analytic procedures in Paluck lab
Our meta-analytic procedures vary somewhat between papers. However, a few guuding principles have held constant, and we've also developed some techniques that we think are helpful for solving methodological issues.

First, at the level of condensing studies to estimates of effect size and variance, we use equations found in @cooper2019 to calculate standardized mean differences (SMD) \textemdash an average treatment effect divided by some measure of standard deviation of the dependent variable. While some literatures might all look at the same dependent variable, or all present the same statistical outcome (e.g. an odds ratio), nothing we've ever meta-analyzed has.

When possible, we prefer Glass's $\Delta$ to Cohen's _d_. The difference is that $\Delta$ standardizes by the standard deviation of the dependent variable for the control group rather than the entire population. our thinking here is that the experimental treatment might change some aspect of the distribution of outcomes for the treated subjects, whereas what we seek to know is the effect of treatment in terms of the underlying distribution of the untreated population. (In practice, standard deviations between treatment and control groups tend to be very similar in papers we've analyzed).

When converting event outcomes, e.g. how often a sample of people reports perpetrating some behavior, to Glass's $\Delta$, we treat each event as draws from a Bernoulli distribution and convert accordingly. The variance of a Bernoulli distribution is $p (1-p)$, which yields a standard deviation of $\sqrt{p (1-p)}$. If some event occurred in 15 out of 100 people in the control group and 10 out of 100 people in the treatment group, our estimate of Glass's $\Delta$ = $\frac{.1 - .15}{sqrt{.15 * (1 - .15)}} = -0.14$. We think this estimator has some desirable theoretical properties relative to conventional methods of converting odds ratios or log odds ratios to SMDs; we elaborate more on this in an appendix to @porat2024. 

In terms of aggregating studies for meta-analysis, we default to random effects estimates rather than fixed effects because in the heterogeneous literatures we analyze, we do not think there is one "true" effect size for which we are searching, but rather a variety of true effect sizes moderated by populations, conditions, and intervention strategies. We cluster standard errors at the level of study, rather than paper or team of authors or lab group, because we assume that the bulk of correlation between outcomes takes place within studies. 

We have written a collection of functions to help implement these defaults, and write papers that use them, called `PaluckMetaSOP,` available at <http://www.github.com/setgree/PaluckMetaSOP>. These functions fall into three categories.

First, we have functions that help convert unstandardized effect sizes into standardized mean differences. `d_calc.R` takes statistical results, sample sizes, a measure of standard deviation, and a class of statistical test and returns an SMD. `var_d_calc.R`  does the same thing for estimates of variance and standard error. `dip_calc` is a helper function that does so for difference in proportions estimates.

Next we have functions that help us report meta-analytic results. `map_robust.R` is a wrapper around the core meta-analytic functions in the `metafor` package [@viechtbauer2010] that integrates well into the `purrr` package from `tidyverse` [@wickham2019], and therefore helps us implement the split-apply-combine framework [@wickham2011]; `study_count.R`, `sum_lm.R` and `sum_tab.R` do the same for existing functions that count the studies in subsets, compute and report linear regression results, and report frequency tables. 

Finally, we have functions that make writing meta-analysis papers easier. This paper was drafted as an Rmarkdown file using the `papaja` package [@R-papaja], and we've written some additional functions that, for instance, combine Rmarkdown rendering, `git add` and `git push` into a single command; convert DOIs to a bibliography; and record a computational environment as a Dockerfile.

Please see the package's vignette on <http://www.github.com/setgree/PaluckMetaSOP> for detailed usage instructions with reference to a demonstrative subset of the data from @paluck2021. 

## Conclusions: hard cases

A classic paper on meta-analysis defines the procedure as "a statistical analysis which combines or integrates the results of several independent clinical trials considered by the analyst to be â€˜combinable'" (p. XXX). Our (non-systematic) review of prior meta-analyses reveals a surprising dearth of attention to how much theoretical work is being done by the phrase "considered by the analyst to be 'combinable.'" @simonsohn2022 argue that researchers who "uncritically" aggregate results from all available studies, as part of an "active effort to eliminate publication bias...might instead amplify the bias from poor research design and execution." We agree with this assessment, but would instead frame it as one of three central instances of a bias-variance tradeoff afflicting meta-analayses. By combining studies with non-comparable designs, interventions, and outcomes, researchers give readers the impression of remarkable precision (re: reduced variance) but at the expense of introducing non-statistical sources of uncertainty (re: bias) on all three fronts. Only by acknowledging these tradeoffs head-on, and devising interpretations and techniques for dealing with them, can we successfully integrate the parallel tracks of meta-analysis and meta-science.

More broadly, this paper seeks to answer two questions: what are we aiming to do when we do meta-analysis, and how do we do it? We that that we've answered these questions to some extent, but many open questions remain. To us, the most pressing issues arise from a lack of clarity about whether meta-analyses are or are not hypothesis-driven. Other meta-analyses in the social sciences are clearly aimed at validating a pre-existing hypothesis. Ours, however, are more exploratory: we aim for evaluation rather than validation. While we might start with some general hypotheses \textemdash e.g. that a social psychology literature from the 2000s is likely to show evidence of selection pressures for statistical significance \textemdash we also develop many of our most important questions after reading papers, which, in the context of writing a meta-analysis, means after data collection has begun. In other words, our meta-analyses are both inductive and deductive inquiries.

This makes writing a pre-analysis plan challenging. Consider a mistake we made while composing the pre-analysis plan for @porat2024. Because we are accustomed to social psychology literatures, we registered the analysis that we would "Evaluate the overall effect size of behavioral outcomes for all of the studies in the dataset, both in the short- and in the long-term." The idea was to measure if effects of interventions attenuate over time, which is a serious concern in psychology. However, what we failed to think through is that while some behavioral outcomes in the sexual violence literature could be measured immediately (e.g. whether someone volunteers for a campus rape education organization [@gillies1997]), most behavioral outcomes in the sexual violence literature need time to accumulate. The Sexual Experiences Survey (SES) measures incidents of sexual violence in a given time period, and an SES administered at the end of a semester was essentially guaranteed to have more incidents than one given immediately after an intervention concluded. This only became clear to us after we began reading papers (though someone more expert than we are could have seen it coming).

The way we resolved this was to make sure that we included, either in the paper or in an appendix, every single pre-registered analysis, even when they were not especially informative. Moving forward, we aim to have pre-specified analysis code run on simulated data [@broockman2016; @blair2019]. However, this is still a work in progress at our lab.

Second, we sometimes encounter methodological errors, or simple implausibilities, in the literature that are difficult to integrate into formal inclusion and exclusion criteria. Should we exclude a study because it has an implausibly *large* effect size, e.g. a t-test value of 36? (One study in the prejudice reduction literature did.) What about errors in the text, or obvious discrepancies between results reported in the text and those in a table? What about a lack of transparent reporting that leads to our spending hours guessing a study's true effects? Each of these is information about a study's true quality; as @simonsohn2022 argue, excluding these studies would be "kind of meritocratic screening of research" that we "engage in when performing virtually every other task in [our] professional research lives."

There are clearly no easy answers to this problem, and we've developed general guidelines as we've progressed. One is that cluster RCTs with fewer than 10 clusters in total are so underpowered that they are effectively quasi-experiments, and we code them as such. Another is that we do not code studies where we need to eyeball a figure to assess effect sizes: researchers must tell us flat out how big an effect was. Likewise, we do not include "the results were significant" or simply "p \< .05." However, if authors tell us that a result was "null" or "not significant" but do not tell us anything more, we set the result to be *d* = 0.01. These rules are not perfect, but ultimately, we need policies that balance type I errors (including a study we shouldn't) and type II errors (excluding studies we should have included). Like research itself, we suspect that this task is not perfectible.

Last, there is a great deal of subjectivity in figuring out what outcomes to code. For @porat2024, we found relative homogeneity of dependent variables, but not so for @paluck2019 or @paluck2021. In the end,this has always been a judgment call, but as we continue writing meta-analyses, we hope to develop principled, general guidelines for this kind of analytic choice.

These open questions notwithstanding, we think meta-analyses can be useful, informative, and a pleasure to write and read. We hope to see them become a standard part of the meta-scientist's toolkit, and likewise for all meta-analyses to become a little more meta-scientific.

## References
