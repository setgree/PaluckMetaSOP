---
title: "Towards meta-scientific meta-analyses: standard operating procedures for systematic reviews in the Paluck Lab"
shorttitle: "Paluck_meta_SOP"

author: 
  - name: "Seth Green"
    affiliation: "1"
    corresponding: yes
    address: "Kahneman-Treisman Center, Princeton University"
    email: "sag2212@columbia.edu"
    role:
      - "Conceptualization"
      - "Writing - Original Draft Preparation"
      - "Writing - Review & Editing"
  - name: "Elizabeth Levy Paluck"
    affiliation: "1"
    role:
      - "Writing - Review & Editing"
      - "Supervision"
  - name: "Roni Porat"
    affiliation: "2"
    role:
      - "Writing - Review & Editing"
      - "Supervision"
  
affiliation:
  - id: "1"
    institution: "Princeton University"
  - id: "2"
    institution: "Hebrew University, Jerusalem"

authornote: |
  This work was supported by [whoever supports it]

abstract: |
  This paper describes the motivations and procedures for meta-analyses by Betsy Levy Paluck and co-authors, with reference to and examples drawn from three such papers (Paluck, Green, and Green 2019, Paluck et al. 2020, and Porat et al. 2024). We first describe our conceptual aims when writing meta-analytic papers, which are to provide an 'on-ramp' to interested readers, to evaluate a literature's strengths and weaknesses, and to illuminate empirical gaps, which we intend as signposts for future researchers. Second, we describe the typical 'flow' of the papers as a whole. Third, we describe our meta-analytic procedures, detailing our default reporting structure and analytic choices and engaging with recent critiques from Slough and Tyson (2022) and Simonsohn, Simmons, and Nelson (2022). Fourth, we introduce a collection of functions we use to implement our meta-analytic choices, available as an R package. Fifth, we conclude with some open questions for meta-analysts.
  
keywords: "meta-analysis, standard-operating-procedures, meta-science"
wordcount: "2240"

bibliography: "meta-sop-references.bib"

floatsintext: no
linenumbers: yes
draft: no
mask: no

figurelist: no
tablelist: no
footnotelist: no

classoption: "man"
output: papaja::apa6_pdf
---

##  Introduction

What purposes do meta-analyses serve above and beyond those met by systematic reviews? Judging by recent papers in social psychology [@pettigrew2006; @bem2015; @cuddy2018; @bezrukova2016], the answer is to bolster and validate existing findings through aggregation. By pooling findings from many studies, researchers furnish meta-analytic estimates that are typically heartening to a literature's boosters by dint of their statistical precision and suggestion of broad applicability across settings. Such papers often have a valedictory quality to them: they provide leading practitioners a chance to survey the body of work they've helped inspire, highlight the strength of the evidence for their subfield's central tenets, and suggest future research directions that assume those tenets are no longer in need of testing.

Meanwhile, a parallel movement in psychology has begun to implement the findings and suggestions of the open science [@nosek2015] and meta-science [@schooler2014] movements into study designs [@ferguson2023], journal policies [@hardwicke2023transparency], and literature reviews [@hardwicke2018]. Some of those reviews provide quantitative assessments of meta-scientific quantitites, for instance, what percentage of a subset of papers are computationally reproducible [@obels2020] or consistent with open data standards [@hardwicke2021]. In some sense, these estimates are meta-analytic, but they aren't meta-analyses in the conventional sense of pooling existing findings into weighted average effects [@glass197; @kleinstauber1996; @cooper2019]. In tone, meta-science papers are often much more critical than meta-analyses are, and they are often written by outsiders or newcomers to an academic field rather than leading experts. Thus, though they share part of their name, the meta-analytic and meta-scientific tradition generally bring very different starting assumptions, use different tools, and are fundamentally written by and aimed at different audiences: from insider to insiders vs. outsiders to everyone.

At the Paluck lab, we seek to synthesize these two approaches by writing meta-analyses that are fundamentally meta-scientific. Over the course of four systematic reviews, three with meta-analytic components [@paluck2009; @paluck2019; @paluck2021; @porat2024], we've developed distinct beliefs about where and when a meta-analysis is useful, how to structure the meta-analytic sections of our systematic reviews, and which specifications are sensible defaults. This paper articulates those beliefs, and aims to illuminate how  meta-analysis might become a standard part of the meta-scientist's toolkit. 

First, we describe the conditions when a meta-analysis adds something above and beyond a systematic review. We believe that meta-analyses are useful for evaluating the strengths and weaknesses of large, heterogeneous literatures, especially those that bring multiple theoretical perspectives to bear. They can also be useful for providing focused re-examination of a literature's main findings as a contrast to previous credulous reviews. Contrary to critics who argue that core assumptions about homogeneity across experimental contrasts and outcomes are necessary for the standard meta-analytic model to hold [@slough2023], we think that comprehensive, heterogeneous meta-analyses can still be useful when these conditions aren't met; in such cases, meta-analytic estimates should be treated as _descriptive_ rather than _causal_ inferences. Moreover, we see value in contrasting all-inclusive estimates, which are fundamentally descriptive, to more rigorous, focused estimates, which, because they reflect strict selection criteria related to study design and outcome measurement, are more plausibly causal. The within-paper contrast in magnitude and precision between these two approaches is often startling and illuminating.

Next, we describe the typical structure of the meta-analysis sections of our papers. In general, we start by describing the main ideas and trends in a literature. This provides interested readers an 'onramp' into a literature's major findings and concerns. We then describe our search processes and meta-analytic methods. We present our meta-analytic findings in our 'results' section. These analyses are structured to start big and go small, meaning that we first present meta-analytic estimates and tests for publication bias for the entire literature; we then hone in on key quantities of interest from smaller subsets of the data. For instance, in a literature with randomized controlled trials, quasi-experiments, and observational designs, as well as a mix of attitudinal and behavioral outcomes, we might present a 3 by 2 table where each subset of study design + outcome strategy gets its own meta-analytic estimate. Alternatively, in a literature with many similar but distinct quantities of interest, e.g. racial/ethnic prejudice vs. prejudice against LGBTQ+ people vs. prejudice against people with physical and mental handicaps, we would present the meta-analytic effects within each subset of data. Our final tests tend to probe differences in effect size by some marker of study quality, e.g. the presence of a pre-analysis plan, or the relationship between effect sizes within studies, e.g. if attitudinal changes tends to predict behavioral changes.

Third, we detail our meta-analytic defaults. For instance, we use a random effects model (as implemented by the `metafor` package in R [@viechtbauer2010]) rather than a fixed effects (sometimes called an equal effects) model where we cluster at the level of an individual study. Further, we prefer Glass's $\Delta$ to Cohen's _d_ so as to avoid an additional assumption about the effects of treatment on the underlying distribution. Here, we also detail a novel estimator for converting difference in proportion to an effect size that has some desirable properties relative to the conventional conversion from an odds ratio [@gomila2021].

Fourth, we introduce an R package, `PaluckMetaSOP`, that contains functions to help us quickly and efficiently perform meta-analyses. We describe the main categories of function and provide brief usage examples.

Finally, we conclude by...

Intersection of a few literatures...

This paper is not a comprehensive overview of how to do meta-analysis. See Cochrane Review for general; Cooper Hedges and Valentine for effect sizes; Experimentology ch 18 and Lakens ch 20 for hands-on guides; 


   7. Cluster at level of study
    8. Glass's $\Delta$ rather than Cohen's d
### Contemporary criticisms of meta-analysis from meta-scientists
Recent papers have raised the possibility that the core assumptions of meta-analysis are often not met in contemporary work.

@slough2023 argue that two core conditions are necessary for meta-analysis to make sense: contrast and target harmonization...

Meanwhile, @simonsohn2022. argue...

These critiques register deeply with us because of our three recent meta-analyses, two strive to be comprehensive and therefore arguably do not meet the assumptions laid out by Slough and Tyson and are guilty of the sins identified by SSN (though SSN also identify one of papers as exemplary)

So does meta-analysis still _work_ under these conditions? Is there something cogent and meaningful here? 

### All models are wrong, but this one is still useful 

We say yes, we still think meta-analysis can be useful even if the underlying literatures do not meet the ideal conditions of a medical literature, with the qualification that meta-analytic estimates need to be presented and  contextualized appropriately. We urge researchers not to throw the baby out with the bathwater -- as always, all models are wrong, but we think this one is useful. 

This article outlines how we make our meta-analyses useful, and to whom. We start by identifying what makes a literature a good candidate for meta-analysis, our overarching purposes when we write one, and who we write such pieces for. We then describe our typical paper structure for a meta-analysis; our default analytic choices; and an R package, `PaluckMetaSOP`, that implements our defaults and helps aids our writing process. We conclude with some open questions and hard cases

[where does this go] this piece also contributes to two burgeoning literatures:
    SOPs ([https://alexandercoppock.com/Green-Lab-SOP/Green_Lab_SOP.pdf](https://alexandercoppock.com/Green-Lab-SOP/Green_Lab_SOP.pdf), [https://www.stat.berkeley.edu/~winston/sop-safety-net.pdf](https://www.stat.berkeley.edu/~winston/sop-safety-net.pdf)) + ones Don cited
    establishing theoretical underpinnings of meta-analysis (Slough and Tyson, SSN, Gechter and Meager)
also munger 2023
- metaketas (slough et al) and psych science accelerator? Those pieces are meta but in a different way.
#### The remainder of this paper, and what this paper is not
- Paper

- What this paper is not: the Cochrane review
        4. The Cochrane review is still the seminal text, and this piece exists in dialogue with it. 

## When are meta-analyses useful and value do they bring?

###  When do we do meta-analyses?

1. Our lab embarks on meta-analyses when
    1. Big literature with heterogeneous research quality
    5. Multiple theoretical approaches whose comparative efficacy is unknown 
    6. Intuition that a comprehensive read through and tests for rigor will reveal collective gaps in understanding
2. Alternatively, if someone else has already written a comprehensive review, there might be room for a focused rejoinder, i.e. Pettigrew and Tropp (2006) amalgamate everything and find large effects, whereas Paluck Green and Green (2019) look at just the best studies and find much more mixed results.


### What purposes do they serve?



1. On-ramp to unfamiliar literature (alternate metaphor: "a lobby," from Toni Morrison's introduction to Beloved), exemplified by
    1. Contact hypothesis: what, where, why
    2. iii. Prejudice reduction: theoretical perspectives, landmark studies
    3. iv. Sexual violence paper: history of zeitgeist perspectives
2. Evaluate rather than transcribe
    4. Describe critique of transcriptive, non-evaluative work in Simonsohn et al. (2022)
    5. We identify high points (excellent studies) and low points (widespread methodological deficiencies) 
    6. place effect sizes in context of real-world impact whenever possible, e.g. the prejudice paper argues that $\Delta$ of 0.27 corresponds to [X] change on ANES survey
3. Illuminate empirical gaps
    7. As our reviews show, applying some fairly minimal quality standards quickly winnows hundreds of studies down to just a few
    8. These studies are likely to be the  most policy-relevant, and oftentimes the 'ideal' study simply hasn't been conducted at all yet
    9. Examples: 
        1. Paluck, Green and Green (2019) highlight lack of studies testing interracial contact among adults -> provide intellectual backdrop for Scacco and Warren (2018), Mousa (2020), and Lowe (2019) -- as well as providing theoretical underpinning for those studies' comparatively underwhelming findings
        2 similar examples from Paluck et al. (2021)? 
        3. (arguably a systematic review suffices for this)

### Who are they for?
1. researchers, especially folks trying to figure out where they can have the most impact
2. policymakers and funders who want a critical review of a literature
3. any interested reader. Our metas tend to be about big, important subjects that people are thinking about. 
## Typical structure of Paluck lab meta-analytic papers



1. Describe context and stakes of the paper (i.e. why it's important to see if these interventions 'work', or why thereâ€™s reason to doubt an established consensus)
2. Intellectual overview of major ideas in the literature
3. Meta-analytic search methods 
4. Meta-analytic approach
5. Quantitative Results
6. Discussion, highlights and gaps
7. Conclusion


## Paluck lab meta-analytic procedures



1. Code more than you think you're going to need
    1. You might be interested in the lasting effects, which means recording the latest possible effect sizes, but if you record the _earliest_ possible effects, you can detect within-study decay
2. Start big and then go small, like a funnel; i.e. meta-analyze *everything* and then zoom in on different subsets of the literature
    2. Why meta-analyze everything?	
        1. Slough and Tyson (2023) argue that meta-analytic estimates only make sense if there's contrast harmony, i.e. if the many studies are clearly testing some common underlying theoretical concept. In some literatures we've looked at, this pretty clearly does not hold, e.g. the average effect of a contact hypothesis field experiment for Muslims and Christians and Nigeria and a diversity training in a U.S. corporate workplace tells you the combined effect of *what* exactly? 
        2.  Ditto with "target harmony", which is whether interventions are aimed *at* changing the same thing. For instance, does it make sense to average the effect of some intervention on self-reported attitudes and behavioral outcomes? As @porat2024 show, these outcomes are not always well correlated, which is prima facie evidence that we don't have target harmony.
        3. Last, integrating observational and randomized studies is often justified as supplementing high studies with high internal validity with studies with high *external* validity, but it's probably more accurate to say that you take some studies that provide unbiased estimates and then, in exchange for greater statistical precision, induce bias arising from non-statistical sources of uncertainty ([Gerber, Green and Kaplan 2004](http://www.donaldgreen.com/wp-content/uploads/2015/09/Gerber-Green-Kaplan-IllusionofLearning.pdf)).
    3. multi-part answer.
        4. Overall meta-analytic estimate provides a within-paper size comparison; your overall effect size is X and your effect size for the very best studies is 1/3 X, that means something.
        5. Test for publication bias should probably look at absolutely everything
        6. Inter-paper comparisons; Paluck, Green and Green (2019) and Paluck et al. (2021) provide estimates of about $\Delta$ = 0.3, and then Green, Smith and Mathur (forthcoming) find $\Delta$ = 0.138, which also means something.
3. Do some serious tests for publication bias: both conventional tests (egger's test, funnel plot, $\Delta$ ~ SE), but also think through where else it might emerge, e.g. compare effect sizes in studies with and without DOIs
4. separate the dataset into different chunks, e.g. by theory, study design, or measurement strategy, and present these meta-analytic estimates side by side
    4. Subset enough and eventually you get to reasonable claim for contrast and mechanism harmony (Slough and Tyson 2022)
5.  Zoom in on and carefully and analyze best studies, both in aggregate and individually
    5. These results might be surprising, e.g. when looking solely at RCTs on perpetration outcomes [@porat2024], slightly fewer than half are self-reported nulls.
6. Meta-analytic defaults
    6. Random effects: any literature we want to look at is going to have heterogeneous inputs. 
    7. Cluster at level of study
    8. Glass's $\Delta$ rather than Cohen's d
    9. Difference in proportion rather than odds ratio
        7. resurface text from appendix to Porat et al. (2024)


## R package: blp_meta_functions



1. Functions fall into four categories
    1. Converting studies to singular estimates of effect size, variance and standard error
    2. Wrapper functions that distill aggregate results into the core findings and make them table-ready
    3. plotting functions
    4. Miscellany: reproducibility, helper functions

## Conclusions: hard cases

1. What do we do when a paper's results are obviously not credible? 
    1. e.g. if they present a result that's well past possible, e.g. t-test value of 36 (one of the papers in the prejudice literature had this, and also the cost-benefit analyses in the SOSA! Intervention in the primary prevention literature)
2. What's the right outcome to code? Often very unclear what's prime or most representative of underlying construct
3. Others...
4. Meta-analyses are still very useful, we think, for readers (both expert and non-expert) and future researchers. 
