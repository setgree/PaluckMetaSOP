% Options for packages loaded elsewhere
\PassOptionsToPackage{unicode}{hyperref}
\PassOptionsToPackage{hyphens}{url}
%
\documentclass[
  man]{apa6}
\usepackage{amsmath,amssymb}
\usepackage{iftex}
\ifPDFTeX
  \usepackage[T1]{fontenc}
  \usepackage[utf8]{inputenc}
  \usepackage{textcomp} % provide euro and other symbols
\else % if luatex or xetex
  \usepackage{unicode-math} % this also loads fontspec
  \defaultfontfeatures{Scale=MatchLowercase}
  \defaultfontfeatures[\rmfamily]{Ligatures=TeX,Scale=1}
\fi
\usepackage{lmodern}
\ifPDFTeX\else
  % xetex/luatex font selection
\fi
% Use upquote if available, for straight quotes in verbatim environments
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
\IfFileExists{microtype.sty}{% use microtype if available
  \usepackage[]{microtype}
  \UseMicrotypeSet[protrusion]{basicmath} % disable protrusion for tt fonts
}{}
\makeatletter
\@ifundefined{KOMAClassName}{% if non-KOMA class
  \IfFileExists{parskip.sty}{%
    \usepackage{parskip}
  }{% else
    \setlength{\parindent}{0pt}
    \setlength{\parskip}{6pt plus 2pt minus 1pt}}
}{% if KOMA class
  \KOMAoptions{parskip=half}}
\makeatother
\usepackage{xcolor}
\usepackage{graphicx}
\makeatletter
\def\maxwidth{\ifdim\Gin@nat@width>\linewidth\linewidth\else\Gin@nat@width\fi}
\def\maxheight{\ifdim\Gin@nat@height>\textheight\textheight\else\Gin@nat@height\fi}
\makeatother
% Scale images if necessary, so that they will not overflow the page
% margins by default, and it is still possible to overwrite the defaults
% using explicit options in \includegraphics[width, height, ...]{}
\setkeys{Gin}{width=\maxwidth,height=\maxheight,keepaspectratio}
% Set default figure placement to htbp
\makeatletter
\def\fps@figure{htbp}
\makeatother
\setlength{\emergencystretch}{3em} % prevent overfull lines
\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}
\setcounter{secnumdepth}{-\maxdimen} % remove section numbering
% Make \paragraph and \subparagraph free-standing
\ifx\paragraph\undefined\else
  \let\oldparagraph\paragraph
  \renewcommand{\paragraph}[1]{\oldparagraph{#1}\mbox{}}
\fi
\ifx\subparagraph\undefined\else
  \let\oldsubparagraph\subparagraph
  \renewcommand{\subparagraph}[1]{\oldsubparagraph{#1}\mbox{}}
\fi
% definitions for citeproc citations
\NewDocumentCommand\citeproctext{}{}
\NewDocumentCommand\citeproc{mm}{%
  \begingroup\def\citeproctext{#2}\cite{#1}\endgroup}
\makeatletter
 % allow citations to break across lines
 \let\@cite@ofmt\@firstofone
 % avoid brackets around text for \cite:
 \def\@biblabel#1{}
 \def\@cite#1#2{{#1\if@tempswa , #2\fi}}
\makeatother
\newlength{\cslhangindent}
\setlength{\cslhangindent}{1.5em}
\newlength{\csllabelwidth}
\setlength{\csllabelwidth}{3em}
\newenvironment{CSLReferences}[2] % #1 hanging-indent, #2 entry-spacing
 {\begin{list}{}{%
  \setlength{\itemindent}{0pt}
  \setlength{\leftmargin}{0pt}
  \setlength{\parsep}{0pt}
  % turn on hanging indent if param 1 is 1
  \ifodd #1
   \setlength{\leftmargin}{\cslhangindent}
   \setlength{\itemindent}{-1\cslhangindent}
  \fi
  % set entry spacing
  \setlength{\itemsep}{#2\baselineskip}}}
 {\end{list}}
\usepackage{calc}
\newcommand{\CSLBlock}[1]{\hfill\break\parbox[t]{\linewidth}{\strut\ignorespaces#1\strut}}
\newcommand{\CSLLeftMargin}[1]{\parbox[t]{\csllabelwidth}{\strut#1\strut}}
\newcommand{\CSLRightInline}[1]{\parbox[t]{\linewidth - \csllabelwidth}{\strut#1\strut}}
\newcommand{\CSLIndent}[1]{\hspace{\cslhangindent}#1}
\ifLuaTeX
\usepackage[bidi=basic]{babel}
\else
\usepackage[bidi=default]{babel}
\fi
\babelprovide[main,import]{english}
% get rid of language-specific shorthands (see #6817):
\let\LanguageShortHands\languageshorthands
\def\languageshorthands#1{}
% Manuscript styling
\usepackage{upgreek}
\captionsetup{font=singlespacing,justification=justified}

% Table formatting
\usepackage{longtable}
\usepackage{lscape}
% \usepackage[counterclockwise]{rotating}   % Landscape page setup for large tables
\usepackage{multirow}		% Table styling
\usepackage{tabularx}		% Control Column width
\usepackage[flushleft]{threeparttable}	% Allows for three part tables with a specified notes section
\usepackage{threeparttablex}            % Lets threeparttable work with longtable

% Create new environments so endfloat can handle them
% \newenvironment{ltable}
%   {\begin{landscape}\centering\begin{threeparttable}}
%   {\end{threeparttable}\end{landscape}}
\newenvironment{lltable}{\begin{landscape}\centering\begin{ThreePartTable}}{\end{ThreePartTable}\end{landscape}}

% Enables adjusting longtable caption width to table width
% Solution found at http://golatex.de/longtable-mit-caption-so-breit-wie-die-tabelle-t15767.html
\makeatletter
\newcommand\LastLTentrywidth{1em}
\newlength\longtablewidth
\setlength{\longtablewidth}{1in}
\newcommand{\getlongtablewidth}{\begingroup \ifcsname LT@\roman{LT@tables}\endcsname \global\longtablewidth=0pt \renewcommand{\LT@entry}[2]{\global\advance\longtablewidth by ##2\relax\gdef\LastLTentrywidth{##2}}\@nameuse{LT@\roman{LT@tables}} \fi \endgroup}

% \setlength{\parindent}{0.5in}
% \setlength{\parskip}{0pt plus 0pt minus 0pt}

% Overwrite redefinition of paragraph and subparagraph by the default LaTeX template
% See https://github.com/crsh/papaja/issues/292
\makeatletter
\renewcommand{\paragraph}{\@startsection{paragraph}{4}{\parindent}%
  {0\baselineskip \@plus 0.2ex \@minus 0.2ex}%
  {-1em}%
  {\normalfont\normalsize\bfseries\itshape\typesectitle}}

\renewcommand{\subparagraph}[1]{\@startsection{subparagraph}{5}{1em}%
  {0\baselineskip \@plus 0.2ex \@minus 0.2ex}%
  {-\z@\relax}%
  {\normalfont\normalsize\itshape\hspace{\parindent}{#1}\textit{\addperi}}{\relax}}
\makeatother

\makeatletter
\usepackage{etoolbox}
\patchcmd{\maketitle}
  {\section{\normalfont\normalsize\abstractname}}
  {\section*{\normalfont\normalsize\abstractname}}
  {}{\typeout{Failed to patch abstract.}}
\patchcmd{\maketitle}
  {\section{\protect\normalfont{\@title}}}
  {\section*{\protect\normalfont{\@title}}}
  {}{\typeout{Failed to patch title.}}
\makeatother

\usepackage{xpatch}
\makeatletter
\xapptocmd\appendix
  {\xapptocmd\section
    {\addcontentsline{toc}{section}{\appendixname\ifoneappendix\else~\theappendix\fi\\: #1}}
    {}{\InnerPatchFailed}%
  }
{}{\PatchFailed}
\keywords{meta-analysis, standard-operating-procedures, meta-science\newline\indent Word count: 2910}
\DeclareDelayedFloatFlavor{ThreePartTable}{table}
\DeclareDelayedFloatFlavor{lltable}{table}
\DeclareDelayedFloatFlavor*{longtable}{table}
\makeatletter
\renewcommand{\efloat@iwrite}[1]{\immediate\expandafter\protected@write\csname efloat@post#1\endcsname{}}
\makeatother
\usepackage{lineno}

\linenumbers
\usepackage{csquotes}
\ifLuaTeX
  \usepackage{selnolig}  % disable illegal ligatures
\fi
\usepackage{bookmark}
\IfFileExists{xurl.sty}{\usepackage{xurl}}{} % add URL line breaks if available
\urlstyle{same}
\hypersetup{
  pdftitle={Towards meta-scientific meta-analyses: standard operating procedures for systematic reviews in the Paluck Lab},
  pdfauthor={Seth Green1, Elizabeth Levy Paluck1, \& Roni Porat2},
  pdflang={en-EN},
  pdfkeywords={meta-analysis, standard-operating-procedures, meta-science},
  hidelinks,
  pdfcreator={LaTeX via pandoc}}

\title{Towards meta-scientific meta-analyses: standard operating procedures for systematic reviews in the Paluck Lab}
\author{Seth Green\textsuperscript{1}, Elizabeth Levy Paluck\textsuperscript{1}, \& Roni Porat\textsuperscript{2}}
\date{}


\shorttitle{Paluck\_meta\_SOP}

\authornote{

This work was supported by {[}whoever supports it{]}

The authors made the following contributions. Seth Green: Conceptualization, Writing - Original Draft Preparation, Writing - Review \& Editing; Elizabeth Levy Paluck: Writing - Review \& Editing, Supervision; Roni Porat: Writing - Review \& Editing, Supervision.

Correspondence concerning this article should be addressed to Seth Green, Kahneman-Treisman Center, Princeton University. E-mail: \href{mailto:sag2212@columbia.edu}{\nolinkurl{sag2212@columbia.edu}}

}

\affiliation{\vspace{0.5cm}\textsuperscript{1} Princeton University\\\textsuperscript{2} Hebrew University, Jerusalem}

\abstract{%
This paper describes the motivations and procedures for meta-analyses by Betsy Levy Paluck and co-authors, with reference to and examples drawn from three such papers (Paluck, Green, and Green 2019, Paluck et al.~2020, and Porat et al.~2024). We first describe our conceptual aims when writing meta-analytic papers, which are to provide an `on-ramp' to interested readers, to evaluate a literature's strengths and weaknesses, and to illuminate empirical gaps, which we intend as signposts for future researchers. Second, we describe the typical `flow' of the papers as a whole. Third, we describe our meta-analytic procedures, detailing our default reporting structure and analytic choices and engaging with recent critiques from Slough and Tyson (2022) and Simonsohn, Simmons, and Nelson (2022). Fourth, we introduce a collection of functions we use to implement our meta-analytic choices, available as an R package. Fifth, we conclude with some open questions for meta-analysts.
}



\begin{document}
\maketitle

\subsection{Introduction: the divergent approaches of meta-analysis and meta-science}\label{introduction-the-divergent-approaches-of-meta-analysis-and-meta-science}

What purposes do meta-analyses serve above and beyond those accomplished by systematic reviews? Judging by recent papers in social psychology (Bem, Tressoldi, Rabeyron, \& Duggan, 2015; Bezrukova, Spell, Perry, \& Jehn, 2016; Cuddy, Schultz, \& Fosse, 2018; Pettigrew \& Tropp, 2006), the answer is to bolster and validate existing findings through aggregation. By pooling findings from many studies, researchers furnish meta-analytic estimates that are typically heartening to a literature's boosters because they are statistically precise and drawn from diverse settings, which gives readers the impression of durable, widely applicable findings. Such papers often have a valedictory quality to them: they provide leading practitioners a chance to survey the body of work they've helped inspire, highlight the strength of the evidence for their subfield's central tenets, and suggest future research directions that assume those tenets are no longer in need of testing.

Meanwhile, a parallel movement in psychology has begun to implement the findings and suggestions of the open science (Nosek, 2015) and meta-science (Schooler, 2014) movements into study designs (Ferguson et al., 2023), journal policies (Hardwicke \& Vazire, 2023), and literature reviews (Hardwicke et al., 2018). Some of those reviews quantitatively assess meta-scientific outcomes, for instance, what percentage of a subset of papers are computationally reproducible (Obels, Lakens, Coles, Gottfried, \& Green, 2020) or consistent with open data standards (Hardwicke et al., 2021). In some sense, these estimates are meta-analytic, but they aren't meta-analyses in the conventional sense of pooling existing findings into weighted average effects (Cooper, Hedges, \& Valentine, 2019; Glass, 1977; Kleinstäuber et al., 1996). In tone, meta-science papers are often much more critical than meta-analyses are, and they are often written by outsiders or newcomers to an academic field who appear more motivated to interrogate than to validate. Thus, despite the superficial similarity of the terms, meta-analytic and meta-scientific papers in social psychology typically have radically different perspectives, assumptions, and goals.

At the Paluck lab, we seek to synthesize these two approaches by writing meta-analyses that are fundamentally meta-scientific. Over the course of four systematic reviews, three with meta-analytic components (Paluck \& Green, 2009; Paluck, Green, \& Green, 2019; Paluck, Porat, Clark, \& Green, 2021; Porat, Gantman, Paluck, Green, \& Pezzuto, 2024), we've developed distinct beliefs about where and when a meta-analysis is useful, how to structure the meta-analytic sections of our systematic reviews, and which specifications are sensible defaults. This paper articulates those beliefs, and aims to illuminate how meta-analysis might become a standard component of the meta-scientific toolkit.

First, we describe the conditions when a meta-analysis adds something above and beyond a systematic review. We believe that meta-analyses are useful for evaluating the strengths and weaknesses of large, heterogeneous literatures, especially those that bring multiple theoretical perspectives to bear. They can also be useful for providing focused re-examination of a literature's main findings as a contrast to previous credulous reviews. Contrary to critics who argue that core assumptions about homogeneity across experimental contrasts and outcomes are necessary for the standard meta-analytic model to hold (Slough \& Tyson, 2023), we think that comprehensive, heterogeneous meta-analyses can still be useful when these conditions aren't met; in such cases, meta-analytic estimates should be treated as \emph{descriptive} rather than \emph{causal} inferences. Moreover, we see value in contrasting all-inclusive estimates, which are fundamentally descriptive, to more rigorous, focused estimates, which, because they reflect strict selection criteria related to study design and outcome measurement, are more plausibly causal. The within-paper contrast in magnitude and precision between these two approaches is often startling and illuminating.

Next, we describe the typical structure of our meta-analytic papers. In general, we start by describing the main ideas and trends in a literature, which provides readers an `onramp' into the field's major findings and concerns. Our papers then describe our search processes and meta-analytic methods, followed by our meta-analytic findings. These analyses are structured to start big and go small, meaning that we first present meta-analytic estimates and tests for publication bias for the entire literature and then hone in on key quantities of interest from smaller subsets of the data. For instance, in a literature with randomized controlled trials, quasi-experiments, and observational designs, as well as a mix of attitudinal and behavioral outcomes, we might first present an overall meta-analytic effect, and then present a 3 by 2 table where each subset of study design + outcome strategy gets its own meta-analytic estimate. Alternatively, in a literature with many similar but distinct quantities of interest, e.g.~racial/ethnic prejudice vs.~prejudice against LGBTQ+ people vs.~prejudice against people with physical and mental disabilities, we would present the meta-analytic effects within each subset of data. Our concluding investigations tend to probe differences in effect size by some marker of study quality, e.g.~the presence of a pre-analysis plan, or the relationship between effect sizes within studies, e.g.~if attitudinal changes tends to predict behavioral changes.

Third, we detail our meta-analytic defaults. For instance, we use a random effects model, as implemented by the \texttt{metafor} package in R (Viechtbauer, 2010), rather than a fixed effects (sometimes called an equal effects) model, and we cluster our analyses at the level of an individual study. Further, we prefer Glass's \(\Delta\) to Cohen's \emph{d} so as to avoid an additional assumption about the effects of treatment on the underlying distribution of outcomes. Here, we also detail a novel estimator for converting difference in proportion to an effect size that has some desirable properties relative to the conventional conversion from an odds ratio (Gomila, 2021).

Fourth, we introduce an R package, \texttt{PaluckMetaSOP}, that contains functions to help us quickly and efficiently perform meta-analyses. We describe the main categories of function and provide brief usage examples.

Finally, we conclude by noting some open questions for meta-scientific meta-analyses. While we tend to begin meta-analytic projects with a sense of what's true and what's interesting, in practice, the most incisive questions often emerge after reading and coding studies, i.e.~after data collection has begun. In other words, meta-analyses are both \emph{hypothesis\_driven} (re: deductive) and \emph{data-driven} (re: inductive); However, typical checks on researcher degrees of freedom assume a hypothesis-driven framework. This makes it tricky to ``tie one's hands to the mast'' (Elster, 1977), e.g., by writing a detailed pre-analysis plan and pre-specifying which outcomes to focus on. We describe how we've dealt with such issues so far and how we think to improve going forward. Last, we discuss the difficulty of integrating indirect cues of study quality, e.g.~obviously mistaken statistical results or special attention devoted to reproducibility, into our analyses in a principled way.

This paper is closely in dialogue with two recent papers that attempt to put meta-analysis on firmer theoretical footing. Slough and Tyson (2023) ``highlight the dangers of conflating conceptual differences across studies with statistical sources of variation'' (p.~29). They argue that for studies to ``identify the same empirical target'' (p.~1), they must have fundamentally \emph{harmonized} contrasts and measurement strategies, i.e.~the ``substantive comparison across studies is the same'' and ``the outcome of interest is the same and it is measured in the same way'' (p.~2). Absent these conditions, meta-analytic results might not be ``meaningful and interpretable'' (p.~2). Contrary to conventional approaches to improving the accuracy of meta-analyses, a lack of target equivalence cannot be ``solved solely with statistical techniques'' (p.~23). Rather, it is more likely to be achieved through ``design or inclusion criteria'' (p.~2). We agree that an internally and externally valid causal meta-analytic estimate requires harmonization of contrasts and outcomes (as well as a high degree of internal validity within constituent studies). However, we also think that meta-analytic estimates which do \emph{not} meet these conditions \textemdash e.g.~those that combine results from many different designs, or whose diverse outcomes bear an unknown relationship to the true outcome of interest \textemdash can still be useful and informative if interpreted and framed correctly. We think that when a literature without clear harmony of the independent and dependent variables is a case where the underlying model might be ``wrong'' but still useful.

The other paper in this vein we look to is Simonsohn, Simmons, and Nelson (2022), who argue that while the typical meta-analysis in social psychology aims ``to be comprehensive, results-focused and transcriptive,'' this is ``misguided, leading to uninterpretable results that misrepresent research literatures.'' First, they argue that a desire for comprehensiveness typically leads the meta-analyst to average ``studies of the highest quality'' with ``studies that lack internal validity or external validity, which are obtained using incorrect statistical techniques, or studies where results seem to arise from methodological artefacts.'' This throw-it-all-in-the-blender approach produces results that are ``virtually guaranteed to lack a meaningful interpretation,'' despite their putative statistical precision. Second, the authors encourage meta-analysts to focus on studies' designs, rather than just their results, to provide ``readers with information that they can use to evaluate'' the quality of a research literature for themselves. Rather than describing every study, meta-analysts ``can succinctly describe and summarize common design failures\ldots and provide detailed descriptions only of the studies they deem most important or compelling.'' This advice ties closely to their last dictum to meta-analysts: to evaluate rather than to transcribe, i.e., detail the evidence from the very best studies, identify common methodological shortcomings of the literature as a whole, and, finally, ``discuss what open questions remain and what kinds of evidence would help answer those questions.''

Our meta-analyses share these goals (and Paluck et al. (2021) is cited therein for being ``selective, design-focused, and evaluative''). Moreover, the authors note that typical guides to meta-analysis ``briefly mention validity concerns but provide little guidance as to how to carry out quality control;'' one of the aims of this paper is to provide such guidance. However, as mentioned above, we part ways from this paper in that we believe comprehensive meta-analytic estimates still have a place in the meta-scientist's toolkit, so long as those estimates are ascribed descriptive rather than causal meaning and are used to contextualize further, more selective analyses.

Our paper is a hybrid of a conceptual analysis of meta-analyses in general and a play-by-play of how we write them. On the latter front, this paper builds on a tradition of Standard Operating Procedures (SOPs) in the behavioral and medical sciences (Lakens, 2023; Rapport et al., 2013; Wallace et al., 2015). In particular, we seek to build on the Green Lab at Columbia's SOP for deviating from a pre-analysis plan (Lin \& Green, 2016). Like that paper, this paper is not intended as a comprehensive guide to the task in question \textemdash see (\textbf{cochrane?}) and Cooper et al. (2019) for textbook-length treatments of meta-analysis, and (\textbf{frank2023?}, chapter whatever), lakens 2019 (chapter whatever) and Slough and Tyson (fortochoming, chapter whatever) for modern treatments of the subject aimed at social scientists. Rather, we articulate our lab's admittedly idiosyncratic approach to a task that many research groups embark on. We hope this will prove interesting and informative, but not definitive: we expect readers to adapt and expand our guidelines to meet their own needs. In general, we hope to see more labs publish papers about how they produce knowledge.'

We now detail our conceptual approach to meta-analyses.

\subsection{Meta-analyses: when, how and for whom}\label{meta-analyses-when-how-and-for-whom}

\subsubsection{When do we write meta-analyses?}\label{when-do-we-write-meta-analyses}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Our lab embarks on meta-analyses when

  \begin{enumerate}
  \def\labelenumii{\arabic{enumii}.}
  \tightlist
  \item
    Big literature with heterogeneous research quality
  \item
    Multiple theoretical approaches whose comparative efficacy is unknown
  \item
    Intuition that a comprehensive read through and tests for rigor will reveal collective gaps in understanding
  \end{enumerate}
\item
  Alternatively, if someone else has already written a comprehensive review, there might be room for a focused rejoinder, i.e.~Pettigrew and Tropp (2006) amalgamate everything and find large effects, whereas Paluck Green and Green (2019) look at just the best studies and find much more mixed results.
\end{enumerate}

\subsubsection{What purposes do they serve?}\label{what-purposes-do-they-serve}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  On-ramp to unfamiliar literature (alternate metaphor: ``a lobby,'' from Toni Morrison's introduction to Beloved), exemplified by

  \begin{enumerate}
  \def\labelenumii{\arabic{enumii}.}
  \tightlist
  \item
    Contact hypothesis: what, where, why
  \item
    \begin{enumerate}
    \def\labelenumiii{\roman{enumiii}.}
    \setcounter{enumiii}{2}
    \tightlist
    \item
      Prejudice reduction: theoretical perspectives, landmark studies
    \end{enumerate}
  \item
    \begin{enumerate}
    \def\labelenumiii{\roman{enumiii}.}
    \setcounter{enumiii}{3}
    \tightlist
    \item
      Sexual violence paper: history of zeitgeist perspectives
    \end{enumerate}
  \end{enumerate}
\item
  Evaluate rather than transcribe

  \begin{enumerate}
  \def\labelenumii{\arabic{enumii}.}
  \setcounter{enumii}{3}
  \tightlist
  \item
    Describe critique of transcriptive, non-evaluative work in Simonsohn et al.~(2022)
  \item
    We identify high points (excellent studies) and low points (widespread methodological deficiencies)
  \item
    place effect sizes in context of real-world impact whenever possible, e.g.~the prejudice paper argues that \(\Delta\) of 0.27 corresponds to {[}X{]} change on ANES survey
  \end{enumerate}
\item
  Illuminate empirical gaps

  \begin{enumerate}
  \def\labelenumii{\arabic{enumii}.}
  \setcounter{enumii}{6}
  \tightlist
  \item
    As our reviews show, applying some fairly minimal quality standards quickly winnows hundreds of studies down to just a few
  \item
    These studies are likely to be the most policy-relevant, and oftentimes the `ideal' study simply hasn't been conducted at all yet
  \item
    Examples:

    \begin{enumerate}
    \def\labelenumiii{\arabic{enumiii}.}
    \tightlist
    \item
      Paluck, Green and Green (2019) highlight lack of studies testing interracial contact among adults -\textgreater{} provide intellectual backdrop for Scacco and Warren (2018), Mousa (2020), and Lowe (2019) -- as well as providing theoretical underpinning for those studies' comparatively underwhelming findings
      2 similar examples from Paluck et al.~(2021)?
    \item
      (arguably a systematic review suffices for this)
    \end{enumerate}
  \end{enumerate}
\end{enumerate}

\subsubsection{Who are they for?}\label{who-are-they-for}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  researchers, especially folks trying to figure out where they can have the most impact
\item
  policymakers and funders who want a critical review of a literature
\item
  any interested reader. Our metas tend to be about big, important subjects that people are thinking about.
  \#\# Typical structure of Paluck lab meta-analytic papers
\item
  Describe context and stakes of the paper (i.e.~why it's important to see if these interventions `work', or why there's reason to doubt an established consensus)
\item
  Intellectual overview of major ideas in the literature
\item
  Meta-analytic search methods
\item
  Meta-analytic approach
\item
  Quantitative Results
\item
  Discussion, highlights and gaps
\item
  Conclusion
  \#\# Paluck lab meta-analytic procedures
\item
  Code more than you think you're going to need

  \begin{enumerate}
  \def\labelenumii{\arabic{enumii}.}
  \tightlist
  \item
    You might be interested in the lasting effects, which means recording the latest possible effect sizes, but if you record the \emph{earliest} possible effects, you can detect within-study decay
  \end{enumerate}
\item
  Start big and then go small, like a funnel; i.e.~meta-analyze \emph{everything} and then zoom in on different subsets of the literature

  \begin{enumerate}
  \def\labelenumii{\arabic{enumii}.}
  \setcounter{enumii}{1}
  \tightlist
  \item
    Why meta-analyze everything?

    \begin{enumerate}
    \def\labelenumiii{\arabic{enumiii}.}
    \tightlist
    \item
      Slough and Tyson (2023) argue that meta-analytic estimates only make sense if there's contrast harmony, i.e.~if the many studies are clearly testing some common underlying theoretical concept. In some literatures we've looked at, this pretty clearly does not hold, e.g.~the average effect of a contact hypothesis field experiment for Muslims and Christians and Nigeria and a diversity training in a U.S. corporate workplace tells you the combined effect of \emph{what} exactly?
    \item
      Ditto with ``target harmony'', which is whether interventions are aimed \emph{at} changing the same thing. For instance, does it make sense to average the effect of some intervention on self-reported attitudes and behavioral outcomes? As Porat et al. (2024) show, these outcomes are not always well correlated, which is prima facie evidence that we don't have target harmony.
    \item
      Last, integrating observational and randomized studies is often justified as supplementing high studies with high internal validity with studies with high \emph{external} validity, but it's probably more accurate to say that you take some studies that provide unbiased estimates and then, in exchange for greater statistical precision, induce bias arising from non-statistical sources of uncertainty (\href{http://www.donaldgreen.com/wp-content/uploads/2015/09/Gerber-Green-Kaplan-IllusionofLearning.pdf}{Gerber, Green and Kaplan 2004}).
    \end{enumerate}
  \item
    multi-part answer.

    \begin{enumerate}
    \def\labelenumiii{\arabic{enumiii}.}
    \setcounter{enumiii}{3}
    \tightlist
    \item
      Overall meta-analytic estimate provides a within-paper size comparison; your overall effect size is X and your effect size for the very best studies is 1/3 X, that means something.
    \item
      Test for publication bias should probably look at absolutely everything
    \item
      Inter-paper comparisons; Paluck, Green and Green (2019) and Paluck et al.~(2021) provide estimates of about \(\Delta\) = 0.3, and then Green, Smith and Mathur (forthcoming) find \(\Delta\) = 0.138, which also means something.
    \end{enumerate}
  \end{enumerate}
\item
  Do some serious tests for publication bias: both conventional tests (egger's test, funnel plot, \(\Delta\) \textasciitilde{} SE), but also think through where else it might emerge, e.g.~compare effect sizes in studies with and without DOIs
\item
  separate the dataset into different chunks, e.g.~by theory, study design, or measurement strategy, and present these meta-analytic estimates side by side

  \begin{enumerate}
  \def\labelenumii{\arabic{enumii}.}
  \setcounter{enumii}{3}
  \tightlist
  \item
    Subset enough and eventually you get to reasonable claim for contrast and mechanism harmony (Slough and Tyson 2022)
  \end{enumerate}
\item
  Zoom in on and carefully and analyze best studies, both in aggregate and individually

  \begin{enumerate}
  \def\labelenumii{\arabic{enumii}.}
  \setcounter{enumii}{4}
  \tightlist
  \item
    These results might be surprising, e.g.~when looking solely at RCTs on perpetration outcomes (Porat et al., 2024), slightly fewer than half are self-reported nulls.
  \end{enumerate}
\item
  Meta-analytic defaults

  \begin{enumerate}
  \def\labelenumii{\arabic{enumii}.}
  \setcounter{enumii}{5}
  \tightlist
  \item
    Random effects: any literature we want to look at is going to have heterogeneous inputs.
  \item
    Cluster at level of study
  \item
    Glass's \(\Delta\) rather than Cohen's d
  \item
    Difference in proportion rather than odds ratio

    \begin{enumerate}
    \def\labelenumiii{\arabic{enumiii}.}
    \setcounter{enumiii}{6}
    \tightlist
    \item
      resurface text from appendix to Porat et al.~(2024)
    \end{enumerate}
  \end{enumerate}
\end{enumerate}

\subsection{R package: blp\_meta\_functions}\label{r-package-blp_meta_functions}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Functions fall into four categories

  \begin{enumerate}
  \def\labelenumii{\arabic{enumii}.}
  \tightlist
  \item
    Converting studies to singular estimates of effect size, variance and standard error
  \item
    Wrapper functions that distill aggregate results into the core findings and make them table-ready
  \item
    plotting functions
  \item
    Miscellany: reproducibility, helper functions
  \end{enumerate}
\end{enumerate}

\subsection{Conclusions: hard cases}\label{conclusions-hard-cases}

This paper seeks to answer two questions: what are we aiming to do when we do meta-analysis, and how do we do it?
- hope we've answered these questions satisfactorily but the big open question has to do with the extent to which metas are or are not hypothesis-driven. Other metas in the social sciences are \emph{clearly} hypothesis driven but ours are more exploratory because they're, in a word, much more meta, much more evaluative and open-ended.
- A lot of checks on RDF assume hypothesis driven framework. They are difficult to integrate into our papers.
1. What do we do when a paper's results are obviously not credible?
1. e.g.~if they present a result that's well past possible, e.g.~t-test value of 36 (one of the papers in the prejudice literature had this, and also the cost-benefit analyses in the SOSA! Intervention in the primary prevention literature)
2. What's the right outcome to code? Often very unclear what's prime or most representative of underlying construct
3. Others\ldots{}
4. Meta-analyses are still very useful, we think, for readers (both expert and non-expert) and future researchers.

\phantomsection\label{refs}
\begin{CSLReferences}{1}{0}
\bibitem[\citeproctext]{ref-bem2015}
Bem, D., Tressoldi, P., Rabeyron, T., \& Duggan, M. (2015). Feeling the future: A meta-analysis of 90 experiments on the anomalous anticipation of random future events. \emph{F1000Research}, \emph{4}.

\bibitem[\citeproctext]{ref-bezrukova2016}
Bezrukova, K., Spell, C. S., Perry, J. L., \& Jehn, K. A. (2016). A meta-analytical integration of over 40 years of research on diversity training evaluation. \emph{Psychological Bulletin}, \emph{142}(11), 1227.

\bibitem[\citeproctext]{ref-cooper2019}
Cooper, H., Hedges, L. V., \& Valentine, J. C. (2019). \emph{The handbook of research synthesis and meta-analysis}. Russell Sage Foundation.

\bibitem[\citeproctext]{ref-cuddy2018}
Cuddy, A. J., Schultz, S. J., \& Fosse, N. E. (2018). P-curving a more comprehensive body of research on postural feedback reveals clear evidential value for power-posing effects: Reply to simmons and simonsohn (2017). \emph{Psychological Science}, \emph{29}(4), 656--666.

\bibitem[\citeproctext]{ref-elster1977}
Elster, J. (1977). Ulysses and the sirens: A theory of imperfect rationality. \emph{Social Science Information}, \emph{16}(5), 469--526.

\bibitem[\citeproctext]{ref-ferguson2023}
Ferguson, J., Littman, R., Christensen, G., Paluck, E. L., Swanson, N., Wang, Z., \ldots{} Pezzuto, J.-H. (2023). Survey of open science practices and attitudes in the social sciences. \emph{Nature Communications}, \emph{14}(1), 5401.

\bibitem[\citeproctext]{ref-glass1977}
Glass, G. V. (1977). 9: Integrating findings: The meta-analysis of research. \emph{Review of Research in Education}, \emph{5}(1), 351--379.

\bibitem[\citeproctext]{ref-gomila2021}
Gomila, R. (2021). Logistic or linear? Estimating causal effects of experimental treatments on binary outcomes using regression analysis. \emph{Journal of Experimental Psychology: General}, \emph{150}(4), 700.

\bibitem[\citeproctext]{ref-hardwicke2021}
Hardwicke, T. E., Bohn, M., MacDonald, K., Hembacher, E., Nuijten, M. B., Peloquin, B. N., \ldots{} Frank, M. C. (2021). Analytic reproducibility in articles receiving open data badges at the journal psychological science: An observational study. \emph{Royal Society Open Science}, \emph{8}(1), 201494.

\bibitem[\citeproctext]{ref-hardwicke2018}
Hardwicke, T. E., Mathur, M. B., MacDonald, K., Nilsonne, G., Banks, G. C., Kidwell, M. C., et al.others. (2018). Data availability, reusability, and analytic reproducibility: Evaluating the impact of a mandatory open data policy at the journal cognition. \emph{Royal Society Open Science}, \emph{5}(8), 180448.

\bibitem[\citeproctext]{ref-hardwicke2023transparency}
Hardwicke, T. E., \& Vazire, S. (2023). Transparency is now the default at psychological science. SAGE Publications Sage CA: Los Angeles, CA.

\bibitem[\citeproctext]{ref-kleinstauber1996}
Kleinstäuber, M., Witthöft, M., Steffanowski, A., Van Marwijk, H., Hiller, W., \& Lambert, M. J. (1996). \emph{Cochrane database of systematic reviews}.

\bibitem[\citeproctext]{ref-lakens2023}
Lakens, D. (2023). \emph{When and how to deviate from a preregistration}. \url{https://doi.org/10.31234/osf.io/ha29k}.

\bibitem[\citeproctext]{ref-lin2016}
Lin, W., \& Green, D. P. (2016). Standard operating procedures: A safety net for pre-analysis plans. \emph{PS: Political Science \& Politics}, \emph{49}(3), 495--500.

\bibitem[\citeproctext]{ref-nosek2015}
Nosek, A., B. A. (2015). Promoting an open research culture. \emph{Science}, \emph{348}(6242), 1422--1425.

\bibitem[\citeproctext]{ref-obels2020}
Obels, P., Lakens, D., Coles, N. A., Gottfried, J., \& Green, S. A. (2020). Analysis of open data and computational reproducibility in registered reports in psychology. \emph{Advances in Methods and Practices in Psychological Science}, \emph{3}(2), 229--237.

\bibitem[\citeproctext]{ref-paluck2009}
Paluck, E. L., \& Green, D. P. (2009). Prejudice reduction: What works? A review and assessment of research and practice. \emph{Annual Review of Psychology}, \emph{60}, 339--367.

\bibitem[\citeproctext]{ref-paluck2019}
Paluck, E. L., Green, S. A., \& Green, D. P. (2019). The contact hypothesis re-evaluated. \emph{Behavioural Public Policy}, \emph{3}(2), 129--158.

\bibitem[\citeproctext]{ref-paluck2021}
Paluck, E. L., Porat, R., Clark, C. S., \& Green, D. P. (2021). Prejudice reduction: Progress and challenges. \emph{Annual Review of Psychology}, \emph{72}, 533--560.

\bibitem[\citeproctext]{ref-pettigrew2006}
Pettigrew, T. F., \& Tropp, L. R. (2006). A meta-analytic test of intergroup contact theory. \emph{Journal of Personality and Social Psychology}, \emph{90}(5), 751.

\bibitem[\citeproctext]{ref-porat2024}
Porat, R., Gantman, A., Paluck, E. L., Green, S. A., \& Pezzuto, J.-H. (2024). Preventing sexual violence -- a behavioral problem without a behaviorally-informed solution. \emph{Psychological Science in the Public Interest}, \emph{25}(1), 1--30.

\bibitem[\citeproctext]{ref-rapport2013}
Rapport, F., Storey, M., Porter, A., Snooks, H., Jones, K., Peconi, J., et al.others. (2013). Qualitative research within trials: Developing a standard operating procedure for a clinical trials unit. \emph{Trials}, \emph{14}(1), 1--8.

\bibitem[\citeproctext]{ref-schooler2014}
Schooler, J. W. (2014). Metascience could rescue the {``replication crisis.''} \emph{Nature}, \emph{515}(7525), 9--9.

\bibitem[\citeproctext]{ref-simonsohn2022}
Simonsohn, U., Simmons, J., \& Nelson, L. D. (2022). Above averaging in literature reviews. \emph{Nature Reviews Psychology}, \emph{1}(10), 551--552.

\bibitem[\citeproctext]{ref-slough2023}
Slough, T., \& Tyson, S. A. (2023). External validity and meta-analysis. \emph{American Journal of Political Science}, \emph{67}(2), 440--455.

\bibitem[\citeproctext]{ref-viechtbauer2010}
Viechtbauer, W. (2010). Conducting meta-analyses in r with the metafor package. \emph{Journal of Statistical Software}, \emph{36}, 1--48.

\bibitem[\citeproctext]{ref-wallace2015}
Wallace, M., Hamesch, K., Lunova, M., Kim, Y., Weiskirchen, R., Strnad, P., \& Friedman, S. (2015). Standard operating procedures in experimental liver research: Thioacetamide model in mice and rats. \emph{Laboratory Animals}, \emph{49}(1\_suppl), 21--29.

\end{CSLReferences}


\end{document}
