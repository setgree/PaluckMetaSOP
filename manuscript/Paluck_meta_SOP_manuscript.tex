% Options for packages loaded elsewhere
\PassOptionsToPackage{unicode}{hyperref}
\PassOptionsToPackage{hyphens}{url}
%
\documentclass[
  ,jou]{apa6}
\usepackage{amsmath,amssymb}
\usepackage{iftex}
\ifPDFTeX
  \usepackage[T1]{fontenc}
  \usepackage[utf8]{inputenc}
  \usepackage{textcomp} % provide euro and other symbols
\else % if luatex or xetex
  \usepackage{unicode-math} % this also loads fontspec
  \defaultfontfeatures{Scale=MatchLowercase}
  \defaultfontfeatures[\rmfamily]{Ligatures=TeX,Scale=1}
\fi
\usepackage{lmodern}
\ifPDFTeX\else
  % xetex/luatex font selection
\fi
% Use upquote if available, for straight quotes in verbatim environments
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
\IfFileExists{microtype.sty}{% use microtype if available
  \usepackage[]{microtype}
  \UseMicrotypeSet[protrusion]{basicmath} % disable protrusion for tt fonts
}{}
\makeatletter
\@ifundefined{KOMAClassName}{% if non-KOMA class
  \IfFileExists{parskip.sty}{%
    \usepackage{parskip}
  }{% else
    \setlength{\parindent}{0pt}
    \setlength{\parskip}{6pt plus 2pt minus 1pt}}
}{% if KOMA class
  \KOMAoptions{parskip=half}}
\makeatother
\usepackage{xcolor}
\usepackage{graphicx}
\makeatletter
\def\maxwidth{\ifdim\Gin@nat@width>\linewidth\linewidth\else\Gin@nat@width\fi}
\def\maxheight{\ifdim\Gin@nat@height>\textheight\textheight\else\Gin@nat@height\fi}
\makeatother
% Scale images if necessary, so that they will not overflow the page
% margins by default, and it is still possible to overwrite the defaults
% using explicit options in \includegraphics[width, height, ...]{}
\setkeys{Gin}{width=\maxwidth,height=\maxheight,keepaspectratio}
% Set default figure placement to htbp
\makeatletter
\def\fps@figure{htbp}
\makeatother
\setlength{\emergencystretch}{3em} % prevent overfull lines
\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}
\setcounter{secnumdepth}{-\maxdimen} % remove section numbering
% Make \paragraph and \subparagraph free-standing
\ifx\paragraph\undefined\else
  \let\oldparagraph\paragraph
  \renewcommand{\paragraph}[1]{\oldparagraph{#1}\mbox{}}
\fi
\ifx\subparagraph\undefined\else
  \let\oldsubparagraph\subparagraph
  \renewcommand{\subparagraph}[1]{\oldsubparagraph{#1}\mbox{}}
\fi
% definitions for citeproc citations
\NewDocumentCommand\citeproctext{}{}
\NewDocumentCommand\citeproc{mm}{%
  \begingroup\def\citeproctext{#2}\cite{#1}\endgroup}
\makeatletter
 % allow citations to break across lines
 \let\@cite@ofmt\@firstofone
 % avoid brackets around text for \cite:
 \def\@biblabel#1{}
 \def\@cite#1#2{{#1\if@tempswa , #2\fi}}
\makeatother
\newlength{\cslhangindent}
\setlength{\cslhangindent}{1.5em}
\newlength{\csllabelwidth}
\setlength{\csllabelwidth}{3em}
\newenvironment{CSLReferences}[2] % #1 hanging-indent, #2 entry-spacing
 {\begin{list}{}{%
  \setlength{\itemindent}{0pt}
  \setlength{\leftmargin}{0pt}
  \setlength{\parsep}{0pt}
  % turn on hanging indent if param 1 is 1
  \ifodd #1
   \setlength{\leftmargin}{\cslhangindent}
   \setlength{\itemindent}{-1\cslhangindent}
  \fi
  % set entry spacing
  \setlength{\itemsep}{#2\baselineskip}}}
 {\end{list}}
\usepackage{calc}
\newcommand{\CSLBlock}[1]{\hfill\break\parbox[t]{\linewidth}{\strut\ignorespaces#1\strut}}
\newcommand{\CSLLeftMargin}[1]{\parbox[t]{\csllabelwidth}{\strut#1\strut}}
\newcommand{\CSLRightInline}[1]{\parbox[t]{\linewidth - \csllabelwidth}{\strut#1\strut}}
\newcommand{\CSLIndent}[1]{\hspace{\cslhangindent}#1}
\ifLuaTeX
\usepackage[bidi=basic]{babel}
\else
\usepackage[bidi=default]{babel}
\fi
\babelprovide[main,import]{english}
% get rid of language-specific shorthands (see #6817):
\let\LanguageShortHands\languageshorthands
\def\languageshorthands#1{}
% Manuscript styling
\usepackage{upgreek}
\captionsetup{font=singlespacing,justification=justified}

% Table formatting
\usepackage{longtable}
\usepackage{lscape}
% \usepackage[counterclockwise]{rotating}   % Landscape page setup for large tables
\usepackage{multirow}		% Table styling
\usepackage{tabularx}		% Control Column width
\usepackage[flushleft]{threeparttable}	% Allows for three part tables with a specified notes section
\usepackage{threeparttablex}            % Lets threeparttable work with longtable

% Create new environments so endfloat can handle them
% \newenvironment{ltable}
%   {\begin{landscape}\centering\begin{threeparttable}}
%   {\end{threeparttable}\end{landscape}}
\newenvironment{lltable}{\begin{landscape}\centering\begin{ThreePartTable}}{\end{ThreePartTable}\end{landscape}}

% Enables adjusting longtable caption width to table width
% Solution found at http://golatex.de/longtable-mit-caption-so-breit-wie-die-tabelle-t15767.html
\makeatletter
\newcommand\LastLTentrywidth{1em}
\newlength\longtablewidth
\setlength{\longtablewidth}{1in}
\newcommand{\getlongtablewidth}{\begingroup \ifcsname LT@\roman{LT@tables}\endcsname \global\longtablewidth=0pt \renewcommand{\LT@entry}[2]{\global\advance\longtablewidth by ##2\relax\gdef\LastLTentrywidth{##2}}\@nameuse{LT@\roman{LT@tables}} \fi \endgroup}

% \setlength{\parindent}{0.5in}
% \setlength{\parskip}{0pt plus 0pt minus 0pt}

% Overwrite redefinition of paragraph and subparagraph by the default LaTeX template
% See https://github.com/crsh/papaja/issues/292
\makeatletter
\renewcommand{\paragraph}{\@startsection{paragraph}{4}{\parindent}%
  {0\baselineskip \@plus 0.2ex \@minus 0.2ex}%
  {-1em}%
  {\normalfont\normalsize\bfseries\itshape\typesectitle}}

\renewcommand{\subparagraph}[1]{\@startsection{subparagraph}{5}{1em}%
  {0\baselineskip \@plus 0.2ex \@minus 0.2ex}%
  {-\z@\relax}%
  {\normalfont\normalsize\itshape\hspace{\parindent}{#1}\textit{\addperi}}{\relax}}
\makeatother

\makeatletter
\usepackage{etoolbox}
\patchcmd{\maketitle}
  {\section{\normalfont\normalsize\abstractname}}
  {\section*{\normalfont\normalsize\abstractname}}
  {}{\typeout{Failed to patch abstract.}}
\patchcmd{\maketitle}
  {\section{\protect\normalfont{\@title}}}
  {\section*{\protect\normalfont{\@title}}}
  {}{\typeout{Failed to patch title.}}
\makeatother

\usepackage{xpatch}
\makeatletter
\xapptocmd\appendix
  {\xapptocmd\section
    {\addcontentsline{toc}{section}{\appendixname\ifoneappendix\else~\theappendix\fi\\: #1}}
    {}{\InnerPatchFailed}%
  }
{}{\PatchFailed}
\keywords{meta-analysis, standard-operating-procedures, meta-science\newline\indent Word count: 6266}
\usepackage{dblfloatfix}


\usepackage{csquotes}
\ifLuaTeX
  \usepackage{selnolig}  % disable illegal ligatures
\fi
\usepackage{bookmark}
\IfFileExists{xurl.sty}{\usepackage{xurl}}{} % add URL line breaks if available
\urlstyle{same}
\hypersetup{
  pdftitle={Towards meta-scientific meta-analyses: standard operating procedures for meta-analysis in the Paluck Lab},
  pdfauthor={Seth A. Green1, Elizabeth Levy Paluck2, \& Roni Porat3},
  pdflang={en-EN},
  pdfkeywords={meta-analysis, standard-operating-procedures, meta-science},
  hidelinks,
  pdfcreator={LaTeX via pandoc}}

\title{Towards meta-scientific meta-analyses: standard operating procedures for meta-analysis in the Paluck Lab}
\author{Seth A. Green\textsuperscript{1}, Elizabeth Levy Paluck\textsuperscript{2}, \& Roni Porat\textsuperscript{3}}
\date{}


\shorttitle{Paluck\_meta\_SOP}

\authornote{

This work was supported by {[}whoever supports it{]}. Thanks to Tara Slough, Don Green, and Alix Winter for valuable comments on an early draft.

The authors made the following contributions. Seth A. Green: Conceptualization, Writing - Original Draft Preparation, Writing - Review \& Editing; Elizabeth Levy Paluck: Writing - Review \& Editing, Supervision; Roni Porat: Writing - Review \& Editing, Supervision.

}

\affiliation{\vspace{0.5cm}\textsuperscript{1} Kahneman-Treisman Center, Princeton University\\\textsuperscript{2} Princeton University\\\textsuperscript{3} Hebrew University, Jerusalem}

\abstract{%
This paper describes the motivations and procedures for meta-analyses by Betsy Levy Paluck and co-authors, with reference to and examples drawn from three such papers (Paluck, Green, \& Green, 2019; Paluck, Porat, Clark, \& Green, 2021; Porat, Gantman, Paluck, Green, \& Pezzuto, 2024). We first describe our overall aims for meta-analysis, and how we think through recent trenchant critiques from Slough and Tyson (2023) and Simonsohn, Simmons, and Nelson (2022). Second, we describe the tradeoff between selective and comprehensive meta-analyses, arguing in favor of comprehensive reviews if resources allow. Third, we describe the typical `flow' of the papers as a whole. Fourth, we describe our meta-analytic procedures, detailing our default analytic choices and introducing an R package that implements those choices. These `Standard Operating Procedures' are intended as a guide to a handful of especially thorny meta-analytic issues. We conclude with some open questions for meta-analysts.
}



\begin{document}
\maketitle

\section{Introduction: bridging the gap between meta-analysis and meta-science}\label{introduction-bridging-the-gap-between-meta-analysis-and-meta-science}

What purposes do meta-analyses serve above and beyond those accomplished by systematic reviews? Judging by recent papers in social psychology (Bem, Tressoldi, Rabeyron, \& Duggan, 2015; Bezrukova, Spell, Perry, \& Jehn, 2016; Cuddy, Schultz, \& Fosse, 2018; Pettigrew \& Tropp, 2006), the answer is to bolster and validate existing findings through aggregation. By pooling findings from many studies, researchers furnish meta-analytic estimates that are statistically precise and drawn from diverse settings, giving readers the impression of durable, robust findings. Such papers often have a valedictory quality to them: they provide leading practitioners a chance to survey the body of work they've helped inspire, highlight the strength of the evidence for their subfield's central tenets, and suggest future research directions that assume those tenets are no longer in need of testing.

Meanwhile, a parallel movement in psychology has begun to implement the recommendations of the open science (Nosek, 2015) and meta-science (Schooler, 2014) movements into study designs (Ferguson et al., 2023), journal policies (Hardwicke \& Vazire, 2023), and literature reviews (Hardwicke et al., 2018). Broadly speaking, meta-science is a set of practices, as well as ``an evolving scientific discipline,'' aiming to ``evaluate and improve research practices;'' instead of theorizing ``about biases (e.g., publication bias, reporting bias, selection bias, confounding, etc.),'' meta-scientists ``examine them across multiple studies'' and ``think about ways to prevent or correct them (Ioannidis, Fanelli, Dunne, \& Goodman, 2015).

Many meta-scientific papers quantitatively assess collective outcomes, for instance, what percentage of a subset of papers are computationally reproducible (Obels, Lakens, Coles, Gottfried, \& Green, 2020) or consistent with open data standards (Hardwicke et al., 2021). In one sense, these estimates are `meta-analytic,' but they aren't meta-analyses in the conventional sense of pooling existing findings into weighted average effects (Cooper, Hedges, \& Valentine, 2019; Glass, 1977; Kleinstäuber et al., 1996). Likewise, while most meta-analyses attempt to assess publication bias (Sutton, 2009; Thornton \& Lee, 2000), which is a fundamentally meta-scientific quantity, the remainder of their analyses typically take the internal validity of the field's findings for granted. We also observe that meta-science papers are much more likely to be written by outsiders or newcomers to a field, and to adopt a much more critical tone, than we see in the typical meta-analysis. Thus, their similar names notwithstanding, meta-analytic and meta-scientific papers in social psychology typically have radically different perspectives, assumptions, and goals.

At the Paluck lab, we seek to synthesize these two approaches by writing meta-analyses that are fundamentally meta-scientific. While writing three such papers (Paluck \& Green, 2009; Paluck et al., 2019, 2021; Porat et al., 2024), we've developed distinct beliefs about when a meta-analysis is useful, how to structure such papers, and which specifications are sensible defaults. This paper articulates those beliefs, and aims to illuminate how meta-analysis might become a standard component of the meta-scientific toolkit.

\subsection{Implementing meta-analytic theory with Standard Operating Procedures}\label{implementing-meta-analytic-theory-with-standard-operating-procedures}

A healthy literature has interrogated meta-analytic methods, noting common pitfalls (Greco, Zangrillo, Biondi-Zoccai, \& Landoni, 2013), statistical errors (Kadlec, Sainani, \& Nimphius, 2023), and practical limitations, e.g.~a widespread lack of reproducibility (Daniel Lakens et al., 2017) or data availability (López-Nicolás et al., 2024). This paper sits in that tradition, but is a bit of an offshoot in that we are not attempting to correct statistical or practical mistakes in meta-analyses, but rather articulate what it is that we are trying to do when we write them. In this regard, this paper is closely in dialogue with two recent contributions that attempt to put meta-analysis on firmer theoretical footing.

Slough and Tyson (2023) ``highlight the dangers of conflating conceptual differences across studies with statistical sources of variation'' (p.~29). They argue that for studies to have target equivalence (the property of identifying ``the same estimand'' (p.~1)), they must have fundamentally \emph{harmonized} contrasts and measurement strategies: the ``substantive comparison across studies is the same'' and ``the outcome of interest is the same and it is measured in the same way'' (p.~2). Absent these conditions, meta-analytic results might not be ``meaningful and interpretable'' (p.~2). Further, target equivalence cannot be achieved ``solely with statistical techniques'' (p.~23); it is a property, rather, of appropriate ``design or inclusion criteria'' (p.~2).

We agree that an internally and externally valid causal meta-analytic estimate requires harmonization of contrasts and outcomes, as well as a high degree of internal validity within constituent studies. However, we also think that meta-analytic estimates which do \emph{not} meet these conditions \textemdash those that combine results from many different designs, or whose diverse outcomes bear an unknown relationship to the true outcome of interest \textemdash can still be useful and informative if we ascribe them \emph{descriptive} rather than causal interpretation. Meta-analyzing a literature without clear harmony of independent and dependent variables is a case where the underlying model might be ``wrong'' but still useful.

The other paper we most closely engage with is Simonsohn et al. (2022), who argue that while the typical meta-analysis in social psychology aims ``to be comprehensive, results-focused and transcriptive,'' this is ``misguided, leading to uninterpretable results that misrepresent research literatures.'' First, they argue that a desire for comprehensiveness typically leads the meta- analyst to average ``studies of the highest quality'' with ``studies that lack internal validity or external validity, which are obtained using incorrect statistical techniques, or studies where results seem to arise from methodological artefacts.'' This throw-it-all-in-the-blender approach produces results that are ``virtually guaranteed to lack a meaningful interpretation'' (p.~XXX) despite their superficial statistical precision. Second, the authors encourage meta-analysts to focus on studies' designs, rather than just their results, to provide ``readers with information that they can use to evaluate'' (p.~XXX) the quality of a research literature for themselves. Rather than describing every study, meta-analysts ``can succinctly describe and summarize common design failures\ldots and provide detailed descriptions only of the studies they deem most important or compelling'' (p.~XXX) This advice ties closely to their last dictum to meta-analysts: to evaluate rather than to transcribe, i.e., detail the evidence from the very best studies, identify common methodological shortcomings of the literature as a whole, and, finally, ``discuss what open questions remain and what kinds of evidence would help answer those questions'' (p.~XXX).

Our meta-analyses share these goals. However, we part ways from this paper in that we believe comprehensive meta-analytic estimates still have a place in the meta-scientist's toolkit, so long as those estimates are ascribed non-causal meaning and are used to contextualize more selective analyses.

This paper also builds on a tradition of Standard Operating Procedures (SOPs) in the behavioral and medical sciences (D. Lakens, 2023; Rapport et al., 2013; Wallace et al., 2015). In particular, we look to Lin and Green (2016), which details how the Green lab at Columbia sets ``default practices to guide decisions when issues arise that were not anticipated'' in pre-analysis plans.

Like that paper, this article is not intended as a comprehensive guide. Specifically, we do not cover identifying an interesting question where aggregating results from many studies will be a useful estimation strategy or searching for those studies (see Paul and Barari (2022) for a concise, step-by-step treatment, as well as Cumpston et al. (2019) and Cooper et al. (2019) for textbook-length treatments of these questions). Instead, we describe how we tackle a few especially thorny issues relating to coding studies and aggregating their results. Most of this paper is about aggregation. Our default procedures are designed to help readers evaluate a body of research for themselves. We hope this will prove interesting and informative to other labs, but not definitive: we expect readers to adapt and expand our guidelines to meet their own needs. (See also Harrer, Cuijpers, Furukawa, and Ebert (2021), Frank et al. (2024), chapter 16 and Daniel Lakens (2022), chapter 11 for hands-on guides to meta-analysis with examples in \texttt{R}.)

\subsection{The remainder of this paper}\label{the-remainder-of-this-paper}

This paper is a hybrid of a conceptual analysis of meta-analyses in general and a report on how we in particular write them. The first section articulates our general approach to meta-analysis. The second describes our typical approach to writing meta-analytic papers. The third details our meta-analytic defaults and introduces an R package that implements them and helps us write papers. Our conclusion recaps and notes some open questions for our lab.

First, we describe our conceptual approach to meta-analysis. We believe that meta-analyses are useful for evaluating the strengths and weaknesses of large, heterogeneous literatures, especially those that bring multiple theoretical perspectives to bear.

Second, We discuss the trade-off between, on the one hand, comprehensive but potentially biased meta-analyses and, on the other, narrower quantitative reviews that set strict inclusion criteria around study designs, interventions, and/or dependent variables.

Third, we describe the default structure and analytic defaults for our quantitative reviews. Our papers explain the main ideas and trends in a literature, our search processes and meta-analytic methods, meta-analytic findings, and a review of open questions, while our quantitative sections are structured to first present a literature's findings at face value, and then analyze subsets of the literature based on differences in study design, measurement strategy, intervention approach, and estimand.

Third, we detail our meta-analytic defaults, e.g.~a preference for Glass's \(\Delta\) over Cohen's \emph{d}, random effects over fixed effects estimates, and clustering our standard errors at the level of a study (as opposed to a paper or a team of authors). We then introduce an R package, \texttt{PaluckMetaSOP}, that contains functions to help us quickly and efficiently perform meta-analyses.

Fourth and finally, we conclude with some open questions for meta-scientific meta-analyses. Our meta-analyses straddle the line between inductive and deductive inquiries, and thus are not an easy fit with conventional approaches to checking researcher degrees of freedom, which assume a deductive approach. We also discuss our evolving thinking about incorporating indirect cues of study quality, such as statistical mistakes (a negative signal) or special attention devoted to reproducibility (a positive one), into our analyses in a principled way.

We now detail our conceptual aims when we write meta-analyses.

\section{The purpose of meta-analysis}\label{the-purpose-of-meta-analysis}

The main goals of our meta-analyses are the goals of any systematic review: to evaluate a body of knowledge for its strengths, weaknesses, central findings, open questions and empirical limitations. We write broadly for any reader who is interested in the underlying question, but generally aim for a non-specialist audience. In our experience, a literature's most noteworthy contributions often come from researchers outside the core field (e.g. Mousa (2020), Boisjoly, Duncan, Kremer, Levy, and Eccles (2006), or Scacco and Warren (2018) for the contact hypothesis, Munger (2017) for anti-prejudice work, or Haushofer, Ringdal, Shapiro, and Wang (2019) for violence against women); to such researchers, we aim to provide signposts, in the form of unearthed, widespread empirical gaps and theoretical limitations, about where future empirical work will be most valuable. We also believe that funders and policymakers will benefit from systematic, critical reviews that do not take subject matter expertise for granted.

\subsection{What makes a literature a good candidate for meta-analysis?}\label{what-makes-a-literature-a-good-candidate-for-meta-analysis}

Large, heterogeneous literatures, especially those with multiple theoretical perspectives, are good candidates for meta-analysis. In such cases, a meta-analysis might provide a useful contribution simply by dividing a literature into its constituent parts. For example, Paluck et al. (2021) categorizes 418 experiments published between 2007 and 2019 and organizes them into thirteen theoretical approaches and twelve categories of prejudice targeted. Simply identifying this heterogeneity of approaches and outcomes, along with describing a few constituent studies from the major theoretical strands in detail, helps the reader evaluate a literature's major findings for herself. She might reasonably wonder: does an intervention that aims to change political attitudes towards the rights of trans persons (Broockman \& Kalla, 2016) shed light on what might reduce tensions between castes in India (Lowe, 2021)? Is an extended contact intervention (Shamoa-Nir \& Razpurker-Apfeld, 2023) similar enough to a diversity training (Chang et al., 2019) that you can pool the effects together into something that coherently reflects the effects of a class of intervention? In other words, can these designs and outcomes be pooled in a sensible way, or should separate strands of the research be evaluated separately? These questions come down to beliefs, and, as Slough and Tyson (2023) argue, cannot be resolved with statistics. A comprehensive and well-organized review is an indispensable tool for clarifying which questions to ask in the first place.

We also encourage authors not to be discouraged from writing meta-analyses of subjects that have been previously reviewed and analyzed, even many times. First, in our experience, simply reproducing the work of previous meta-analyses verbatim might generate surprises, e.g.~studies that have been miscoded or questionable applications of inclusion criteria. Second, we share Munger's (2023) concern for ``temporal validity:'' because the conditions under which findings hold are likely to change over time, ``no research design, no empirical knowledge, is perfectible'' (p.~1). For instance, the implicit bias (Gawronski, 2019) and symbolic racism (McConahay \& Hough Jr, 1976; Sears \& Henry, 2003) research agendas emerged because researchers observed a dramatic drop in how readily white Americans would express overtly racist ideas in the 1970s and 80s, which called for a paradigm shift in measurement. Meta-analyses are essential for assessing which classes of interventions still `work' in light of such paradigm shifts.

Third, as the credibility revolution spreads across the social sciences (Angrist \& Pischke, 2010; Samii, 2016; Vazire, 2018), we can generally expect recent research to meet comparatively higher standards of rigor and transparency, and therefore have a disproportionate impact on our understanding of the world.

Fourth, even in fields with many systematic reviews, there's still often an open lane for a paper that's laser-focused on the findings of the best, most policy-relevant research (Paluck et al., 2019) or that's both comprehensive and quantitative. For instance, while reviewing prior papers on sexual violence for Porat et al. (2024), we found no theoretically comprehensive reviews that also had meta-analyses: all quantitative reviews were focused on smaller subsets of the literature.

\subsection{Addressing contemporary criticisms by relaxing the causality assumption}\label{addressing-contemporary-criticisms-by-relaxing-the-causality-assumption}

Slough and Tyson (2023) and Simonsohn et al. (2022) have raised trenchant critiques about whether the typical meta-analytic estimate is meaningful. Both papers argue that meta-analyses should be more selective in what papers they include \textemdash Slough and Tyson (2023) to assure harmonization of the independent and dependent variables, and Simonsohn et al. (2022) to prevent mixing ``studies of the highest quality'' with ``studies that lack internal validity'' (p.~XXX).

Of our three meta-analyses, Paluck et al. (2019) most closely hews to this model of inquiry. That paper looked solely at randomized controlled trials where intergroup contact was the treatment variable and where there was at least a day of delay between the commencement of treatment and outcome measurement. Our subsequent two meta-analyses (Paluck et al., 2021; Porat et al., 2024), however, combine diverse intervention strategies, outcomes, and study designs. It is worth asking whether the resulting analyses we provide are unbiased and conceptually coherent estimates of overall effects. Consider the sexual violence literature, which includes, for instance, a building-based intervention comprising ``higher levels of faculty/security presence in safe/unsafe `hot spots' mapped by students'' (Taylor, Stein, Mumford, \& Woods, 2013), and a module of peer-based consent training on a college campus (Crane, 2017). When you average these two things, what does the resulting quantity signify? Ascribing a causal meaning to that average \textemdash saying that both interventions are tests of the same core ideas, just in different settings \textemdash requires some strong additional assumptions.

Likewise with dependent variables. In that paper, we coded the Sexual Experiences Survey Koss \& Gidycz (1985) for behavioral outcomes and the Illinois Rape Myth Acceptance Scale (Thelan \& Meadows, 2022) for ideas-based outcomes when they were available, but the nearest substitutes for each, in our assessment, when they weren't. What additional assumptions do we need to argue that these multitudinous outcomes are coherently integrable?

We take a different tack: we drop the assumption that a pooled meta-analytic effect should be considered causal. Instead, when we combine studies from heterogeneous designs, treatments, and outcomes, the pooled estimate is a \emph{descriptive statistic} representing the average size of changes measured by a group of researchers, according to them, with outcomes they chose. If all the meta-analytic estimates in our papers were placed on a spectrum of credulousness, this estimate would be on the most credulous end: it tells us how a field is doing at its main task according to itself.

We were not as clear about the distinction between descriptive and causal analyses as we should have been in our three previous meta-analyses. Our thinking on this matter has evolved over time and we aim to be clearer in future papers.

\section{Selective vs.~comprehensive meta-analayses}\label{selective-vs.-comprehensive-meta-analayses}

The simplest, conceptually clearest way to address the critiques of Slough and Tyson (2023) and Simonsohn et al. (2022) is to set strict inclusion criteria relating to study design, intervention strategy, and measurement We think such a paper can make a valuable contribution, in no small part by revealing a paucity of rigorous evidence, as Paluck et al. (2019) did for the intergroup contact literature. We'd also note that a selective meta-analysis is a substantially less time-consuming undertaking: Porat et al. (2024) took six years, five authors, and a team of RAs, whereas Paluck et al. (2019) was basically written in a summer by three people (although identifying the universe of relevant studies was partly accomplished during other projects).

However, if resources allow, we favor comprehensive meta-analyses for several reasons.

\subsubsection{Quantitatively assessing publication bias, confounding bias, and measurement harmonization}\label{quantitatively-assessing-publication-bias-confounding-bias-and-measurement-harmonization}

First, coding more literature means more accurate tests of publication bias. Classical approaches to assessing publication bias in meta-analysis are ``based on the fact that precision in estimating the underlying treatment effect will increase as the sample size of component studies increases'' (Egger, Smith, Schneider, \& Minder, 1997). The funnel plot detects publication bias under the assumption that smaller, more imprecisely estimated studies that produce null or backlash results are more likely to be shelved than large, precisely estimated studies that produce the same.\footnote{The underlying theory of researcher behavior seems to be a kind of widespread acceptance of the sunk cost fallacy: once you've invested the kind of time and energy that a large, rigorous study requires, you'll push through to publication even if you'd put the results back in the `file drawer' (Rosenthal, 1979) if you'd put less work in.} The more studies included, the more precise and accurate the estimate. (Having said that, estimates of publication bias can still be interesting in more selective subsets, e.g. Paluck et al. (2019)).

Second, coding studies with heterogeneous designs allows for comparisons between them, which are often illuminating. Porat et al. (2024), for example, finds a pooled effect size of 0.503 within observational designs, 0.287 for quasi-experiments, and 0.307 for randomized controlled trials. This is not a well-identified estimate of the `effect' of rigor \textemdash randomization was not itself randomly assigned, and many other things varied between these studies \textemdash but it think it's good start for for establishing a quantitative prior on the magnitude of confounding bias in a non-randomized portion of a literature.

Third, comprehensive quantitative estimates allow for assessing differences in effect size between outcomes and how they're measured. This can be interesting both within literatures and across them. The three meta-analyses discussed in this paper mostly measure attitudes, and their overall effect sizes all fall between 0.28 and 0.357: a relatively small range. However, for Porat et al. (2024), the key quantity of interest \textemdash sexual violence \textemdash is fundamentally a behavioral problem. There, we found a sharp divergence between changes in ideas and changes in behaviors: 0.366 for ideas-based outcomes but just 0.033 for perpetration outcomes and 0.046 for victimization, neither of which is statistically significant. This divergence serves as a ``hoop test'' (Collier, 2011) that the ideas-based and behavioral outcomes are not measurement harmonized (Slough \& Tyson, 2023), and so cannot be combined into a causal analysis.

\subsubsection{Assessing comparative efficacy of different intervention strategies}\label{assessing-comparative-efficacy-of-different-intervention-strategies}

Fourth, comprehensive meta-analyses allow for comparisons between classes of interventions. Paluck et al. (2021) looked at nine separate categories of intervention strategy to assess their efficacy, rigor, and robustness. For example, the ``20 experiments testing antibias, multicultural, and moral education programs'' have an average effect size of \emph{d} = 0.30, compared to \emph{d} = 0.43 for 12 entertainment interventions. This does not necessarily mean that entertainment interventions reduce prejudice more effectively than antibias trainings do \textemdash there are many confounding differences between these two categories of study \textemdash but the differences do tell the reader something about how effectively people in different disciplines are manipulating the key quantity of interest by their own lights.

\subsubsection{Benchmarking focused analyses with comprehensive estimates}\label{benchmarking-focused-analyses-with-comprehensive-estimates}

Fifth, an initial pooled estimate provides a benchmark for all subsequent analyses, including those with sufficient internal validity and harmonization of treatments and outcomes to be plausibly causal. For example, the pooled effect observed in Porat et al. (2024) is 0.28, but interventions with a bystander component (Banyard, Moynihan, \& Plante, 2007; Banyard, Plante, \& Moynihan, 2004) furnish a sharp null effect o perpetration (0.019) and victimization (-0.009) outcomes. This gap between, on the one hand, the superficially encouraging results of the literature as a whole and, on the other, the bleaker effects of a leading theoretical approach on the most crucial outcomes, is a call to action to researchers that a change of direction is needed.

Finally, we'd note that a more focused analysis of the very best studies can be encompassed within a comprehensive meta-analysis: the dataset assembled in Paluck et al. (2021) could (hypothetically) be filtered to reproduce the dataset of Paluck et al. (2019) precisely, but the reverse is not true.

In sum, we favor comprehensive meta-analyses if possible. However, researchers might split the difference between these approaches by conducting a theoretically comprehensive literature review, coding data from every study, but only meta-analyzing research that meets strict inclusion criteria. We recommend tailoring your approach to your key questions and outcomes of interest.

We now turn to the pragmatic section of our paper, where we detail the typical structure of our meta-analytic papers, our analytic defaults, and an R package that implements those defaults.

\section{Default structure and analytic choices for Paluck lab meta-analytic papers}\label{default-structure-and-analytic-choices-for-paluck-lab-meta-analytic-papers}

We do not expect anything in this section to be groundbreaking or novel. However, we think there is pedagogical value in explicitly sharing a lab's approach to a class of paper. (Caveat that each paper was tailored to its venue and there is some variation between them about where the following sections occur.)

\subsection{How our meta-analyses typically unfold}\label{how-our-meta-analyses-typically-unfold}

Our meta-analyses' introductions describe the context and stakes of the paper. Paluck et al. (2019) and Porat et al. (2024) cover the major theoretical developments in their respective literatures, whereas Paluck et al. (2021) describes the need for an update on the major developments since a previous review (Paluck \& Green, 2009). Porat et al. (2024) focuses particularly on `zeitgeist' studies and ideas, which we think works nicely as a framing device for subsequent quantitative analyses, especially if some of those zeitgeist notions are revealed to have a surprising lack of evidence for their efficacy.

We then typically provide details about our database of studies and how we assembled it. These sections are pretty standard for meta-analyses, e.g.~including our PRISMA diagrams (Moher, Liberati, Tetzlaff, Altman, \& Group*, 2009), search terms, where and when our studies took place, and criteria for study collection.

Next, we detail our meta-analytic methods. These sections are not intended as exhaustive treatments of meta-analysis, but rather to detail a few important choices, e.g.~how we selected dependent variables (Paluck et al., 2019), our contrasts between overall results and those from ``studies with reasonably good statistical precision'' (Paluck et al., 2021, p. 14.7), and how readers can access our original datasets and reproduce our analyses (all three papers).

Next, we come to the main course: our meta-analyses. These sections are tailored to their literatures, and so follow distinct paths. However, we can describe a few general guiding principles.

First, we typically start big and then go small, meaning we start by meta-analyzing everything and then zoom in on different subsets of the literature. As we argued above, we think that starting big is an effective framing device for our more focused analyses. We typically follow this with various tests for publication bias, e.g.~a plot of effect sizes by standard errors, a statistical summary of the relationship between the two, and tests for systematic differences in effect sizes found in published or unpublished studies.

Our more focused analyses typically apply the ``split-apply-combine'' framework for data analysis (Wickham, 2011). For instance, in a literature with randomized controlled trials, quasi-experiments, and observational designs, as well as a mix of attitudinal and behavioral outcomes, we would present a 3 by 2 table where each subset of study design + outcome strategy gets its own meta-analytic estimate (table 2 in Porat et al., 2024). Alternatively, in a literature with many similar but distinct quantities of interest, e.g.~racial/ethnic prejudice vs.~prejudice against LGBTQ+ people vs.~prejudice against people with physical and mental disabilities, we would present the meta-analytic effects within each subset of data. These sub-analyses aim to identify two meta-scientific desiderata. The first is places where basic research has not yet been done, e.g.~the finding in Paluck et al. (2019) of zero evaluations of interracial contact in adults with both random assignment and delayed measurement. The second is the subsets of data that have sufficient contrast and measurement harmonization to have plausible target equivalence.

Finally, we aim to include tests of the relationship between study quality and effect size. Paluck et al. (2021) found a highly significant relationship between sample size and effect size, while Paluck et al. (2019) found that studies with pre-analysis plans had an average effect size of 0.016, vs.~0.451 for everything else.

Alternatively, we might conclude by testing the major theoretical assumptions of a literature. For example, the literature reviewed in Porat et al. (2024) almost universally assumes that changes in ideas will lead to changes in behaviors, but in the subset of studies that measured both, we found a disappointing lack of correlation between the two.

This typically concludes our meta-analytic sections, at which point we move to a discussion of open questions and next steps. Here we write specifically for researchers. In Porat et al. (2024), we highlight positive developments in measurement strategies as well as some viable theories of behavioral change that have yet to be tested. In Paluck et al. (2019), we encourage researchers to test which of the moderating conditions proposed in Allport (1954) are most essential for anti-prejudicial effects, and call on researchers to fill in important gaps in basic research gaps. In Paluck et al. (2021), we discuss some concerning trends in measurement and design, and then celebrate seven `landmark' studies. These papers ``are exceptionally well-designed and executed'' and ``provide a glimpse of what a meta-analysis would reveal if we could weight studies by quality as well as quantity'' (p.~14.19).

\subsection{Default meta-analytic procedures in Paluck lab}\label{default-meta-analytic-procedures-in-paluck-lab}

Our meta-analytic procedures vary somewhat between papers. However, a few guiding principles have held constant, and we've also developed some techniques that we think are helpful for solving methodological issues.

First, at the level of condensing studies to estimates of effect size and variance, we use equations found in Cooper et al. (2019) to calculate standardized mean differences (SMD) \textemdash an average treatment effect divided by some measure of standard deviation of the dependent variable. While some literatures might all look at the same dependent variable, or all present the same statistical outcome (e.g.~an odds ratio), we've never meta-analyzed a literature that does so.

We prefer Glass's \(\Delta\) to Cohen's \emph{d}. The difference is that \(\Delta\) standardizes by the standard deviation of the dependent variable for the control group rather than the entire population. We prefer \(\Delta\) because the experimental treatment might change some aspect of the distribution of outcomes for the treated subjects, and what we seek to know is the effect of treatment in terms of the underlying distribution of the untreated population. When the standard deviation of the control group's outcomes is not presented, we will standardize by whatever is available. (In practice, standard deviations between treatment and control groups tend to be very similar in papers we've analyzed).

When converting event outcomes, e.g.~how often a sample of people reports perpetrating some behavior, to Glass's \(\Delta\), we treat each event as draws from a Bernoulli distribution whose variance is \(p (1-p)\) and whose standard deviation is \(\sqrt{p(1-p)}\). If some event occurred in 15 out of 100 people in the control group and 10 out of 100 people in the treatment group, our estimate of Glass's \(\Delta\) = \(\frac{.1 - .15}{\sqrt{.15 * .85}} = -0.14\). We think this estimator has some desirable theoretical properties relative to conventional methods of converting odds ratios or log odds ratios to SMDs (Gomila, 2021); we elaborate more on this in an appendix to Porat et al. (2024).

In terms of aggregating studies for meta-analysis, we default to random effects estimates rather than fixed effects (sometimes, and probably more accurately, called ``equal effects'' (Viechtbauer, 2010)); in the heterogeneous literatures we analyze, there is likely no singular ``true'' effect size for which we are searching, but rather a variety of true effect sizes moderated by populations, conditions, and intervention strategies. We cluster standard errors at the level of study, rather than paper or team of authors or lab group, because we assume that the bulk of correlation between outcomes takes place within studies.

\subsubsection{Implementation with an R package}\label{implementation-with-an-r-package}

We have written a collection of functions to help implement these defaults, and write papers that use them, called \texttt{PaluckMetaSOP.} It is available at \url{http://www.github.com/setgree/PaluckMetaSOP}. Its functions fall into three categories.

First, we wrote functions to convert unstandardized effect sizes into standardized mean differences. \texttt{d\_calc.R} takes a statistical result, sample sizes for treatment and control, a measure of standard deviation, and a class of statistical test and returns an SMD. \texttt{var\_d\_calc.R} does the same thing for estimates of variance and standard error. \texttt{dip\_calc} is a helper function that does so for difference in proportions estimates.

Next we have functions that help us report meta-analytic results. \texttt{map\_robust.R} is a wrapper around the core meta-analytic functions in the \texttt{metafor} package (Viechtbauer, 2010) that integrates well into the \texttt{purrr} package from \texttt{tidyverse} (Wickham et al., 2019), and therefore helps us implement the split-apply-combine framework (Wickham, 2011). \texttt{study\_count.R}, \texttt{sum\_lm.R} and \texttt{sum\_tab.R} do the same for existing functions that count the studies in subsets, compute and report linear regression results, and report frequency tables.

Finally, we have functions that make writing meta-analysis papers easier. This paper was drafted as an Rmarkdown file using the \texttt{papaja} package (Aust \& Barth, 2023), and we've written some additional functions that, for instance, combine Rmarkdown rendering, \texttt{git\ add} and \texttt{git\ push} into a single command; convert DOIs to a bibliography; and record a computational environment as a Dockerfile (Moreau, Wiebels, \& Boettiger, 2023).

Please see the package's vignette on \url{http://www.github.com/setgree/PaluckMetaSOP} for detailed usage instructions with reference to a demonstrative subset of the data from Paluck et al. (2021).

\section{Conclusions: hard cases}\label{conclusions-hard-cases}

A classic paper on meta-analysis defines the procedure as ``a statistical analysis which combines or integrates the results of several independent clinical trials considered by the analyst to be `combinable'\,'' (p.~XXX). Our (non-systematic) review of prior meta-analyses reveals a surprising dearth of attention to how much theoretical work is being done by the phrase ``considered by the analyst to be `combinable.'\,'' Simonsohn et al. (2022) argue that researchers who ``uncritically'' aggregate results from all available studies, as part of an ``active effort to eliminate publication bias\ldots might instead amplify the bias from poor research design and execution'' (p.~XXX). We agree with this assessment, but would instead frame it as one of three crucial bias-variance trade-offs facing meta-analysts. By combining studies with non-comparable designs, interventions, and outcomes, researchers give readers the impression of remarkable precision (re: reduced variance) but at the expense of introducing non-statistical sources of uncertainty (re: bias) on all three fronts. Only by acknowledging these trade-offs head-on, and devising techniques for dealing with them, can we write meta-analyses that are fundamentally incredulous, evaluative, and careful about causal inference: in a word, meta-scientific.

More broadly, this paper seeks to answer two questions: what are we aiming to do when we meta-analyze, and how do we do it? We hope that we've answered these questions to some extent, but many open questions remain. To us, the most pressing issues arise from a lack of clarity about whether meta-analyses are, or should be, hypothesis-driven. Typical checks on researcher degrees of freedom (Gelman \& Loken, 2013; Simonsohn, Nelson, \& Simmons, 2014) assume a hypothesis-driven framework, and indeed, meta-analyses in the social sciences are usually aimed at validating pre-existing hypotheses. Ours, however, are more exploratory: we aim for evaluation rather than validation. While we might start with some general hypotheses, these might be about a discipline rather than a literature. For example, our prior is that a social psychology literature from the 2000s is likely to show evidence of selection pressures for statistical significance. However, we also develop many of our most important questions after reading papers, which, in the context of writing a meta-analysis, means after data collection has begun. In other words, our meta-analyses are both inductive and deductive inquiries. We are left with a conundrum: we wish to ``tie our hands to the mast'' (Elster, 1977), e.g.~by writing detailed pre-analysis plans and sticking to them, while also acknowledging that the most incisive questions often emerge after reading and coding studies, i.e.~after data collection has begun.

Further, meta-analyses that are written before data collection are at risk of going off track by proceeding from false premises. Consider a mistake we made while composing the pre-analysis plan for Porat et al. (2024). Because we are accustomed to social psychology literatures, we registered the analysis that we would evaluate ``the overall effect size of behavioral outcomes for all of the studies in the dataset, both in the short- and in the long-term.'' The idea was to measure if effects of interventions attenuate over time, which is a serious concern in psychology. However, while some behavioral outcomes in the sexual violence literature are measured immediately (e.g.~whether someone volunteers for a campus rape education organization (Gillies, 1997)), most behavioral outcomes need time to accumulate. The Sexual Experiences Survey (SES) measures incidents of sexual violence in a given time period, and an SES administered at the end of a semester is essentially guaranteed to have more incidents than one given immediately after an intervention concludes. This only became clear to us after we began reading papers (though someone with better foresight could have anticipated the issue).

The way we resolved this was to make sure that we included, either in the paper or in an appendix, every single pre-registered analysis, even when they were not especially informative, and to explain the above issue in our appendix. Moving forward, we aim to have pre-specified analysis code run on simulated data (Blair, Cooper, Coppock, \& Humphreys, 2019; Broockman \& Kalla, 2016) and to devote more time to anticipating such issues. However, this is still a work in progress at our lab.

Second, we sometimes encounter indirect signals of study quality in the literature that are difficult to integrate into formal inclusion and exclusion criteria. Should we exclude a study because it has an implausibly \emph{large} effect size, e.g.~a t-test value of 36? (One study in the prejudice reduction literature did.) What about errors in the text, or obvious discrepancies between results reported in the text and those in a table? What about a lack of transparent reporting that leads to our spending substantial time guessing a study's true effects? Each of these is information about a study's true quality; as Simonsohn et al. (2022) argue, excluding these studies would be the same ``kind of meritocratic screening of research'' that we ``engage in when performing virtually every other task in {[}our{]} professional research lives.'' Conversely, we might notice that a paper pays special attention to computational reproducibility, or was published in a journal where reproducibility checks are part of the publication process (Lowe, 2021); this is also a signal of study quality in the opposite direction. Some of these data points are easy to code, e.g.~data availability, and can be included in our meta-analyses; but some are not, e.g.~a the sensibility of a paper's statistical approaches.

There are clearly no easy answers to this problem, but we've developed general guidelines as we've progressed. One is that cluster RCTs with fewer than 10 clusters in total are so underpowered that they are effectively quasi-experiments, and we coded them as such in Porat et al. (2024). Another is that we do not code studies where we need to eyeball a figure to assess effect sizes: researchers should not make us guess. Likewise, we do not convert ``the results were significant'' or simply ``p \textless{} .05'' into quantitative estimates. However, if authors tell us that a result was ``null'' or ``not significant'' but say anything more, we set the result to be 0.01. These rules are not perfect, but ultimately, we need policies that balance type I errors (including a study we should exclude) and type II errors (excluding studies we should include). Like research itself, we suspect that this task is not perfectible.

Last, there is a great deal of subjectivity in figuring out what outcomes to code. For Porat et al. (2024), we found relative homogeneity of dependent variables, but not so for Paluck et al. (2019) or Paluck et al. (2021). In the end,this has always been a judgment call, but as we continue writing meta-analyses, we hope to develop principled, general guidelines for this kind of analytic choice.

These open questions notwithstanding, we think meta-analyses can be useful, informative, and a pleasure to write and read. We hope to see them become a standard part of the meta-scientist's toolkit, and likewise for all meta-analyses to become more meta-scientific.

\section*{References}\label{references}
\addcontentsline{toc}{section}{References}

\phantomsection\label{refs}
\begin{CSLReferences}{1}{0}
\bibitem[\citeproctext]{ref-allport1954}
Allport, G. W. (1954). \emph{The nature of prejudice}.

\bibitem[\citeproctext]{ref-angrist2010}
Angrist, J. D., \& Pischke, J.-S. (2010). The credibility revolution in empirical economics: How better research design is taking the con out of econometrics. \emph{Journal of Economic Perspectives}, \emph{24}(2), 3--30.

\bibitem[\citeproctext]{ref-R-papaja}
Aust, F., \& Barth, M. (2023). \emph{{papaja}: {Prepare} reproducible {APA} journal articles with {R Markdown}}. Retrieved from \url{https://github.com/crsh/papaja}

\bibitem[\citeproctext]{ref-banyard2007}
Banyard, V. L., Moynihan, M. M., \& Plante, E. G. (2007). Sexual violence prevention through bystander education: An experimental evaluation. \emph{Journal of Community Psychology}, \emph{35}(4), 463--481.

\bibitem[\citeproctext]{ref-banyard2004}
Banyard, V. L., Plante, E. G., \& Moynihan, M. M. (2004). Bystander education: Bringing a broader community perspective to sexual violence prevention. \emph{Journal of Community Psychology}, \emph{32}(1), 61--79.

\bibitem[\citeproctext]{ref-bem2015}
Bem, D., Tressoldi, P., Rabeyron, T., \& Duggan, M. (2015). Feeling the future: A meta-analysis of 90 experiments on the anomalous anticipation of random future events. \emph{F1000Research}, \emph{4}.

\bibitem[\citeproctext]{ref-bezrukova2016}
Bezrukova, K., Spell, C. S., Perry, J. L., \& Jehn, K. A. (2016). A meta-analytical integration of over 40 years of research on diversity training evaluation. \emph{Psychological Bulletin}, \emph{142}(11), 1227.

\bibitem[\citeproctext]{ref-blair2019}
Blair, G., Cooper, J., Coppock, A., \& Humphreys, M. (2019). Declaring and diagnosing research designs. \emph{American Political Science Review}, \emph{113}(3), 838--859.

\bibitem[\citeproctext]{ref-boisjoly2006}
Boisjoly, J., Duncan, G. J., Kremer, M., Levy, D. M., \& Eccles, J. (2006). Empathy or antipathy? The impact of diversity. \emph{American Economic Review}, \emph{96}(5), 1890--1905.

\bibitem[\citeproctext]{ref-broockman2016}
Broockman, D., \& Kalla, J. (2016). Durably reducing transphobia: A field experiment on door-to-door canvassing. \emph{Science}, \emph{352}(6282), 220--224.

\bibitem[\citeproctext]{ref-chang2019}
Chang, E. H., Milkman, K. L., Gromet, D. M., Rebele, R. W., Massey, C., Duckworth, A. L., \& Grant, A. M. (2019). The mixed effects of online diversity training. \emph{Proceedings of the National Academy of Sciences}, \emph{116}(16), 7778--7783.

\bibitem[\citeproctext]{ref-collier2011}
Collier, D. (2011). Understanding process tracing. \emph{PS: Political Science \& Politics}, \emph{44}(4), 823--830.

\bibitem[\citeproctext]{ref-cooper2019}
Cooper, H., Hedges, L. V., \& Valentine, J. C. (2019). \emph{The handbook of research synthesis and meta-analysis}. Russell Sage Foundation.

\bibitem[\citeproctext]{ref-crane2017}
Crane, P. R. (2017). \emph{The POWER of consent: An evaluation of peer-based consent programming in sexual assault prevention} (PhD thesis). Ohio University.

\bibitem[\citeproctext]{ref-cuddy2018}
Cuddy, A. J., Schultz, S. J., \& Fosse, N. E. (2018). P-curving a more comprehensive body of research on postural feedback reveals clear evidential value for power-posing effects: Reply to simmons and simonsohn (2017). \emph{Psychological Science}, \emph{29}(4), 656--666.

\bibitem[\citeproctext]{ref-cumpston2019}
Cumpston, M., Li, T., Page, M. J., Chandler, J., Welch, V. A., Higgins, J. P., \& Thomas, J. (2019). Updated guidance for trusted systematic reviews: A new edition of the cochrane handbook for systematic reviews of interventions. \emph{The Cochrane Database of Systematic Reviews}, \emph{2019}(10).

\bibitem[\citeproctext]{ref-egger1997}
Egger, M., Smith, G. D., Schneider, M., \& Minder, C. (1997). Bias in meta-analysis detected by a simple, graphical test. \emph{Bmj}, \emph{315}(7109), 629--634.

\bibitem[\citeproctext]{ref-elster1977}
Elster, J. (1977). Ulysses and the sirens: A theory of imperfect rationality. \emph{Social Science Information}, \emph{16}(5), 469--526.

\bibitem[\citeproctext]{ref-ferguson2023}
Ferguson, J., Littman, R., Christensen, G., Paluck, E. L., Swanson, N., Wang, Z., \ldots{} Pezzuto, J.-H. (2023). Survey of open science practices and attitudes in the social sciences. \emph{Nature Communications}, \emph{14}(1), 5401.

\bibitem[\citeproctext]{ref-frank2024}
Frank, M. C., Braginsky, M., Cachia, J., Coles, N., Hardwicke, T. E., Hawkins, R. D., \ldots{} Williams, R. (2024). \emph{Experimentology: An open science approach to experimental psychology methods}. MIT Press. \url{https://doi.org/10.7551/mitpress/14810.001.0001}

\bibitem[\citeproctext]{ref-gawronski2019}
Gawronski, B. (2019). Six lessons for a cogent science of implicit bias and its criticism. \emph{Perspectives on Psychological Science}, \emph{14}(4), 574--595.

\bibitem[\citeproctext]{ref-gelman2013}
Gelman, A., \& Loken, E. (2013). The garden of forking paths: Why multiple comparisons can be a problem, even when there is no {``fishing expedition''} or {``p-hacking''} and the research hypothesis was posited ahead of time. \emph{Department of Statistics, Columbia University}, \emph{348}, 1--17.

\bibitem[\citeproctext]{ref-gillies1997}
Gillies, R. A. (1997). \emph{Providing direct counter-arguments to challenge male audiences' attitudes toward rape}. University of Missouri-Columbia.

\bibitem[\citeproctext]{ref-glass1977}
Glass, G. V. (1977). 9: Integrating findings: The meta-analysis of research. \emph{Review of Research in Education}, \emph{5}(1), 351--379.

\bibitem[\citeproctext]{ref-gomila2021}
Gomila, R. (2021). Logistic or linear? Estimating causal effects of experimental treatments on binary outcomes using regression analysis. \emph{Journal of Experimental Psychology: General}, \emph{150}(4), 700.

\bibitem[\citeproctext]{ref-greco2013}
Greco, T., Zangrillo, A., Biondi-Zoccai, G., \& Landoni, G. (2013). Meta-analysis: Pitfalls and hints. \emph{Heart, Lung and Vessels}, \emph{5}(4), 219.

\bibitem[\citeproctext]{ref-hardwicke2021}
Hardwicke, T. E., Bohn, M., MacDonald, K., Hembacher, E., Nuijten, M. B., Peloquin, B. N., \ldots{} Frank, M. C. (2021). Analytic reproducibility in articles receiving open data badges at the journal psychological science: An observational study. \emph{Royal Society Open Science}, \emph{8}(1), 201494.

\bibitem[\citeproctext]{ref-hardwicke2018}
Hardwicke, T. E., Mathur, M. B., MacDonald, K., Nilsonne, G., Banks, G. C., Kidwell, M. C., et al.others. (2018). Data availability, reusability, and analytic reproducibility: Evaluating the impact of a mandatory open data policy at the journal cognition. \emph{Royal Society Open Science}, \emph{5}(8), 180448.

\bibitem[\citeproctext]{ref-hardwicke2023}
Hardwicke, T. E., \& Vazire, S. (2023). Transparency is now the default at psychological science. SAGE Publications Sage CA: Los Angeles, CA.

\bibitem[\citeproctext]{ref-harrer2021}
Harrer, M., Cuijpers, P., Furukawa, T., \& Ebert, D. (2021). \emph{Doing meta-analysis with r: A hands-on guide}. Chapman; Hall/CRC.

\bibitem[\citeproctext]{ref-haushofer2019}
Haushofer, J., Ringdal, C., Shapiro, J. P., \& Wang, X. Y. (2019). \emph{Income changes and intimate partner violence: Evidence from unconditional cash transfers in kenya}. National Bureau of Economic Research.

\bibitem[\citeproctext]{ref-ioannidis2015}
Ioannidis, J. P., Fanelli, D., Dunne, D. D., \& Goodman, S. N. (2015). Meta-research: Evaluation and improvement of research methods and practices. \emph{PLoS Biology}, \emph{13}(10), e1002264.

\bibitem[\citeproctext]{ref-kadlec2023}
Kadlec, D., Sainani, K. L., \& Nimphius, S. (2023). With great power comes great responsibility: Common errors in meta-analyses and meta-regressions in strength \& conditioning research. \emph{Sports Medicine}, \emph{53}(2), 313--325.

\bibitem[\citeproctext]{ref-kleinstauber1996}
Kleinstäuber, M., Witthöft, M., Steffanowski, A., Van Marwijk, H., Hiller, W., \& Lambert, M. J. (1996). \emph{Cochrane database of systematic reviews}.

\bibitem[\citeproctext]{ref-koss1985}
Koss, M. P., \& Gidycz, C. A. (1985). Sexual experiences survey: Reliability and validity. \emph{Journal of Consulting and Clinical Psychology}, \emph{53}(3), 422.

\bibitem[\citeproctext]{ref-koss1982}
Koss, M. P., \& Oros, C. J. (1982). Sexual experiences survey: A research instrument investigating sexual aggression and victimization. \emph{Journal of Consulting and Clinical Psychology}, \emph{50}(3), 455.

\bibitem[\citeproctext]{ref-lakens2022}
Lakens, Daniel. (2022). \emph{Improving your statistical inferences}. \url{https://doi.org/10.5281/zenodo.6409077}

\bibitem[\citeproctext]{ref-lakens2023}
Lakens, D. (2023). \emph{When and how to deviate from a preregistration}. \url{https://doi.org/10.31234/osf.io/ha29k}.

\bibitem[\citeproctext]{ref-lakens2017}
Lakens, Daniel, LeBel, E., Page-Gould, E., Assen, M. van, Spellman, B., Schönbrodt, F., \& Hertogs, R. (2017). Examining the reproducibility of meta-analyses in psychology. \emph{Retrieved from Osf. Io/Q23ye}.

\bibitem[\citeproctext]{ref-lin2016}
Lin, W., \& Green, D. P. (2016). Standard operating procedures: A safety net for pre-analysis plans. \emph{PS: Political Science \& Politics}, \emph{49}(3), 495--500.

\bibitem[\citeproctext]{ref-lopez2024}
López-Nicolás, R., Lakens, D., López-López, J. A., Rubio-Aparicio, M., Sandoval-Lentisco, A., López-Ibáñez, C., \ldots{} Sánchez-Meca, J. (2024). Reproducibility of published meta-analyses on clinical-psychological interventions. \emph{Advances in Methods and Practices in Psychological Science}, \emph{7}(1), 25152459231202929.

\bibitem[\citeproctext]{ref-lowe2021}
Lowe, M. (2021). Types of contact: A field experiment on collaborative and adversarial caste integration. \emph{American Economic Review}, \emph{111}(6), 1807--1844.

\bibitem[\citeproctext]{ref-mcconahay1976}
McConahay, J. B., \& Hough Jr, J. C. (1976). Symbolic racism. \emph{Journal of Social Issues}, \emph{32}(2), 23--45.

\bibitem[\citeproctext]{ref-moher2009}
Moher, D., Liberati, A., Tetzlaff, J., Altman, D. G., \& Group*, P. (2009). Preferred reporting items for systematic reviews and meta-analyses: The PRISMA statement. \emph{Annals of Internal Medicine}, \emph{151}(4), 264--269.

\bibitem[\citeproctext]{ref-moreau2023}
Moreau, D., Wiebels, K., \& Boettiger, C. (2023). Containers for computational reproducibility. \emph{Nature Reviews Methods Primers}, \emph{3}(1), 50.

\bibitem[\citeproctext]{ref-mousa2020}
Mousa, S. (2020). Building social cohesion between christians and muslims through soccer in post-ISIS iraq. \emph{Science}, \emph{369}(6505), 866--870.

\bibitem[\citeproctext]{ref-munger2017}
Munger, K. (2017). Tweetment effects on the tweeted: Experimentally reducing racist harassment. \emph{Political Behavior}, \emph{39}, 629--649.

\bibitem[\citeproctext]{ref-munger2023}
Munger, K. (2023). Temporal validity as meta-science. \emph{Research \& Politics}, \emph{10}(3), 20531680231187271.

\bibitem[\citeproctext]{ref-nosek2015}
Nosek, A., B. A. (2015). Promoting an open research culture. \emph{Science}, \emph{348}(6242), 1422--1425.

\bibitem[\citeproctext]{ref-obels2020}
Obels, P., Lakens, D., Coles, N. A., Gottfried, J., \& Green, S. A. (2020). Analysis of open data and computational reproducibility in registered reports in psychology. \emph{Advances in Methods and Practices in Psychological Science}, \emph{3}(2), 229--237.

\bibitem[\citeproctext]{ref-paluck2009}
Paluck, E. L., \& Green, D. P. (2009). Prejudice reduction: What works? A review and assessment of research and practice. \emph{Annual Review of Psychology}, \emph{60}, 339--367.

\bibitem[\citeproctext]{ref-paluck2019}
Paluck, E. L., Green, S. A., \& Green, D. P. (2019). The contact hypothesis re-evaluated. \emph{Behavioural Public Policy}, \emph{3}(2), 129--158.

\bibitem[\citeproctext]{ref-paluck2021}
Paluck, E. L., Porat, R., Clark, C. S., \& Green, D. P. (2021). Prejudice reduction: Progress and challenges. \emph{Annual Review of Psychology}, \emph{72}, 533--560.

\bibitem[\citeproctext]{ref-paul2022meta}
Paul, J., \& Barari, M. (2022). Meta-analysis and traditional systematic literature reviews---what, why, when, where, and how? \emph{Psychology \& Marketing}, \emph{39}(6), 1099--1115.

\bibitem[\citeproctext]{ref-pettigrew2006}
Pettigrew, T. F., \& Tropp, L. R. (2006). A meta-analytic test of intergroup contact theory. \emph{Journal of Personality and Social Psychology}, \emph{90}(5), 751.

\bibitem[\citeproctext]{ref-porat2024}
Porat, R., Gantman, A., Paluck, E. L., Green, S. A., \& Pezzuto, J.-H. (2024). Preventing sexual violence -- a behavioral problem without a behaviorally-informed solution. \emph{Psychological Science in the Public Interest}, \emph{25}(1), 1--30.

\bibitem[\citeproctext]{ref-rapport2013}
Rapport, F., Storey, M., Porter, A., Snooks, H., Jones, K., Peconi, J., et al.others. (2013). Qualitative research within trials: Developing a standard operating procedure for a clinical trials unit. \emph{Trials}, \emph{14}(1), 1--8.

\bibitem[\citeproctext]{ref-rosenthal1979}
Rosenthal, R. (1979). The file drawer problem and tolerance for null results. \emph{Psychological Bulletin}, \emph{86}(3), 638.

\bibitem[\citeproctext]{ref-samii2016}
Samii, C. (2016). Causal empiricism in quantitative research. \emph{The Journal of Politics}, \emph{78}(3), 941--955.

\bibitem[\citeproctext]{ref-scacco2018}
Scacco, A., \& Warren, S. S. (2018). Can social contact reduce prejudice and discrimination? Evidence from a field experiment in nigeria. \emph{American Political Science Review}, \emph{112}(3), 654--677.

\bibitem[\citeproctext]{ref-schooler2014}
Schooler, J. W. (2014). Metascience could rescue the {``replication crisis.''} \emph{Nature}, \emph{515}(7525), 9--9.

\bibitem[\citeproctext]{ref-sears2003}
Sears, D. O., \& Henry, P. J. (2003). The origins of symbolic racism. \emph{Journal of Personality and Social Psychology}, \emph{85}(2), 259.

\bibitem[\citeproctext]{ref-shamoa2023}
Shamoa-Nir, L., \& Razpurker-Apfeld, I. (2023). Can you imagine this? Imagined contact as a strategy to promote positive intergroup relations. \emph{Frontiers in Psychology}, \emph{14}.

\bibitem[\citeproctext]{ref-simonsohn2014}
Simonsohn, U., Nelson, L. D., \& Simmons, J. P. (2014). P-curve: A key to the file-drawer. \emph{Journal of Experimental Psychology: General}, \emph{143}(2), 534.

\bibitem[\citeproctext]{ref-simonsohn2022}
Simonsohn, U., Simmons, J., \& Nelson, L. D. (2022). Above averaging in literature reviews. \emph{Nature Reviews Psychology}, \emph{1}(10), 551--552.

\bibitem[\citeproctext]{ref-slough2023}
Slough, T., \& Tyson, S. A. (2023). External validity and meta-analysis. \emph{American Journal of Political Science}, \emph{67}(2), 440--455.

\bibitem[\citeproctext]{ref-sutton2009}
Sutton, A. J. (2009). Publication bias. \emph{The Handbook of Research Synthesis and Meta-Analysis}, \emph{2}, 435--452.

\bibitem[\citeproctext]{ref-taylor2013}
Taylor, B. G., Stein, N. D., Mumford, E. A., \& Woods, D. (2013). Shifting boundaries: An experimental evaluation of a dating violence prevention program in middle schools. \emph{Prevention Science}, \emph{14}(1), 64--76.

\bibitem[\citeproctext]{ref-thelan2022}
Thelan, A. R., \& Meadows, E. A. (2022). The illinois rape myth acceptance scale---subtle version: Using an adapted measure to understand the declining rates of rape myth acceptance. \emph{Journal of Interpersonal Violence}, \emph{37}(19-20), NP17807--NP17833.

\bibitem[\citeproctext]{ref-thornton2000}
Thornton, A., \& Lee, P. (2000). Publication bias in meta-analysis: Its causes and consequences. \emph{Journal of Clinical Epidemiology}, \emph{53}(2), 207--216.

\bibitem[\citeproctext]{ref-vazire2018}
Vazire, S. (2018). Implications of the credibility revolution for productivity, creativity, and progress. \emph{Perspectives on Psychological Science}, \emph{13}(4), 411--417.

\bibitem[\citeproctext]{ref-viechtbauer2010}
Viechtbauer, W. (2010). Conducting meta-analyses in r with the metafor package. \emph{Journal of Statistical Software}, \emph{36}, 1--48.

\bibitem[\citeproctext]{ref-wallace2015}
Wallace, M., Hamesch, K., Lunova, M., Kim, Y., Weiskirchen, R., Strnad, P., \& Friedman, S. (2015). Standard operating procedures in experimental liver research: Thioacetamide model in mice and rats. \emph{Laboratory Animals}, \emph{49}(1\_suppl), 21--29.

\bibitem[\citeproctext]{ref-wickham2011}
Wickham, H. (2011). The split-apply-combine strategy for data analysis. \emph{Journal of Statistical Software}, \emph{40}, 1--29.

\bibitem[\citeproctext]{ref-wickham2019}
Wickham, H., Averick, M., Bryan, J., Chang, W., McGowan, L. D., François, R., et al.others. (2019). Welcome to the tidyverse. \emph{Journal of Open Source Software}, \emph{4}(43), 1686.

\end{CSLReferences}


\end{document}
