% Options for packages loaded elsewhere
\PassOptionsToPackage{unicode}{hyperref}
\PassOptionsToPackage{hyphens}{url}
%
\documentclass[
  man]{apa6}
\usepackage{amsmath,amssymb}
\usepackage{iftex}
\ifPDFTeX
  \usepackage[T1]{fontenc}
  \usepackage[utf8]{inputenc}
  \usepackage{textcomp} % provide euro and other symbols
\else % if luatex or xetex
  \usepackage{unicode-math} % this also loads fontspec
  \defaultfontfeatures{Scale=MatchLowercase}
  \defaultfontfeatures[\rmfamily]{Ligatures=TeX,Scale=1}
\fi
\usepackage{lmodern}
\ifPDFTeX\else
  % xetex/luatex font selection
\fi
% Use upquote if available, for straight quotes in verbatim environments
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
\IfFileExists{microtype.sty}{% use microtype if available
  \usepackage[]{microtype}
  \UseMicrotypeSet[protrusion]{basicmath} % disable protrusion for tt fonts
}{}
\makeatletter
\@ifundefined{KOMAClassName}{% if non-KOMA class
  \IfFileExists{parskip.sty}{%
    \usepackage{parskip}
  }{% else
    \setlength{\parindent}{0pt}
    \setlength{\parskip}{6pt plus 2pt minus 1pt}}
}{% if KOMA class
  \KOMAoptions{parskip=half}}
\makeatother
\usepackage{xcolor}
\usepackage{graphicx}
\makeatletter
\def\maxwidth{\ifdim\Gin@nat@width>\linewidth\linewidth\else\Gin@nat@width\fi}
\def\maxheight{\ifdim\Gin@nat@height>\textheight\textheight\else\Gin@nat@height\fi}
\makeatother
% Scale images if necessary, so that they will not overflow the page
% margins by default, and it is still possible to overwrite the defaults
% using explicit options in \includegraphics[width, height, ...]{}
\setkeys{Gin}{width=\maxwidth,height=\maxheight,keepaspectratio}
% Set default figure placement to htbp
\makeatletter
\def\fps@figure{htbp}
\makeatother
\setlength{\emergencystretch}{3em} % prevent overfull lines
\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}
\setcounter{secnumdepth}{-\maxdimen} % remove section numbering
% Make \paragraph and \subparagraph free-standing
\ifx\paragraph\undefined\else
  \let\oldparagraph\paragraph
  \renewcommand{\paragraph}[1]{\oldparagraph{#1}\mbox{}}
\fi
\ifx\subparagraph\undefined\else
  \let\oldsubparagraph\subparagraph
  \renewcommand{\subparagraph}[1]{\oldsubparagraph{#1}\mbox{}}
\fi
% definitions for citeproc citations
\NewDocumentCommand\citeproctext{}{}
\NewDocumentCommand\citeproc{mm}{%
  \begingroup\def\citeproctext{#2}\cite{#1}\endgroup}
\makeatletter
 % allow citations to break across lines
 \let\@cite@ofmt\@firstofone
 % avoid brackets around text for \cite:
 \def\@biblabel#1{}
 \def\@cite#1#2{{#1\if@tempswa , #2\fi}}
\makeatother
\newlength{\cslhangindent}
\setlength{\cslhangindent}{1.5em}
\newlength{\csllabelwidth}
\setlength{\csllabelwidth}{3em}
\newenvironment{CSLReferences}[2] % #1 hanging-indent, #2 entry-spacing
 {\begin{list}{}{%
  \setlength{\itemindent}{0pt}
  \setlength{\leftmargin}{0pt}
  \setlength{\parsep}{0pt}
  % turn on hanging indent if param 1 is 1
  \ifodd #1
   \setlength{\leftmargin}{\cslhangindent}
   \setlength{\itemindent}{-1\cslhangindent}
  \fi
  % set entry spacing
  \setlength{\itemsep}{#2\baselineskip}}}
 {\end{list}}
\usepackage{calc}
\newcommand{\CSLBlock}[1]{\hfill\break\parbox[t]{\linewidth}{\strut\ignorespaces#1\strut}}
\newcommand{\CSLLeftMargin}[1]{\parbox[t]{\csllabelwidth}{\strut#1\strut}}
\newcommand{\CSLRightInline}[1]{\parbox[t]{\linewidth - \csllabelwidth}{\strut#1\strut}}
\newcommand{\CSLIndent}[1]{\hspace{\cslhangindent}#1}
\ifLuaTeX
\usepackage[bidi=basic]{babel}
\else
\usepackage[bidi=default]{babel}
\fi
\babelprovide[main,import]{english}
% get rid of language-specific shorthands (see #6817):
\let\LanguageShortHands\languageshorthands
\def\languageshorthands#1{}
% Manuscript styling
\usepackage{upgreek}
\captionsetup{font=singlespacing,justification=justified}

% Table formatting
\usepackage{longtable}
\usepackage{lscape}
% \usepackage[counterclockwise]{rotating}   % Landscape page setup for large tables
\usepackage{multirow}		% Table styling
\usepackage{tabularx}		% Control Column width
\usepackage[flushleft]{threeparttable}	% Allows for three part tables with a specified notes section
\usepackage{threeparttablex}            % Lets threeparttable work with longtable

% Create new environments so endfloat can handle them
% \newenvironment{ltable}
%   {\begin{landscape}\centering\begin{threeparttable}}
%   {\end{threeparttable}\end{landscape}}
\newenvironment{lltable}{\begin{landscape}\centering\begin{ThreePartTable}}{\end{ThreePartTable}\end{landscape}}

% Enables adjusting longtable caption width to table width
% Solution found at http://golatex.de/longtable-mit-caption-so-breit-wie-die-tabelle-t15767.html
\makeatletter
\newcommand\LastLTentrywidth{1em}
\newlength\longtablewidth
\setlength{\longtablewidth}{1in}
\newcommand{\getlongtablewidth}{\begingroup \ifcsname LT@\roman{LT@tables}\endcsname \global\longtablewidth=0pt \renewcommand{\LT@entry}[2]{\global\advance\longtablewidth by ##2\relax\gdef\LastLTentrywidth{##2}}\@nameuse{LT@\roman{LT@tables}} \fi \endgroup}

% \setlength{\parindent}{0.5in}
% \setlength{\parskip}{0pt plus 0pt minus 0pt}

% Overwrite redefinition of paragraph and subparagraph by the default LaTeX template
% See https://github.com/crsh/papaja/issues/292
\makeatletter
\renewcommand{\paragraph}{\@startsection{paragraph}{4}{\parindent}%
  {0\baselineskip \@plus 0.2ex \@minus 0.2ex}%
  {-1em}%
  {\normalfont\normalsize\bfseries\itshape\typesectitle}}

\renewcommand{\subparagraph}[1]{\@startsection{subparagraph}{5}{1em}%
  {0\baselineskip \@plus 0.2ex \@minus 0.2ex}%
  {-\z@\relax}%
  {\normalfont\normalsize\itshape\hspace{\parindent}{#1}\textit{\addperi}}{\relax}}
\makeatother

\makeatletter
\usepackage{etoolbox}
\patchcmd{\maketitle}
  {\section{\normalfont\normalsize\abstractname}}
  {\section*{\normalfont\normalsize\abstractname}}
  {}{\typeout{Failed to patch abstract.}}
\patchcmd{\maketitle}
  {\section{\protect\normalfont{\@title}}}
  {\section*{\protect\normalfont{\@title}}}
  {}{\typeout{Failed to patch title.}}
\makeatother

\usepackage{xpatch}
\makeatletter
\xapptocmd\appendix
  {\xapptocmd\section
    {\addcontentsline{toc}{section}{\appendixname\ifoneappendix\else~\theappendix\fi\\: #1}}
    {}{\InnerPatchFailed}%
  }
{}{\PatchFailed}
\keywords{meta-analysis, standard-operating-procedures, meta-science\newline\indent Word count: 2637}
\DeclareDelayedFloatFlavor{ThreePartTable}{table}
\DeclareDelayedFloatFlavor{lltable}{table}
\DeclareDelayedFloatFlavor*{longtable}{table}
\makeatletter
\renewcommand{\efloat@iwrite}[1]{\immediate\expandafter\protected@write\csname efloat@post#1\endcsname{}}
\makeatother
\usepackage{lineno}

\linenumbers
\usepackage{csquotes}
\ifLuaTeX
  \usepackage{selnolig}  % disable illegal ligatures
\fi
\usepackage{bookmark}
\IfFileExists{xurl.sty}{\usepackage{xurl}}{} % add URL line breaks if available
\urlstyle{same}
\hypersetup{
  pdftitle={Towards meta-scientific meta-analyses: standard operating procedures for systematic reviews in the Paluck Lab},
  pdfauthor={Seth Green1, Elizabeth Levy Paluck1, \& Roni Porat2},
  pdflang={en-EN},
  pdfkeywords={meta-analysis, standard-operating-procedures, meta-science},
  hidelinks,
  pdfcreator={LaTeX via pandoc}}

\title{Towards meta-scientific meta-analyses: standard operating procedures for systematic reviews in the Paluck Lab}
\author{Seth Green\textsuperscript{1}, Elizabeth Levy Paluck\textsuperscript{1}, \& Roni Porat\textsuperscript{2}}
\date{}


\shorttitle{Paluck\_meta\_SOP}

\authornote{

This work was supported by {[}whoever supports it{]}

The authors made the following contributions. Seth Green: Conceptualization, Writing - Original Draft Preparation, Writing - Review \& Editing; Elizabeth Levy Paluck: Writing - Review \& Editing, Supervision; Roni Porat: Writing - Review \& Editing, Supervision.

Correspondence concerning this article should be addressed to Seth Green, Kahneman-Treisman Center, Princeton University. E-mail: \href{mailto:sag2212@columbia.edu}{\nolinkurl{sag2212@columbia.edu}}

}

\affiliation{\vspace{0.5cm}\textsuperscript{1} Princeton University\\\textsuperscript{2} Hebrew University, Jerusalem}

\abstract{%
This paper describes the motivations and procedures for meta-analyses by Betsy Levy Paluck and co-authors, with reference to and examples drawn from three such papers (Paluck, Green, and Green 2019, Paluck et al.~2020, and Porat et al.~2024). We first describe our conceptual aims when writing meta-analytic papers, which are to provide an `on-ramp' to interested readers, to evaluate a literature's strengths and weaknesses, and to illuminate empirical gaps, which we intend as signposts for future researchers. Second, we describe the typical `flow' of the papers as a whole. Third, we describe our meta-analytic procedures, detailing our default reporting structure and analytic choices and engaging with recent critiques from Slough and Tyson (2022) and Simonsohn, Simmons, and Nelson (2022). Fourth, we introduce a collection of functions we use to implement our meta-analytic choices, available as an R package. Fifth, we conclude with some open questions for meta-analysts.
}



\begin{document}
\maketitle

\subsection{Introduction: the divergent approaches of meta-analysis and meta-science}\label{introduction-the-divergent-approaches-of-meta-analysis-and-meta-science}

What purposes do meta-analyses serve above and beyond those accomplished by systematic reviews? Judging by recent papers in social psychology (Bem, Tressoldi, Rabeyron, \& Duggan, 2015; Bezrukova, Spell, Perry, \& Jehn, 2016; Cuddy, Schultz, \& Fosse, 2018; Pettigrew \& Tropp, 2006), the answer is to bolster and validate existing findings through aggregation. By pooling findings from many studies, researchers furnish meta-analytic estimates that are typically heartening to a literature's boosters because they are statistically precise and drawn from diverse settings, which gives readers the impression of durable, widely applicable findings. Such papers often have a valedictory quality to them: they provide leading practitioners a chance to survey the body of work they've helped inspire, highlight the strength of the evidence for their subfield's central tenets, and suggest future research directions that assume those tenets are no longer in need of testing.

Meanwhile, a parallel movement in psychology has begun to implement the recommendations of the open science (Nosek, 2015) and meta-science (Schooler, 2014) movements into study designs (Ferguson et al., 2023), journal policies (Hardwicke \& Vazire, 2023), and literature reviews (Hardwicke et al., 2018). Broadly speaking, meta-scientific papers aim to assess whether a literature's findings are \emph{credible} rather than to confirm them. While meta-analyses seek to integrate a set of results (taken more or less at face value), meta-science papers ask instead whether a literature's designs, analyses, and implementations license the inferences their authors draw.

Some meta-scientific reviews quantitatively assess collective outcomes, for instance, what percentage of a subset of papers are computationally reproducible (Obels, Lakens, Coles, Gottfried, \& Green, 2020) or consistent with open data standards (Hardwicke et al., 2021). In one sense, these estimates are `meta-analytic,' but they aren't meta-analyses in the conventional sense of pooling existing findings into weighted average effects (Cooper, Hedges, \& Valentine, 2019; Glass, 1977; Kleinstäuber et al., 1996). In tone, meta-science papers are often much more critical than meta-analyses are, and they are often written by outsiders or newcomers to an academic field who appear more motivated to interrogate than to validate. Thus, their similar names notwithstanding, meta-analytic and meta-scientific papers in social psychology typically have radically different perspectives, assumptions, and goals.

At the Paluck lab, we seek to synthesize these two approaches by writing meta-analyses that are fundamentally meta-scientific. Over the course of four systematic reviews, three with meta-analytic components (Paluck \& Green, 2009; Paluck, Green, \& Green, 2019; Paluck, Porat, Clark, \& Green, 2021; Porat, Gantman, Paluck, Green, \& Pezzuto, 2024), we've developed distinct beliefs about where and when a meta-analysis is useful, how to structure the meta-analytic sections of our systematic reviews, and which specifications are sensible defaults. This paper articulates those beliefs, and aims to illuminate how meta-analysis might become a standard component of the meta-scientific toolkit.

First, we describe our conceptual aims with meta-analysis, and where ameta-analysis adds something above and beyond a systematic review. We believe that meta-analyses are useful for evaluating the strengths and weaknesses of large, heterogeneous literatures, especially those that bring multiple theoretical perspectives to bear. We further believe that condensing each study down to a point estimate (or cluster of estimates) and associated variance(s) can furnish

Second, we detail our thinking about the tradeoff between comprehensive, potentially biased meta-analytic estimates and focused but also narrower quantitative reviews that set strict inclusion criteria around study design, interventions, and/or dependent variables. Contrary to critics who argue that core assumptions about homogeneity across experimental contrasts and outcomes are necessary for the standard meta-analytic model to hold (Slough \& Tyson, 2023), we think that comprehensive, heterogeneous meta-analyses can still be useful when these conditions aren't met; in such cases, meta-analytic estimates should be treated as \emph{descriptive} rather than \emph{causal} inferences. Moreover, we see value in contrasting all-inclusive estimates, which are fundamentally descriptive, to more rigorous, focused estimates, which, because they reflect strict selection criteria related to study design and outcome measurement, are more plausibly causal. The within-paper contrast in magnitude and precision between these two approaches is often startling and illuminating.

Third, we describe the typical structure of our meta-analytic papers. In general, we start by describing the main ideas and trends in a literature, which provides readers an `onramp' into the field's major findings and concerns. Our papers then describe our search processes and meta-analytic methods, followed by our meta-analytic findings. These analyses are structured to start big and go small, meaning that we first present meta-analytic estimates and tests for publication bias for the entire literature and then hone in on key quantities of interest from smaller subsets of the data. For instance, in a literature with randomized controlled trials, quasi-experiments, and observational designs, as well as a mix of attitudinal and behavioral outcomes, we might first present an overall meta-analytic effect, and then present a 3 by 2 table where each subset of study design + outcome strategy gets its own meta-analytic estimate. Alternatively, in a literature with many similar but distinct quantities of interest, e.g.~racial/ethnic prejudice vs.~prejudice against LGBTQ+ people vs.~prejudice against people with physical and mental disabilities, we would present the meta-analytic effects within each subset of data. Our concluding investigations tend to probe differences in effect size by some marker of study quality, e.g.~the presence of a pre-analysis plan, or the relationship between effect sizes within studies, e.g.~if changes to ideas tend to predict changes in behaviors.

Fourth, we detail our meta-analytic defaults. For instance, we use a random effects model, as implemented by the \texttt{metafor} package in R (Viechtbauer, 2010), rather than a fixed effects (sometimes called an equal effects) model, and we cluster our analyses at the level of an individual study. Further, we prefer Glass's \(\Delta\) to Cohen's \emph{d} so as to avoid an additional assumption about the effects of treatment on the underlying distribution of outcomes. Here, we also detail a novel estimator for converting difference in proportion to an effect size that has some desirable properties relative to the conventional conversion from an odds ratio (Gomila, 2021).

Fifth, we introduce an R package, \texttt{PaluckMetaSOP}, that contains functions to help us quickly and efficiently perform meta-analyses. We describe the main categories of function and provide brief usage examples.

Finally, we conclude by noting some open questions for meta-scientific meta-analyses. While we tend to begin meta-analytic projects with a sense of what's true and what's interesting, in practice, the most incisive questions often emerge after reading and coding studies, i.e.~after data collection has begun. In other words, meta-analyses are both \emph{hypothesis\_driven} (re: deductive) and \emph{data-driven} (re: inductive); However, typical checks on researcher degrees of freedom assume a hypothesis-driven framework. This makes it tricky to ``tie one's hands to the mast'' (Elster, 1977), e.g., by writing a detailed pre-analysis plan and pre-specifying which outcomes to focus on. We describe how we've dealt with such issues so far and how we think to improve going forward. Last, we discuss the difficulty of integrating indirect cues of study quality, e.g.~obviously mistaken statistical results or special attention devoted to reproducibility, into our analyses in a principled way.

Previous papers have noted potential pitfalls for meta-analysts (e.g. Greco, Zangrillo, Biondi-Zoccai, and Landoni (2013)), detailed common statistical errors in their application (Kadlec, Sainani, \& Nimphius, 2023), or noted that they are often not computationally reproducible (Daniel Lakens et al., 2017). This paper sits in that tradition, but is a bit of an offshoot in that we are not attempting to correct statistical or practical mistakes in meta-analyses, but rather articulate what it is that we are trying to do when we write them. In this regard, this paper is closely in dialogue with two recent papers that attempt to put meta-analysis on firmer theoretical footing. Slough and Tyson (2023) ``highlight the dangers of conflating conceptual differences across studies with statistical sources of variation'' (p.~29). They argue that for studies to ``identify the same empirical target'' (p.~1), they must have fundamentally \emph{harmonized} contrasts and measurement strategies, i.e.~the ``substantive comparison across studies is the same'' and ``the outcome of interest is the same and it is measured in the same way'' (p.~2). Absent these conditions, meta-analytic results might not be ``meaningful and interpretable'' (p.~2). Contrary to conventional approaches to improving the accuracy of meta-analyses, a lack of target equivalence cannot be ``solved solely with statistical techniques'' (p.~23). Rather, it is more likely to be achieved through ``design or inclusion criteria'' (p.~2).

We agree that an internally and externally valid causal meta-analytic estimate requires harmonization of contrasts and outcomes (as well as a high degree of internal validity within constituent studies). However, we also think that meta-analytic estimates which do \emph{not} meet these conditions \textemdash e.g.~those that combine results from many different designs, or whose diverse outcomes bear an unknown relationship to the true outcome of interest \textemdash can still be useful and informative if interpreted and framed correctly. We think that when a literature without clear harmony of the independent and dependent variables is a case where the underlying model might be ``wrong'' but still useful.

The other paper in this vein we look to is Simonsohn, Simmons, and Nelson (2022), who argue that while the typical meta-analysis in social psychology aims ``to be comprehensive, results-focused and transcriptive,'' this is ``misguided, leading to uninterpretable results that misrepresent research literatures.'' First, they argue that a desire for comprehensiveness typically leads the meta- analyst to average ``studies of the highest quality'' with ``studies that lack internal validity or external validity, which are obtained using incorrect statistical techniques, or studies where results seem to arise from methodological artefacts.'' This throw-it-all-in-the-blender approach produces results that are ``virtually guaranteed to lack a meaningful interpretation,'' despite their putative statistical precision. Second, the authors encourage meta-analysts to focus on studies' designs, rather than just their results, to provide ``readers with information that they can use to evaluate'' the quality of a research literature for themselves. Rather than describing every study, meta-analysts ``can succinctly describe and summarize common design failures\ldots and provide detailed descriptions only of the studies they deem most important or compelling.'' This advice ties closely to their last dictum to meta-analysts: to evaluate rather than to transcribe, i.e., detail the evidence from the very best studies, identify common methodological shortcomings of the literature as a whole, and, finally, ``discuss what open questions remain and what kinds of evidence would help answer those questions.''

Our meta-analyses share these goals (and Paluck et al. (2021) is cited therein for being ``selective, design-focused, and evaluative''). Moreover, the authors note that typical guides to meta-analysis ``briefly mention validity concerns but provide little guidance as to how to carry out quality control;'' one of the aims of this paper is to provide such guidance. However, as mentioned above, we part ways from this paper in that we believe comprehensive meta-analytic estimates still have a place in the meta-scientist's toolkit, so long as those estimates are ascribed descriptive rather than causal meaning and are used to contextualize further, more selective analyses.

Our paper is a hybrid of a conceptual analysis of meta-analyses in general and a report on how we in particular write them. On the latter front, this paper builds on a tradition of Standard Operating Procedures (SOPs) in the behavioral and medical sciences (D. Lakens, 2023; Rapport et al., 2013; Wallace et al., 2015). In particular, we look to Lin and Green (2016), which details how the Green lab at Columbia sets ``default practices to guide decisions when issues arise that were not anticipated in'' pre-analysis plans. Like that paper, this article is not intended as a comprehensive guide to the task in question \textemdash see Cumpston et al. (2019) and Cooper et al. (2019) for textbook-length treatments of meta-analysis, and Frank et al. (2024), chapter 16, Daniel Lakens (2022), chapter 11 and Slough and Tyson (forthcoming), chapter 5 for modern treatments of the subject aimed at social scientists. Rather, we articulate our lab's admittedly idiosyncratic approach to a task that many research groups embark on. We hope this will prove interesting and informative, but not definitive: we expect readers to adapt and expand our guidelines to meet their own needs. In general, we hope to see more labs publish papers about how they produce knowledge.

We now detail our conceptual aims when we write meta-analyses.

\subsection{Meta-analyses: Purposes and contributiouns}\label{meta-analyses-purposes-and-contributiouns}

\subsubsection{Our overarching meta-analytic agenda}\label{our-overarching-meta-analytic-agenda}

The main goals of our meta-analysis are the goals of any systematic review: to evaluate a body of knowledge for its strengths, weaknesses, central findings, open questions and empirical limitations. We write broadly for any reader who is interested in the underlying question, but generally aim for a non-specialist audience. In our experience, a literature's most noteworthy contributions often come from researchers outside the core field (e.g. Mousa (2020), Boisjoly, Duncan, Kremer, Levy, and Eccles (2006), or Scacco and Warren (2018) for the contact hypothesis, Munger (2017) for anti-prejudice work, or Haushofer, Ringdal, Shapiro, and Wang (2019) for violence against women); to such researchers, we aim to provide signposts, in the form of unearthed, widespread empirical gaps and theoretical limitations, about where future empirical work will be most valuable. We also believe that funders and policymakers will generally benefit from systematic, critical reviews that do not take subject matter expertise for granted.

In general, large, heterogeneous literatures, especially those with multiple theoretical perspectives, are good candidates for meta-analysis. In such cases, a meta-analysis might provide a useful contribution simply by dividing a literature into its constituent parts. For example, Paluck et al. (2021) categorizes 418 experiments published between 2007 and 2019 and organizes them into thirteen theoretical approaches and twelve categories of prejudice targeted. Simply identifying this heterogeneity of approaches and outcomes, along with describing a few constituent studies from the major theoretical strands in detail, helps the reader evaluate a literature's major findings for herself. In this particular case, a reader might reasonably wonder: does an intervention that aims to change political attitudes towards the rights of trans persons (Broockman \& Kalla, 2016) shed light on what might reduce tensions between castes in India (Lowe, 2021)? Is an extended contact intervention (Shamoa-Nir \& Razpurker-Apfeld, 2023) similar enough to a diversity training (Chang et al., 2019) that you can pool the effects together into something that coherently reflects the effects of a class of intervention? These questions come down to beliefs, and, as Slough and Tyson (2023) argue, cannot be resolved with statistics. A comprehensive and well-organized review is an indispensable tool for clarifying which questions to ask in the first place.

We also encourage authors not to be discouraged from writing meta-analyses of subjects that have been previously reviewed and analyzed, even many times. First, in our experience, simply reproducing the work of previous meta-analyses verbatim might generate surprises. For example, Pettigrew \& Troop's landmark meta-analysis -Pettigrew and Tropp (2006) identified 515 studies and 36 ``true experiments'' (p.~759) on intergroup contact; however, when we took a look at these papers for ourselves, we found that many ``were mislabeled as randomly assigned, did not feature `actual face-to-face interaction' or did not have a non-contact control group;'' between these issues and papers that lacked long-term outcome measurement, we ended up with nine experiments that met our inclusion criteria. Second, we share Munger's -Munger (2023) cocnern for ``temporal validity,'' the idea that because the conditions under which findings hold might change dramatically going forward, ``no research design, no empirical knowledge, is perfectible'' (p.~1). For instance, the implicit bias (Gawronski, 2019) and symbolic racism (McConahay \& Hough Jr, 1976; Sears \& Henry, 2003) research agendas emerged because researchers observed a dramatic drop in how readily white Americans would express overtly racist ideas in the 1970s and 80s, which called for a paradigm shift in measurement. Meta-analyses are essential for assessing which classes of interventions still `work' in light of such paradigm shifts. Further, as the credibility revolution spreads across the social sciences (Angrist \& Pischke, 2010; Samii, 2016; Vazire, 2018), we can generally expect recent research to meet comparatively higher standards of rigor and transparency, and therefore have a disproportionate impact on our understanding of the world. Fourth, even in fields with many systematic reviews, there's still often an open lane for a paper that's laser-focused on the findings of the best, most policy-relevant research (Paluck et al., 2019) or that's both comprehensive and quantitative (Paluck et al., 2021; Porat et al., 2024).

\subsubsection{Why meta-analysis and not just a systematic review?}\label{why-meta-analysis-and-not-just-a-systematic-review}

The aims we articulated above could all be met by a non-quantitative systematic review. However, We think that meta-analysis can help illuminate several meta-scientific inferences.

The first is publication bias. Classical approaches to assessing publication bias in meta-analysis are ``based on the fact that precision in estimating the underlying treatment effect will increase as the sample size of component studies increases'' (Egger, Smith, Schneider, \& Minder, 1997). The funnel plot detects publication bias under the assumption that smaller, more imprecisely estimated studies that produce null or backlash results are more likely to be shelved than large, precisely estimated studies that produce the same. (The underlying theory of researcher behavior seems to be a kind of widespread acceptance of the sunk cost fallacy \textemdash that once you've invested the kind of time and energy that a large, rigorous study requires, you'll push through to publication even if you'd put the results back in the `file drawer' (Rosenthal, 1979) if you'd put less work in.) Alternatively, one can simply plot the relationship between effect size and standard error, as we did in Paluck et al. (2019), where the strong positive relationship between the two suggested that ``a very large study would be expected to produce a minuscule \emph{increase} in prejudice.'' This class of methods requires estimates of effect size and variance for each study. Further, estimates of publication bias can be interesting \emph{between} metas, for instance, the evidence of publication bias that we found in our 2019 and 2021 papers but did not find in Porat et al. (2024), which drew more from public health perspectives and departments than did the previous two papers. This kind of evidence can shed light on whether (and how) the credibility revolution is taking hold across disciplines \textemdash a meta-meta-scientific inference.

Second, quantitative estimates allow for head-to-head tests of efficacy across different classes of interventions. For example, Paluck et al. (2021) estimated that the ``20 experiments testing antibias, multicultural, and moral education programs'' have an average effect size of \emph{d} = 0.30, compared to \emph{d} = 0.43 for 12 entertainment interventions. This does not necessarily mean that entertainment interventions reduce prejudice more effectively than antibias trainiings do \textemdash there are many confounding differences between these two categories of study \textemdash but the differences do tell the reader something about how effectively people in different disciplines are manipulating the key quantity of interest by their own lights.

Third, relationships between effect and study quality, measured in a variety of ways, are often illuminating. Paluck et al. (2019), for instance, found that studies with pre-analysis plans had an average effect size of 0.016, vs.~0.451 for everything else. This is not a well-identified estimate of the `effect' of pre-analysis plans \textemdash pre-analysis plans were not randomly assigned, and many other things varied between these studies \textemdash but it is at least a `hoop test' (Collier, 2011) for the hypothesis that p-hacking is a problem in the contact hypothesis literature. Likewise, Paluck et al. (2021) found a highly significant relationship between meta-analytic effect size and sample sizes, with the smallest quintile of studies producing an average effect of \emph{d} = 0.61 vs \emph{d} = 0.19 for the largest quintile. This is, again, not a well-identified test of the effect of increasing sample size, but it does suggest that the literature's ``results are not robust to the most basic assessments of study quality'' (p.~149).

Fourth, quantitative estimates allow for assessing differences in effect size between literatures. In general, we don't ascribe any particular meaning to small differences in estimated effect size \textemdash the difference between d = 0.3 vs d = 0.28 might mean something in some disciplines, but given the prevalence of measurement error in the fields we've analyzed (Schmidt \& Hunter, 1996), we would call this difference insignificant., However, \emph{large} differences between literatures raise some interesting questions. The three meta-analyses discussed in this paper all found overall effect sizes between 0.28 and 0.357: a relatively small range, whereas Green, Mathur, and Smith (2024), which sets relatively stringent inclusion criteria for both independent and dependent variables, finds an average effect size of 0.131 of interventions intended to reduce consumption of meat and animal products. Again, this is not a well-identified effect; these literatures are different across many dimensions. But the difference in effect sizes between a narrow, selective meta-analysis and its bigger siblings suggests that there are systematic differences between the literatures, and the researchers, that are worth exploring.

We now turn to the pros and cons of these two different approaches in more detail.

\subsection{The tradeoff between comprehensiveness and narrow, focused reviews}\label{the-tradeoff-between-comprehensiveness-and-narrow-focused-reviews}

Slough and Tyson (2023) and Simonsohn et al. (2022) have raised trenchant critiques about whether the typical meta-analytic estimate is meaningful. Both papers argue that meta-analyses should be more selective in what papers they include \textemdash Slough and Tyson to assure harmonization of the independent and dependent variables, and Simmons et al.~to prevent mixing ``studies of the highest quality'' with ``studies that lack internal validity.''

Of our three meta-analyses, Paluck et al. (2019) most closely hews to this model of inquiry. That paper looked solely at randomized controlled trials where intergroup contact was the treatment variable and where there was at least a day of delay between the commencement of treatment and outcome measurement. (Admittedly we could have tightened our study even further by looking for harmonized dependent variables, but given the diversity of outcomes we found, that would have effectively precluded meta-analysis.) Our subsequent two meta-analyses (Paluck et al., 2021; Porat et al., 2024), are much more clearly at risk of the kinds of incoherence that recent critics identify. Both studies include a mixture of experiments, quasi-experiments, and observational designs (so long as there was a control group with pre- and post-treatment outcomes); combine papers that are drawing from vastly different theoretical traditions, and mix divergent dependent variables.

Although we haven't seen the issue portrayed in these terms, each of these is a bias-variance trade-off. Conventional meta-analyses consistently resolve this trade-off in favor of reduced variance with only minimal attention to concordant bias. When this challenge \emph{is} discussed, it's typically framed in terms of a desirable trade between internal and external validity \{NOTE: cite Roe and Just (2009) for now but ultimately we want a quote from a meta about this\}, in the sense that what's lost in terms of unbiased causal inference is appropriately compensated for by gains in knowledge about the diversity of settings in which a treatment works.

In general, we do not find this argument persuasive.As Thye (2000) puts it, ``if there are doubts or questions about whether a relationship is real or spurious, then whether or not the finding applies to other settings is irrelevant'' (p.~1303). There is a reasonable counterargument that if randomized controlled trials and quasi-experimental/observational designs tend to yield roughly equivalent effect sizes, this would ameliorate concerns about potential confounders or reverse causality in the observational literature. (Indeed, Pettigrew and Tropp (2006) argue this, but as mentioned above, when we re-examined the assembled RCTs, we found them much too scant to say anything definitive.)

Either way, this does not address potential bias introduced by non-harmonized treatments and outcomes. The threat there is not from confounders, but the much more conceptual problem that it simply doesn't make sense to pool what's being pooled. Consider the sexual violence literature, which includes, for instance, a building-based intervention comprising ``higher levels of faculty/security presence in safe/unsafe `hot spots' mapped by students'' (Taylor, Stein, Mumford, \& Woods, 2013), and a module of peer-based consent training on a college campus (Crane, 2017). When you average these two things, what does the resulting quantity signify? Ascribing a causal meaning to that average \textemdash saying that both interventions are tests of the same core ideas, just in different settings \textemdash requires some strong additional assumptions. Likewise with dependent variables; in that paper, we coded the Sexual Experiences Survey Koss \& Gidycz (1985) for behavioral outcomes and the Illinois Rape Myth Acceptance Scale (Thelan \& Meadows, 2022) for ideas-based outcomes when they were available, but the nearest substitutes for each, in our assessment, when they weren't. What additional assumptions do we need to argue that these multitudinous outcomes are coherently integrable?

We take a different tack: to remove the assumption that a pooled meta-analytic effect should be considered causal. Instead, when we combine studies from heterogeneous designs, treatments, and outcomes, the pooled estimate is a \emph{descriptive} statistic representing the average size of changes measured by a group of researchers, according to them and with outcomes they chose. For Paluck et al. (2021), thus number represented the average effects found by prejudice researchers between 2007 and 2019; for Porat et al. (2024), the average effects found by sexual violence researchers between 1986 and 2018.

We think there is value in inclusive, comprehensive searches and analyses, even if the overall pooled estimate is not causal. First, the pooled estimate provides a benchmark for all subsequent analyses, including those with sufficient internal validity and harmonization of treatments and outcomes to be plausibly causal. For example, the pooled effect observed in Porat et al. (2024) is \(\Delta\) = 0.28, but just 0.086 for perpetration outcomes with randomized designs; of the 28 studies meeting these stricter inclusion criteria, 16 self-reported null results. We think this gap between the superficially encouraging results of the literature as a whole and the bleaker results of the best studies looking at the most crucial outcome in the primary prevention literature is a call to action to researchers. Whatever solace they take in the putative effects of the entire literature should be weighed against the scant evidence of change on perpetration outcomes. The contrast helps build the intuition that the literature is in need of a change of direction.

Second, how well different outcomes are harmonized or not is, in some cases, measurable. In Porat et al. (2024), we find overall random effects estimates of \(\Delta\) = 0.071 for behavioral outcomes and \(\Delta\) = 0.366 for ideas-based results. We further tested whether changes in ideas predicted changes in behaviors in studies that measured both, and found a disappointing lack of correlation between the two. This suggests that these classes of outcomes are not fundamentally assessing the same theoretical quantity, and that the paradigm on which this literature is based \textemdash myths about sexual violence cause sexual violence, so changing the myths should reduce the violence \textemdash does not seem to hold. Here, it made more sense to \emph{test} for harmonization than to assume it didn't exist. Even in cases where such analyses aren't viable, researchers can still categorize each point estimate by its class of dependent variable and analyze the results separately.

Third, coding a wide variety of interventions allows for subsequent analyses to focus on the effects of particular interventions, or classes of intervention, and comparing them to each other. Again, the overall meta-analytic estimate provides a backdrop to these more focused analyses.

Fourth, starting from a precise estimate, gleaned from hundreds of studies, and then moving to a much higher quality but sparser dataset of the very best studies is an effective narrative device. Consider the contrast provided by Pettigrew and Tropp (2006) and Paluck et al. (2019). The former study observes that across hundreds of studies and hundreds of thousands of people that contact typically reduces prejudice. The latter study, however, notes that as of 2018, there had been zero studies studying interracial contact in adults that had both random assignment and a delay of at least a single day between the beginning of treatment and outcome measurement. In other words, some essential basic research was missing. Here, we thiink the apparent solidity of the first claim makes the second all the more startling.

In sum, we see value to identifying, coding, and heterogeneous estimates from study designs, interventions, and outcomes. However, a focused meta-analysis with strict inclusion criteria can also have appreciable impact on the field. So when is one straetgy or the other appropriate?

In general, we favor comprehensive meta-analyses if resources allow. These projects are much more labor-intensive and time-consuming: Paluck et al. (2019) was basically written in a summer by three people, whereas Porat et al. (2024) took six years, five authors, and a team of RAs. (Admittedly the pandemic didn't help.) However, there are two main reasons to strive for comprehensiveness. The first is to develop, and then impart to readers, as thorough an understanding of an underlying literature as possible. For example, for our review of main ideas and zeitgeist studies in ``The Leading Approach to Reducing Sexual Violence'' in Porat et al. (2024), there is simply no substitute for reading hundreds of studies. (We'd also add that quantitative coding studies is a kind of enforcement mechanism for reading a paper closely.) Second, a more focused analysis of the very best studies can be encompassed within a comprehensive meta-analysis. For example, the dataset assembled in Paluck et al. (2021) could (hypothetically) be filtered to reproduce the dataset of Paluck et al. (2019) precisely, but the revers, of course, is not true.

However, a more focused meta-analysis can still be a very useful contribution, especially as a rejoinder to prior credulous reviews.

\subsection{Typical structure of Paluck lab meta-analytic papers}\label{typical-structure-of-paluck-lab-meta-analytic-papers}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Describe context and stakes of the paper (i.e.~why it's important to see if these interventions `work', or why there's reason to doubt an established consensus)
\item
  Intellectual overview of major ideas in the literature
\item
  descriptive statistics and/or qualitative overview about the database

  \begin{enumerate}
  \def\labelenumii{\arabic{enumii}.}
  \tightlist
  \item
    Contact hypothesis: what, where, why
  \item
    \begin{enumerate}
    \def\labelenumiii{\roman{enumiii}.}
    \setcounter{enumiii}{2}
    \tightlist
    \item
      Prejudice reduction: theoretical perspectives, landmark studies
    \end{enumerate}
  \item
    \begin{enumerate}
    \def\labelenumiii{\roman{enumiii}.}
    \setcounter{enumiii}{3}
    \tightlist
    \item
      Sexual violence paper: history of zeitgeist perspectives
    \end{enumerate}
  \end{enumerate}
\item
  Evaluate rather than transcribe
\item
  Meta-analytic search methods
\item
  Meta-analytic approach6
\item
  Quantitative Results
\item
  Discussion, highlights and gaps

  \begin{enumerate}
  \def\labelenumii{\roman{enumii}.}
  \tightlist
  \item
    We identify high points (excellent studies) and low points (widespread methodological deficiencies), like simonsohn et al call for
  \item
    place effect sizes in context of real-world impact whenever possible, e.g.~the prejudice paper argues that \(\Delta\) of 0.27 corresponds to {[}X{]} change on ANES survey
  \item
    As our reviews show, applying some fairly minimal quality standards quickly winnows hundreds of studies down to just a few
  \item
    These studies are likely to be the most policy-relevant, and oftentimes the `ideal' study simply hasn't been conducted at all yet

    \begin{enumerate}
    \def\labelenumiii{\alph{enumiii}.}
    \tightlist
    \item
      Paluck, Green and Green (2019) highlight lack of studies testing interracial contact among adults -\textgreater{} provide intellectual backdrop for Scacco and Warren (2018), Mousa (2020), and Lowe (2019) -- as well as providing theoretical underpinning for those studies' comparatively underwhelming findings
    \end{enumerate}
  \end{enumerate}
\item
  Conclusion
\end{enumerate}

\subsection{Paluck lab meta-analytic procedures}\label{paluck-lab-meta-analytic-procedures}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Code more than you think you're going to need

  \begin{enumerate}
  \def\labelenumii{\arabic{enumii}.}
  \tightlist
  \item
    You might be interested in the lasting effects, which means recording the latest possible effect sizes, but if you record the \emph{earliest} possible effects, you can detect within-study decay
  \end{enumerate}
\item
  Start big and then go small, like a funnel; i.e.~meta-analyze \emph{everything} and then zoom in on different subsets of the literature
  5. Test for publication bias should probably look at absolutely everything
  6. Inter-paper comparisons; Paluck, Green and Green (2019) and Paluck et al.~(2021) provide estimates of about \(\Delta\) = 0.3, and then Green, Smith and Mathur (forthcoming) find \(\Delta\) = 0.138, which also means something.
\item
  Do some serious tests for publication bias: both conventional tests (egger's test, funnel plot, \(\Delta\) \textasciitilde{} SE), but also think through where else it might emerge, e.g.~compare effect sizes in studies with and without DOIs
\item
  separate the dataset into different chunks, e.g.~by theory, study design, or measurement strategy, and present these meta-analytic estimates side by side

  \begin{enumerate}
  \def\labelenumii{\arabic{enumii}.}
  \setcounter{enumii}{3}
  \tightlist
  \item
    Subset enough and eventually you get to reasonable claim for contrast and mechanism harmony (Slough and Tyson 2022)
  \end{enumerate}
\item
  Zoom in on and carefully and analyze best studies, both in aggregate and individually

  \begin{enumerate}
  \def\labelenumii{\arabic{enumii}.}
  \setcounter{enumii}{4}
  \tightlist
  \item
    These results might be surprising, e.g.~when looking solely at RCTs on perpetration outcomes (Porat et al., 2024), slightly fewer than half are self-reported nulls.
  \end{enumerate}
\item
  Meta-analytic defaults

  \begin{enumerate}
  \def\labelenumii{\arabic{enumii}.}
  \setcounter{enumii}{5}
  \tightlist
  \item
    Random effects: any literature we want to look at is going to have heterogeneous inputs.
  \item
    Cluster at level of study
  \item
    Glass's \(\Delta\) rather than Cohen's d
  \item
    Difference in proportion rather than odds ratio

    \begin{enumerate}
    \def\labelenumiii{\arabic{enumiii}.}
    \setcounter{enumiii}{6}
    \tightlist
    \item
      resurface text from appendix to Porat et al.~(2024)
    \end{enumerate}
  \end{enumerate}
\end{enumerate}

\subsection{R package: blp\_meta\_functions}\label{r-package-blp_meta_functions}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Functions fall into four categories

  \begin{enumerate}
  \def\labelenumii{\arabic{enumii}.}
  \tightlist
  \item
    Converting studies to singular estimates of effect size, variance and standard error
  \item
    Wrapper functions that distill aggregate results into the core findings and make them table-ready
  \item
    plotting functions
  \item
    Miscellany: reproducibility, helper functions
  \end{enumerate}
\end{enumerate}

\subsection{Conclusions: hard cases}\label{conclusions-hard-cases}

A classic paper on meta-analysis defines the procedure as ``a statistical analysis which combines or integrates the results of several independent clinical trials considered by the analyst to be `combinable'\,'' (p.~XXX). Our (non-systematic) review of prior meta-analyses reveals a surprising dearth of attention to how much theoretical work is being done by the phrase ``considered by the analyst to be `combinable.'\,'' Simonsohn et al. (2022) argue that researchers who ``uncritically'' aggregate results from all available studies, as part of an ``active effort to eliminate publication bias\ldots might instead amplify the bias from poor research design and execution.'' We agree with this assessment, but would instead frame it as one of three central instances of a bias-variance tradeoff afflicting meta-analayses. By combining studies with non-comparable designs, interventions, and outcomes, researchers give readers the impression of remarkable precision (re: reduced variance) but at the expense of introducing non-statistical sources of uncertainty (re: bias) on all three fronts. Only by acknowledging these tradeoffs head-on, and devising interpretations and techniques for dealing with them, can we successfully integrate the parallel tracks of meta-analysis and meta-science.

More broadly, this paper seeks to answer two questions: what are we aiming to do when we do meta-analysis, and how do we do it? We that that we've answered these questions to some extent, but many open questions remain. To us, the most pressing boil down to a lack of clarity about whether meta-analyses are or are not hypothesis-driven. Other meta-analyses in the social sciences are clearly aimed at validating a pre-existing hypothesis. Ours, however, are more exploratory, and as befitting our meta-scientific framework, are more aimed at evaluating a literature's credibility. While we might start with some general hypotheses \textemdash e.g.~that a social psychology literature from the 2000s is likely to show evidence of selection pressures for statistical significance \textemdash we also develop many of our most important questions after reading papers, which, in the context of a

\begin{itemize}
\tightlist
\item
  A lot of checks on RDF assume hypothesis driven framework. They are difficult to integrate into our papers.
\end{itemize}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  What do we do when a paper's results are obviously not credible?

  \begin{enumerate}
  \def\labelenumii{\arabic{enumii}.}
  \tightlist
  \item
    e.g.~if they present a result that's well past possible, e.g.~t-test value of 36 (one of the papers in the prejudice literature had this, and also the cost-benefit analyses in the SOSA! Intervention in the primary prevention literature)
  \end{enumerate}
\item
  What's the right outcome to code? Often very unclear what's prime or most representative of underlying construct
\item
  Others\ldots{}
\item
  Meta-analyses are still very useful, we think, for readers (both expert and non-expert) and future researchers.
\end{enumerate}

\phantomsection\label{refs}
\begin{CSLReferences}{1}{0}
\bibitem[\citeproctext]{ref-angrist2010}
Angrist, J. D., \& Pischke, J.-S. (2010). The credibility revolution in empirical economics: How better research design is taking the con out of econometrics. \emph{Journal of Economic Perspectives}, \emph{24}(2), 3--30.

\bibitem[\citeproctext]{ref-bem2015}
Bem, D., Tressoldi, P., Rabeyron, T., \& Duggan, M. (2015). Feeling the future: A meta-analysis of 90 experiments on the anomalous anticipation of random future events. \emph{F1000Research}, \emph{4}.

\bibitem[\citeproctext]{ref-bezrukova2016}
Bezrukova, K., Spell, C. S., Perry, J. L., \& Jehn, K. A. (2016). A meta-analytical integration of over 40 years of research on diversity training evaluation. \emph{Psychological Bulletin}, \emph{142}(11), 1227.

\bibitem[\citeproctext]{ref-boisjoly2006}
Boisjoly, J., Duncan, G. J., Kremer, M., Levy, D. M., \& Eccles, J. (2006). Empathy or antipathy? The impact of diversity. \emph{American Economic Review}, \emph{96}(5), 1890--1905.

\bibitem[\citeproctext]{ref-broockman2016}
Broockman, D., \& Kalla, J. (2016). Durably reducing transphobia: A field experiment on door-to-door canvassing. \emph{Science}, \emph{352}(6282), 220--224.

\bibitem[\citeproctext]{ref-chang2019}
Chang, E. H., Milkman, K. L., Gromet, D. M., Rebele, R. W., Massey, C., Duckworth, A. L., \& Grant, A. M. (2019). The mixed effects of online diversity training. \emph{Proceedings of the National Academy of Sciences}, \emph{116}(16), 7778--7783.

\bibitem[\citeproctext]{ref-collier2011}
Collier, D. (2011). Understanding process tracing. \emph{PS: Political Science \& Politics}, \emph{44}(4), 823--830.

\bibitem[\citeproctext]{ref-cooper2019}
Cooper, H., Hedges, L. V., \& Valentine, J. C. (2019). \emph{The handbook of research synthesis and meta-analysis}. Russell Sage Foundation.

\bibitem[\citeproctext]{ref-crane2017}
Crane, P. R. (2017). \emph{The POWER of consent: An evaluation of peer-based consent programming in sexual assault prevention} (PhD thesis). Ohio University.

\bibitem[\citeproctext]{ref-cuddy2018}
Cuddy, A. J., Schultz, S. J., \& Fosse, N. E. (2018). P-curving a more comprehensive body of research on postural feedback reveals clear evidential value for power-posing effects: Reply to simmons and simonsohn (2017). \emph{Psychological Science}, \emph{29}(4), 656--666.

\bibitem[\citeproctext]{ref-cumpston2019}
Cumpston, M., Li, T., Page, M. J., Chandler, J., Welch, V. A., Higgins, J. P., \& Thomas, J. (2019). Updated guidance for trusted systematic reviews: A new edition of the cochrane handbook for systematic reviews of interventions. \emph{The Cochrane Database of Systematic Reviews}, \emph{2019}(10).

\bibitem[\citeproctext]{ref-egger1997}
Egger, M., Smith, G. D., Schneider, M., \& Minder, C. (1997). Bias in meta-analysis detected by a simple, graphical test. \emph{Bmj}, \emph{315}(7109), 629--634.

\bibitem[\citeproctext]{ref-elster1977}
Elster, J. (1977). Ulysses and the sirens: A theory of imperfect rationality. \emph{Social Science Information}, \emph{16}(5), 469--526.

\bibitem[\citeproctext]{ref-ferguson2023}
Ferguson, J., Littman, R., Christensen, G., Paluck, E. L., Swanson, N., Wang, Z., \ldots{} Pezzuto, J.-H. (2023). Survey of open science practices and attitudes in the social sciences. \emph{Nature Communications}, \emph{14}(1), 5401.

\bibitem[\citeproctext]{ref-frank2024}
Frank, M. C., Braginsky, M., Cachia, J., Coles, N., Hardwicke, T. E., Hawkins, R. D., \ldots{} Williams, R. (2024). \emph{Experimentology: An open science approach to experimental psychology methods}. MIT Press. \url{https://doi.org/10.7551/mitpress/14810.001.0001}

\bibitem[\citeproctext]{ref-gawronski2019}
Gawronski, B. (2019). Six lessons for a cogent science of implicit bias and its criticism. \emph{Perspectives on Psychological Science}, \emph{14}(4), 574--595.

\bibitem[\citeproctext]{ref-glass1977}
Glass, G. V. (1977). 9: Integrating findings: The meta-analysis of research. \emph{Review of Research in Education}, \emph{5}(1), 351--379.

\bibitem[\citeproctext]{ref-gomila2021}
Gomila, R. (2021). Logistic or linear? Estimating causal effects of experimental treatments on binary outcomes using regression analysis. \emph{Journal of Experimental Psychology: General}, \emph{150}(4), 700.

\bibitem[\citeproctext]{ref-greco2013}
Greco, T., Zangrillo, A., Biondi-Zoccai, G., \& Landoni, G. (2013). Meta-analysis: Pitfalls and hints. \emph{Heart, Lung and Vessels}, \emph{5}(4), 219.

\bibitem[\citeproctext]{ref-green2024}
Green, S. A., Mathur, M., \& Smith, B. (2024). \emph{Environmental \& health appeals are the most effective vegan outreach strategies}.

\bibitem[\citeproctext]{ref-hardwicke2021}
Hardwicke, T. E., Bohn, M., MacDonald, K., Hembacher, E., Nuijten, M. B., Peloquin, B. N., \ldots{} Frank, M. C. (2021). Analytic reproducibility in articles receiving open data badges at the journal psychological science: An observational study. \emph{Royal Society Open Science}, \emph{8}(1), 201494.

\bibitem[\citeproctext]{ref-hardwicke2018}
Hardwicke, T. E., Mathur, M. B., MacDonald, K., Nilsonne, G., Banks, G. C., Kidwell, M. C., et al.others. (2018). Data availability, reusability, and analytic reproducibility: Evaluating the impact of a mandatory open data policy at the journal cognition. \emph{Royal Society Open Science}, \emph{5}(8), 180448.

\bibitem[\citeproctext]{ref-hardwicke2023transparency}
Hardwicke, T. E., \& Vazire, S. (2023). Transparency is now the default at psychological science. SAGE Publications Sage CA: Los Angeles, CA.

\bibitem[\citeproctext]{ref-haushofer2019}
Haushofer, J., Ringdal, C., Shapiro, J. P., \& Wang, X. Y. (2019). \emph{Income changes and intimate partner violence: Evidence from unconditional cash transfers in kenya}. National Bureau of Economic Research.

\bibitem[\citeproctext]{ref-kadlec2023}
Kadlec, D., Sainani, K. L., \& Nimphius, S. (2023). With great power comes great responsibility: Common errors in meta-analyses and meta-regressions in strength \& conditioning research. \emph{Sports Medicine}, \emph{53}(2), 313--325.

\bibitem[\citeproctext]{ref-kleinstauber1996}
Kleinstäuber, M., Witthöft, M., Steffanowski, A., Van Marwijk, H., Hiller, W., \& Lambert, M. J. (1996). \emph{Cochrane database of systematic reviews}.

\bibitem[\citeproctext]{ref-koss1985}
Koss, M. P., \& Gidycz, C. A. (1985). Sexual experiences survey: Reliability and validity. \emph{Journal of Consulting and Clinical Psychology}, \emph{53}(3), 422.

\bibitem[\citeproctext]{ref-koss1982}
Koss, M. P., \& Oros, C. J. (1982). Sexual experiences survey: A research instrument investigating sexual aggression and victimization. \emph{Journal of Consulting and Clinical Psychology}, \emph{50}(3), 455.

\bibitem[\citeproctext]{ref-lakens2022}
Lakens, Daniel. (2022). \emph{Improving your statistical inferences}. \url{https://doi.org/10.5281/zenodo.6409077}

\bibitem[\citeproctext]{ref-lakens2023}
Lakens, D. (2023). \emph{When and how to deviate from a preregistration}. \url{https://doi.org/10.31234/osf.io/ha29k}.

\bibitem[\citeproctext]{ref-lakens2017}
Lakens, Daniel, LeBel, E., Page-Gould, E., Assen, M. van, Spellman, B., Schönbrodt, F., \& Hertogs, R. (2017). Examining the reproducibility of meta-analyses in psychology. \emph{Retrieved from Osf. Io/Q23ye}.

\bibitem[\citeproctext]{ref-lin2016}
Lin, W., \& Green, D. P. (2016). Standard operating procedures: A safety net for pre-analysis plans. \emph{PS: Political Science \& Politics}, \emph{49}(3), 495--500.

\bibitem[\citeproctext]{ref-lowe2021}
Lowe, M. (2021). Types of contact: A field experiment on collaborative and adversarial caste integration. \emph{American Economic Review}, \emph{111}(6), 1807--1844.

\bibitem[\citeproctext]{ref-mcconahay1976}
McConahay, J. B., \& Hough Jr, J. C. (1976). Symbolic racism. \emph{Journal of Social Issues}, \emph{32}(2), 23--45.

\bibitem[\citeproctext]{ref-mousa2020}
Mousa, S. (2020). Building social cohesion between christians and muslims through soccer in post-ISIS iraq. \emph{Science}, \emph{369}(6505), 866--870.

\bibitem[\citeproctext]{ref-munger2017}
Munger, K. (2017). Tweetment effects on the tweeted: Experimentally reducing racist harassment. \emph{Political Behavior}, \emph{39}, 629--649.

\bibitem[\citeproctext]{ref-munger2023}
Munger, K. (2023). Temporal validity as meta-science. \emph{Research \& Politics}, \emph{10}(3), 20531680231187271.

\bibitem[\citeproctext]{ref-nosek2015}
Nosek, A., B. A. (2015). Promoting an open research culture. \emph{Science}, \emph{348}(6242), 1422--1425.

\bibitem[\citeproctext]{ref-obels2020}
Obels, P., Lakens, D., Coles, N. A., Gottfried, J., \& Green, S. A. (2020). Analysis of open data and computational reproducibility in registered reports in psychology. \emph{Advances in Methods and Practices in Psychological Science}, \emph{3}(2), 229--237.

\bibitem[\citeproctext]{ref-paluck2009}
Paluck, E. L., \& Green, D. P. (2009). Prejudice reduction: What works? A review and assessment of research and practice. \emph{Annual Review of Psychology}, \emph{60}, 339--367.

\bibitem[\citeproctext]{ref-paluck2019}
Paluck, E. L., Green, S. A., \& Green, D. P. (2019). The contact hypothesis re-evaluated. \emph{Behavioural Public Policy}, \emph{3}(2), 129--158.

\bibitem[\citeproctext]{ref-paluck2021}
Paluck, E. L., Porat, R., Clark, C. S., \& Green, D. P. (2021). Prejudice reduction: Progress and challenges. \emph{Annual Review of Psychology}, \emph{72}, 533--560.

\bibitem[\citeproctext]{ref-pettigrew2006}
Pettigrew, T. F., \& Tropp, L. R. (2006). A meta-analytic test of intergroup contact theory. \emph{Journal of Personality and Social Psychology}, \emph{90}(5), 751.

\bibitem[\citeproctext]{ref-porat2024}
Porat, R., Gantman, A., Paluck, E. L., Green, S. A., \& Pezzuto, J.-H. (2024). Preventing sexual violence -- a behavioral problem without a behaviorally-informed solution. \emph{Psychological Science in the Public Interest}, \emph{25}(1), 1--30.

\bibitem[\citeproctext]{ref-rapport2013}
Rapport, F., Storey, M., Porter, A., Snooks, H., Jones, K., Peconi, J., et al.others. (2013). Qualitative research within trials: Developing a standard operating procedure for a clinical trials unit. \emph{Trials}, \emph{14}(1), 1--8.

\bibitem[\citeproctext]{ref-roe2009}
Roe, B. E., \& Just, D. R. (2009). Internal and external validity in economics research: Tradeoffs between experiments, field experiments, natural experiments, and field data. \emph{American Journal of Agricultural Economics}, \emph{91}(5), 1266--1271.

\bibitem[\citeproctext]{ref-rosenthal1979}
Rosenthal, R. (1979). The file drawer problem and tolerance for null results. \emph{Psychological Bulletin}, \emph{86}(3), 638.

\bibitem[\citeproctext]{ref-samii2016}
Samii, C. (2016). Causal empiricism in quantitative research. \emph{The Journal of Politics}, \emph{78}(3), 941--955.

\bibitem[\citeproctext]{ref-scacco2018}
Scacco, A., \& Warren, S. S. (2018). Can social contact reduce prejudice and discrimination? Evidence from a field experiment in nigeria. \emph{American Political Science Review}, \emph{112}(3), 654--677.

\bibitem[\citeproctext]{ref-schmidt1996}
Schmidt, F. L., \& Hunter, J. E. (1996). Measurement error in psychological research: Lessons from 26 research scenarios. \emph{Psychological Methods}, \emph{1}(2), 199.

\bibitem[\citeproctext]{ref-schooler2014}
Schooler, J. W. (2014). Metascience could rescue the {``replication crisis.''} \emph{Nature}, \emph{515}(7525), 9--9.

\bibitem[\citeproctext]{ref-sears2003}
Sears, D. O., \& Henry, P. J. (2003). The origins of symbolic racism. \emph{Journal of Personality and Social Psychology}, \emph{85}(2), 259.

\bibitem[\citeproctext]{ref-shamoa2023}
Shamoa-Nir, L., \& Razpurker-Apfeld, I. (2023). Can you imagine this? Imagined contact as a strategy to promote positive intergroup relations. \emph{Frontiers in Psychology}, \emph{14}.

\bibitem[\citeproctext]{ref-simonsohn2022}
Simonsohn, U., Simmons, J., \& Nelson, L. D. (2022). Above averaging in literature reviews. \emph{Nature Reviews Psychology}, \emph{1}(10), 551--552.

\bibitem[\citeproctext]{ref-slough2023}
Slough, T., \& Tyson, S. A. (2023). External validity and meta-analysis. \emph{American Journal of Political Science}, \emph{67}(2), 440--455.

\bibitem[\citeproctext]{ref-taylor2013}
Taylor, B. G., Stein, N. D., Mumford, E. A., \& Woods, D. (2013). Shifting boundaries: An experimental evaluation of a dating violence prevention program in middle schools. \emph{Prevention Science}, \emph{14}(1), 64--76.

\bibitem[\citeproctext]{ref-thelan2022}
Thelan, A. R., \& Meadows, E. A. (2022). The illinois rape myth acceptance scale---subtle version: Using an adapted measure to understand the declining rates of rape myth acceptance. \emph{Journal of Interpersonal Violence}, \emph{37}(19-20), NP17807--NP17833.

\bibitem[\citeproctext]{ref-thye2000}
Thye, S. R. (2000). Reliability in experimental sociology. \emph{Social Forces}, \emph{78}(4), 1277--1309.

\bibitem[\citeproctext]{ref-vazire2018}
Vazire, S. (2018). Implications of the credibility revolution for productivity, creativity, and progress. \emph{Perspectives on Psychological Science}, \emph{13}(4), 411--417.

\bibitem[\citeproctext]{ref-viechtbauer2010}
Viechtbauer, W. (2010). Conducting meta-analyses in r with the metafor package. \emph{Journal of Statistical Software}, \emph{36}, 1--48.

\bibitem[\citeproctext]{ref-wallace2015}
Wallace, M., Hamesch, K., Lunova, M., Kim, Y., Weiskirchen, R., Strnad, P., \& Friedman, S. (2015). Standard operating procedures in experimental liver research: Thioacetamide model in mice and rats. \emph{Laboratory Animals}, \emph{49}(1\_suppl), 21--29.

\end{CSLReferences}


\end{document}
