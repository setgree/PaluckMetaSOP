Standard Operating Procedures for Meta-Analysis in the Paluck Lab

By Seth Ariel Green, Roni Porat, and Betsy Levy Paluck

Abstract: this paper describes the motivations and procedures for meta-analyses and systematic reviews by Betsy Levy Paluck and co-authors, with reference and examples drawn from four such papers (Paluck and Green 2009, Paluck, Green, and Green 2019, Paluck et al. 2021, and Gantman et al. 2024). We first describe our conceptual aims when writing meta-analytic papers, which are to provide an 'on-ramp' to interested readers, to evaluate a literature's strengths and weaknesses, and to illuminate empirical gaps, which we intend as signposts for future researchers. Second, we describe the typical 'flow' of the papers as a whole. Third, we describe our meta-analytic procedures, detailing our default reporting structure and analytic choices and engaging with recent critiques from Slough and Tyson (2023) and Simonsohn, Simmons, and Nelson (2022). Fourth, we introduce a collection of functions we use to implement our meta-analytic choices, available as an R package. Fifth, we conclude with some open questions for meta-analysts.


1: Introduction: when are (comprehensive) meta-analyses useful?
	A. Big literature with heterogeneous research quality
	B. Multiple theoretical approaches whose comparative efficacy is unknown
	C. Intuition that a comprehensive read through and tests for rigor will reveal collective gaps in understanding
2. Our meta-analytic aims
	A. On-ramp to unfamiliar literature
		i. Alternate metaphor: "a lobby" (Toni Morrison's introduction to Beloved)
		ii. Contact hypothesis: what, where, why
		iii. Prejudice reduction: theoretical perspectives, landmark studies
		iv. Sexual violence paper: history of zeitgeist perspectives
	B. Evaluate rather than transcribe
		i. Describe critique of transcriptive, non-evaluative work in Simonsohn et al. (2022)
		ii. Highlight high points (excellent studies) and low points (widespread methodological deficiencies)
		iii. place effect sizes in context of real-world impact whenever possible
			a. Prejudice paper: Delta of X corresponds to Y change on Z survey
	C. Illuminate empirical gaps
		i. As these papers show, applying some fairly minimal quality standards quickly winnows hundreds of studies down to just a few
		ii. These studies are likely to be the  most policy-relevant, and oftentimes the 'ideal' study simply hasn't been conducted at all yet
		iii. Examples:
			a. Paluck and Green (2009) highlight lack of field experiments -> Paluck (2009) runs a field experiment
			b. Paluck, Green and Green (2019) highlights lack of studies testing interracial contact among adults -> provides intellectual backdrop for Scacco and Warren (2018), Mousa (2020), and Lowe (2019) -- as well as providing theoretical underpinning for those studies' comparatively underwhelming findings
			c. similar examples from Paluck et al. (2021)?
3. Typical structure of Paluck lab meta-analytic papers
	A. Describe context and stakes of the paper (i.e. why it's important to see if these interventions 'work')
	B. Intellectual overview of major ideas in the literature
	C. Meta-analytic search methods
	D. Meta-analytic approach
	E. Quantitative Results
	F. Discussion, highlights and gaps
	G. Conclusion
4. Paluck lab meta-analytic procedures
	A. Code more than you think you're going to need
	B. Start big and go small, i.e. meta-analyze *everything* and then zoom in on different subsets of the literature
		i. Why meta-analyze everything?
			a. Slough and Tyson (2023) argue that meta-analytic estimates only make sense if there's mechanism harmony, i.e. if the many studies are clearly testing some common underlying theoretical concept. In some literatures we've looked at, this pretty clearly does not hold, e.g. the average effect of a contact hypothesis field experiment for Muslims and Christians and Nigeria  and a diversity training in a U.S. corporate workplace tells you the median effect of *what* exactly?
			b. Ditto with "target harmony", which is whether interventions are aimed *at* changing the same thing. For instance, does it make sense to average the effect of some intervention on self-reported attitudes and behavioral outcomes? As Gantman et al. (2023) shows, these outcomes are not always well correlated, which is prima facie evidence that we don't have target harmony.
			c. Last, integrating observational and randomized studies is often justified as supplementing high studies with high internal validity with studies with high *external* validity, but it's probably more accurate to say that you take some studies that provide unbiased estimates and then, for in exchange for greater statistical precision, you induce bias arising from non-statistical sources of uncertainty that's impossible to quantify (Kaplan Green and Gerber 2002).
		ii: multi-part answer
			a. Overall meta-analytic estimate provides a within-paper size comparison; your overall effect size is X and your effect size for the very best studies is 1/3 X, that means something.
			b. Test for publication bias should probably look at absolutely everything
			c. Inter-paper comparisons; Paluck, Green and Green (2019) and Paluck et al. (2021) provide estimates of about ∆ = 0.3, and then Green, Smith and Mathur (forthcoming) find ∆ 0.138, that also means something.
	C. Do some serious tests for publication bias: both conventional tests (egger's test, funnel plot, ∆ ~ SE), but also think through where else it might emerge
	D. separate the dataset into different chunks, e.g. by theory, study design, or measurement strategy, and present overall meta-analytic estimates side by side
		i. Subset enough and eventually you get to reasonable claim for target and mechanism harmony
	E. zoom in on and carefully and analyze best studies, both in aggregate and individually
		i. These results might be surprising, e.g. when looking solely at RCTs on perpetration outcomes (Gantman et al. 2024), slightly fewer than half are self-reported nulls.
	F. Meta-analytic defaults
		i. Random effects: any literature we want to look at is going to have heterogeneous inputs.
		ii. Cluster at level of study
		iii. Glass's ∆ rather than Cohen's d
		iv. Difference in proportion rather than odds ratio
			a. resurface text from appendix to Gantman et al. (2024)
5. R package: blp_meta_functions
	A. Functions fall into four categories
		i. Converting studies to singular estimates of effect size, variance and standard error
		ii. Wrapper functions that distill aggregate results into the core findings and make them table-ready
		iii. plotting functions
		iv. Miscellany: reproducibility, helper functions
6. Conclusions: hard cases
	A. What do we do when a paper's results are obviously not credible?
		i. e.g. if they present a result that's well past possible, e.g. t-test value of 36
	B. What's the right outcome to code? Often very unclear what's prime or most representative of underlying construct
	C. Others...
	D. Still very useful we think for readers (both expert and non-expert) and future researchers.


### Meta-analysis at 50: from social to medical science and back again

- Since the term was first coined in 1976 [@glass1976], meta-analysis has come to be understood as a crucial tool for synthesizing evidence in the medical and natural sciences (citation)

- core idea, as a 1990 NYT article put put it, is to allow researchers "to draw more reliable inferences or new conclusions from the combined results than from smaller studies that may be inconclusive individually" [@altman1990]
- labeled as controversial in that article, but the examples are all from medicine. then [debated vigorously in 2012](https://www.nytimes.com/2012/10/16/science/stanford-organic-food-study-and-vagaries-of-meta-analyses.html)
- take an excellent conventional meta-analysis, e.g. @meremikwu2012. This study looks at Intermittent Preventive Treatment for malaria in seven trials, with  12,589 subjects,all of which  "were conducted in West Africa, and six of seven trials were restricted to children aged less than 5 years." The study finds that IPT leads to significant reductions in both malaria cases and severe malaria cases, "a small reduction in all‐cause mortality," and that "drug‐related adverse events, if they occur, are probably rare."
- Note four qualities of this meta-analysis:
    1. All RCTs
    2. Singular,cohesive treatment
    3. Seven different draws from a common population
    4. Coherent, commonly measured outcomes that clearly represent key quantities of interest
In the past 20 years, meta-analysis has become much more common in the social sciences and in psychological science in particular.  But if we look at some prominent ones, we might say that these conditions of the "ideal" meta-analysis don't really hold.
    1. Methodological diversity (Pettigrew and Tropp 2006 integrate studies of widely different designs)
    2. Very diverse treatments (https://psycnet.apa.org/record/2016-43598-001)
    3. Diverse settings (cite example)
    4. Broad array of outcome measures (Paluck et al. 2020)

#### Notes

1. Meta-analyses are common in the social sciences – cite some big ones.
2. And yet, meta-analyses are surprisingly undertheorized.
    1. Most of the guides for meta-analysis are aimed at medical doctors and researchers and presume that you contrast and target harmonization (Slough and Tyson 2022) – in other words, that the “X” and the “Y” in “does X influence Y?” are basically the same across studies, and the goal is just to average their results.
        1. Sometimes that holds, e.g. [deworming](https://scholar.harvard.edu/files/kremer/files/meta-analysis_deworming_world_bank_working_paper_dec_2016.pdf) or [insecticide-treated bednets](https://www.mmv.org/sites/default/files/uploads/docs/access/SMC_Tool_Kit/publications/Meremikww-ipt-review.pdf)
        2. But in the behavioral sciences, the inputs are probably a lot more heterogeneous, e.g. [diversity](https://psycnet.apa.org/record/2016-43598-001) [training](https://compass.onlinelibrary.wiley.com/doi/10.1111/spc3.12741?af=R) or [the](https://pubmed.ncbi.nlm.nih.gov/16737372/) [contact](https://psycnet.apa.org/record/2015-07056-001) [hypothesis](https://journals.sagepub.com/doi/abs/10.1177/1088868318762647), as are the outputs, as are the outputs, e.g. a mix of attitudinal and behavioral outcomes and sometimes totally bespoke instruments.
        3. Looking at [the Cochrane Review](https://training.cochrane.org/handbook/current/chapter-10), we see guidelines that are thorough, but generally propose statistical solutions to what are fundamentally non-statistical sources of uncertainty. By contrast, [Simmonsohn et al. (2022)](https://www.nature.com/articles/s44159-022-00101-8) argue for _design_-based solutions.
    2. Over the course of 8 years and three meta-analyses, we’ve developed a distinct perspective about when to do meta-analysis, what it’s useful for, and how to do it. This article articulates that perspective.
    3. SOPs ([https://alexandercoppock.com/Green-Lab-SOP/Green_Lab_SOP.pdf](https://alexandercoppock.com/Green-Lab-SOP/Green_Lab_SOP.pdf), [https://www.stat.berkeley.edu/~winston/sop-safety-net.pdf](https://www.stat.berkeley.edu/~winston/sop-safety-net.pdf))
4. So when are meta-analyses useful?


\newpage
```{r echo=F, include=F}
library(papaja)
cite_r("meta-sop-references.bib", style = "text")
# https://bookdown.org/yihui/rmarkdown-cookbook/bibliography.html
# for more on citation
```

   7. Cluster at level of study
    8. Glass's $\Delta$ rather than Cohen's d

These critiques register deeply with us because of our three recent meta-analyses, two strive to be comprehensive and therefore arguably do not meet the assumptions laid out by Slough and Tyson and are guilty of the sins identified by SSN (though SSN also identify one of papers as exemplary)

So does meta-analysis still _work_ under these conditions? Is there something cogent and meaningful here?


### All models are wrong, but this one is still useful

This evidence suggests, we think, that the paradigm on which this literature is based \textemdash myths about sexual violence cause sexual violence, so changing the myths should reduce the violence \textemdash does not seem to hold. This would not have been possible without quantitative assessment.

There is a reasonable counterargument that if randomized controlled trials and quasi-experimental or observational designs tend to yield roughly equivalent effect sizes, this would ameliorate concerns about potential confounders or reverse causality in the observational literature. (Indeed, @pettigrew2006 argue this, but as mentioned above, when we re-examined the assembled RCTs, we found them much too scant to say anything definitive.)

Our general thought is that comprehensive meta-analyses are especially valuable contributions if framed and interpreted correctly, but that selective, narrow meta-analyses can still be useful and interesting. In particular, we see value in contrasting all-inclusive estimates, which are fundamentally descriptive, to more rigorous, focused estimates, which, if they reflect strict selection criteria, are more plausibly causal. The within-paper contrast in magnitude and precision between these two approaches is often startling and illuminating.

Our quantitative analyses are structured to start big and go small. We first present meta-analytic estimates and tests for publication bias for the entire literature and then hone in on key quantities of interest from smaller subsets of the data. Our concluding investigations tend to probe differences in effect size by some marker of study quality, e.g. the presence of a pre-analysis plan or sample size.

One potential response to the bias introduced by between internal and external validity: what's lost in terms of unbiased causal inference is appropriately compensated for by gains in knowledge about the diversity of settings in which a treatment works. (See @mathur2022 for discussion.)


We do not find this argument persuasive. As @thye2000 puts it, "if there are doubts or questions about whether a relationship is real or spurious, then whether or not the finding applies to other settings is irrelevant" (p. 1303). Moreover, this does not address potential bias introduced by non-harmonized treatments and outcomes.

We see a variety of responses to these challenges in the literature. @lemmer2015, for example, detail inclusion criteria for all three categories: "interethnic contact intervention implemented in a field setting...with at least one indicator of ethnic prejudice" from studies that had "randomized posttest only with control (POWC), pretest–posttest with control (PPWC), and pretest–posttest single group (PPSG)" designs (p. 154). They justify the inclusion of PPSG designs "because the application of other designs is often not possible in real-world settings. The exclusion of studies with this methodological weaker design would mean to not consider a great deal of relevant results" (p. 156). @bezrukova2016, in their meta-analysis of diversity trainings, also exclude purely retrospective designs, and hypothesize that studies with stronger designs should show larger results. In their section on eligilibty criteria, they provide examples of what they mean by "diversity," "training," and "diversity training outcome" (p. 7).

These approaches are reasonable on their face; as always, however, the devil is in the details.
The threat there is not from confounders, but the much more conceptual problem that it simply doesn't make sense to pool what's being pooled.
Estimates of publication bias can also be interesting between meta-analyses. For instance, we found strong evidence of publication bias in @paluck2019 and @paluck2021 but not in @porat2024, which drew more from public health perspectives and departments than did the previous two papers. This kind of finding sheds light on whether, and along which dimensions, the credibility revolution is taking hold across disciplines.

 We took this approach in @paluck2019. In contrast to Pettigrew & Troop's landmark meta-analysis [-@pettigrew2006], which identified 515 studies and 36 "true experiments" (p. 759) on intergroup contact, we found just 27 experiments featuring at least a day of delay between intervention and measurement.^[We also found that many of the studies identified by @pettigrew2006 as experiments had been "mislabeled as randomly assigned, did not feature 'actual face-to-face interaction' or did not have a non-contact control group" (p. 136).] We also found that as of 2018, there had been zero evaluations of interracial contact in adults that had both random assignment and a delay of at least a single day between the beginning of treatment and outcome measurement. This sparsity of empirical results helps provide an intellectual backdrop to researchers who find inconclusive or null results by showing that a putatively solid finding is resting on a weak foundation. For example, @scacco2018 and @mousa2020 ran landmark contact hypothesis papers that found much less salubrious effects than the literature's boosters might have expected. We hope that the analyses in @paluck2019 helped make these results seem more legible, and less like statistical anomalies, to readers.
